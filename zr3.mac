; Copyright 2011-2017 - Mersenne Research, Inc.  All rights reserved
; Author:  George Woltman
; Email: woltman@alum.mit.edu
;

;;
;;
;; All new macros for version 29 of gwnum.  Do an AVX-512 radix-3 step in an FFT.
;; The forward FFT macros multiply by the sin/cos values at the end of the macro
;; and the inverse FFTs multiply by the sin/cos values at the start of the macro.
;; We also implement the Daniel J. Bernstein (DJB) "exponent-1" idea to save sin/cos
;; memory.
;;
;;

xx

;;
;; ************************************* three-complex-djbfft variants ******************************************
;;

;; The standard version
zr3_3cl_three_complex_djbfft_preload MACRO
	zr3_3c_djbfft_cmn_preload
	ENDM
zr3_3cl_three_complex_djbfft MACRO srcreg,srcinc,d1,screg,scinc,maxrpt,L1pt,L1pd
	zr3_3c_djbfft_cmn srcreg,srcinc,0,d1,noexec,screg,screg+32,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

;; Like standard but uses rbx to index into source data
zr3_f3cl_three_complex_djbfft_preload MACRO
	zr3_3c_djbfft_cmn_preload
	ENDM
zr3_f3cl_three_complex_djbfft MACRO srcreg,srcinc,d1,screg,scinc,maxrpt,L1pt,L1pd
	zr3_3c_djbfft_cmn srcreg,srcinc,rbx,d1,noexec,screg,screg+32,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

;; Like standard, but uses a six_reals sin/cos table
zr3_r3cl_three_complex_djbfft_preload MACRO
	zr3_3c_djbfft_cmn_preload
	ENDM
zr3_r3cl_three_complex_djbfft MACRO srcreg,srcinc,d1,screg,scinc,maxrpt,L1pt,L1pd
	zr3_3c_djbfft_cmn srcreg,srcinc,0,d1,noexec,screg+64,screg+96,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

; Like the standard version except vbroadcastsd is used to reduce sin/cos data
zr3_b3cl_three_complex_djbfft_preload MACRO
	zr3_3c_djbfft_cmn_preload
	ENDM
zr3_b3cl_three_complex_djbfft MACRO srcreg,srcinc,d1,screg,scinc,maxrpt,L1pt,L1pd
	zr3_3c_djbfft_cmn srcreg,srcinc,0,d1,exec,screg,screg+8,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

; Like b3cl but extracts the sin/cos data to broadcasts from
; the six-real/three_complex sin/cos table
zr3_rb3cl_three_complex_djbfft_preload MACRO
	zr3_3c_djbfft_cmn_preload
	ENDM
zr3_rb3cl_three_complex_djbfft MACRO srcreg,srcinc,d1,screg,scinc,maxrpt,L1pt,L1pd
	zr3_3c_djbfft_cmn srcreg,srcinc,0,d1,exec,screg+8,screg+48,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

;; Common code to do a 3-complex FFT.  The input values are R1+R4i, R2+R5i, R3+R6i
;; A 3-complex FFT is:
;; Res1:  (R1+R2+R3) + (I1+I2+I3)i
;; Res2:  (R1-.5R2-.866I2-.5R3+.866I3) + (I1-.5I2+.866R2-.5I3-.866R3)i
;; Res3:  (R1-.5R2+.866I2-.5R3-.866I3) + (I1-.5I2-.866R2-.5I3+.866R3)i
;; Then multiply 2 of the 3 results by twiddle factors.
zr3_3c_djbfft_cmn_preload MACRO
	ENDM
zr3_3c_djbfft_cmn MACRO srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,screg,scinc,maxrpt,L1pt,L1pd
	vmovapd	zmm0, [srcreg+srcoff+d1]	;; R2
	vmovapd	zmm1, [srcreg+srcoff+2*d1]	;; R3
	vaddpd	zmm2, zmm0, zmm1		;; R2 + R3
	vmovapd	zmm3, [srcreg+srcoff+d1+32]	;; I2
	vmovapd	zmm4, [srcreg+srcoff+2*d1+32]	;; I3
	vaddpd	zmm5, zmm3, zmm4		;; I2 + I3
	vsubpd	zmm0, zmm0, zmm1		;; R2 - R3
	vmovapd	zmm1, ZMM_HALF
	vmulpd	zmm6, zmm1, zmm2		;; 0.5 * (R2 + R3)
	vsubpd	zmm3, zmm3, zmm4		;; I2 - I3
	vmulpd	zmm1, zmm1, zmm5		;; 0.5 * (I2 + I3)
	vmovapd	zmm7, [srcreg+srcoff]		;; R1
	vaddpd	zmm2, zmm7, zmm2		;; R1 + R2 + R3 (final R1)
	vmovapd	zmm4, ZMM_P866
	vmulpd	zmm0, zmm4, zmm0		;; 0.866 * (R2 - R3)
	vsubpd	zmm7, zmm7, zmm6		;; (R1-.5R2-.5R3)
	vmulpd	zmm3, zmm4, zmm3		;; 0.866 * (I2 - I3)
	vmovapd	zmm6, [srcreg+srcoff+32]	;; I1
	vaddpd	zmm5, zmm6, zmm5		;; I1 + I2 + I3 (final I1)
	vsubpd	zmm6, zmm6, zmm1		;; (I1-.5I2-.5I3)

	L1prefetchw srcreg+L1pd, L1pt

	vsubpd	zmm1, zmm7, zmm3		;; Final R2
	vaddpd	zmm7, zmm7, zmm3		;; Final R3
	vaddpd	zmm3, zmm6, zmm0		;; Final I2
	vsubpd	zmm6, zmm6, zmm0		;; Final I3

	L1prefetchw srcreg+d1+L1pd, L1pt

no bcast vmovapd zmm0, [screg2]			;; cosine/sine
bcast	vbroadcastsd zmm0, Q [screg2]		;; cosine/sine
	vmulpd	zmm4, zmm1, zmm0		;; A2 = R2 * cosine/sine
	vsubpd	zmm4, zmm4, zmm3		;; A2 = A2 - I2
	vmulpd	zmm3, zmm3, zmm0		;; B2 = I2 * cosine/sine
	vaddpd	zmm3, zmm3, zmm1		;; B2 = B2 + R2

	L1prefetchw srcreg+2*d1+L1pd, L1pt

	vmulpd	zmm1, zmm7, zmm0		;; A3 = R3 * cosine/sine
	vaddpd	zmm1, zmm1, zmm6		;; A3 = A3 + I3
	vmulpd	zmm6, zmm6, zmm0		;; B3 = I3 * cosine/sine
	vsubpd	zmm6, zmm6, zmm7		;; B3 = B3 - R3

no bcast vmovapd zmm0, [screg1]			;; sine
bcast	vbroadcastsd zmm0, Q [screg1]		;; sine
	vmulpd	zmm4, zmm4, zmm0		;; A2 = A2 * sine (new R2)
	vmulpd	zmm3, zmm3, zmm0		;; B2 = B2 * sine (new I2)
	vmulpd	zmm1, zmm1, zmm0		;; A3 = A3 * sine (new R3)
	vmulpd	zmm6, zmm6, zmm0		;; B3 = B3 * sine (new I3)

	zstore	[srcreg], zmm2			;; Save R1
	zstore	[srcreg+32], zmm5		;; Save I1
	zstore	[srcreg+d1], zmm4		;; Save R2
	zstore	[srcreg+d1+32], zmm3		;; Save I2
	zstore	[srcreg+2*d1], zmm1		;; Save R3
	zstore	[srcreg+2*d1+32], zmm6		;; Save I3
	bump	srcreg, srcinc
	bump	screg, scinc
	ENDM

;; 64-bit version

IFDEF X86_64

zr3_3c_djbfft_cmn_preload MACRO
	vmovapd	zmm15, ZMM_HALF
	vmovapd	zmm14, ZMM_P866
	ENDM

zr3_3c_djbfft_cmn MACRO srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,screg,scinc,maxrpt,L1pt,L1pd
	IF maxrpt GE 5 AND maxrpt MOD 5 EQ 0
	zr3_3c_djbfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbfft_unroll 0,noexec,exec,exec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbfft_unroll 1,exec,exec,exec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbfft_unroll 2,exec,exec,exec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbfft_unroll 3,exec,exec,exec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbfft_unroll 4,exec,exec,noexec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbfft_unroll 5,exec,noexec,noexec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	bump	srcreg, 5*srcinc
	bump	screg, 5*scinc
	ELSEIF maxrpt GE 4 AND maxrpt MOD 4 EQ 0
	zr3_3c_djbfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbfft_unroll 0,noexec,exec,exec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbfft_unroll 1,exec,exec,exec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbfft_unroll 2,exec,exec,exec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbfft_unroll 3,exec,exec,noexec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbfft_unroll 4,exec,noexec,noexec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	bump	srcreg, 4*srcinc
	bump	screg, 4*scinc
	ELSEIF maxrpt GE 3 AND maxrpt MOD 3 EQ 0
	zr3_3c_djbfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbfft_unroll 0,noexec,exec,exec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbfft_unroll 1,exec,exec,exec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbfft_unroll 2,exec,exec,noexec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbfft_unroll 3,exec,noexec,noexec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	bump	srcreg, 3*srcinc
	bump	screg, 3*scinc
	ELSEIF maxrpt GE 2 AND maxrpt MOD 2 EQ 0
	zr3_3c_djbfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbfft_unroll 0,noexec,exec,exec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbfft_unroll 1,exec,exec,noexec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbfft_unroll 2,exec,noexec,noexec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	bump	srcreg, 2*srcinc
	bump	screg, 2*scinc
	ELSE
	zr3_3c_djbfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbfft_unroll 0,noexec,exec,noexec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbfft_unroll 1,exec,noexec,noexec,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	bump	srcreg, srcinc
	bump	screg, scinc
	ENDIF
	ENDM

zr3_3c_djbfft_unroll MACRO iter,prev,this,next,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd

;; On first call, init registers

	IF iter EQ -1
y0	TEXTEQU	<zmm0>
y1	TEXTEQU	<zmm1>
y2	TEXTEQU	<zmm2>
y3	TEXTEQU	<zmm3>
y4	TEXTEQU	<zmm4>
y5	TEXTEQU	<zmm5>
y6	TEXTEQU	<zmm6>
y7	TEXTEQU	<zmm7>
y8	TEXTEQU	<zmm8>
y9	TEXTEQU	<zmm9>
y10	TEXTEQU	<zmm10>
y11	TEXTEQU	<zmm11>
y12	TEXTEQU	<zmm12>
y13	TEXTEQU	<zmm13>
	ENDIF

;; On later calls, previous R2,A3,B2,B3,sine will be in y0-4.  This R1,I1,R2+R3,I2+I3,0.5*(R2+R3),0.5*(I2+I3),T2,T4 will be in y5-12.
;; The remaining register is free (zmm14 and zmm15 are preloaded constants 0.866 and 0.5).

this	vsubpd	y9, y5, y9				;; T1 = R1-.5R2-.5R3		; 1-3
prev	vmulpd	y1, y1, y4				;; A3 = A3 * sine (new R3)	; 1-5
this no bcast vmovapd y13, [screg2+iter*scinc]		;; cosine/sine
this bcast vbroadcastsd y13, Q [screg2+iter*scinc]	;; cosine/sine

this	vaddpd	y5, y5, y7				;; R1 + R2 + R3 (final R1)	; 2-4
prev	vmulpd	y2, y2, y4				;; B2 = B2 * sine (new I2)	; 2-6
next	vmovapd	y7, [srcreg+(iter+1)*srcinc+srcoff+d1]	;; R2

this	vsubpd	y10, y6, y10				;; T3 = I1-.5I2-.5I3		; 3-5
prev	vmulpd	y3, y3, y4				;; B3 = B3 * sine (new I3)	; 3-7
next	vmovapd	y4, [srcreg+(iter+1)*srcinc+srcoff+2*d1] ;; R3

this	vaddpd	y6, y6, y8				;; I1 + I2 + I3 (final I1)	; 4-6
this next zloop_unrolled_one

this	vsubpd	y8, y9, y11				;; T1-T2 (final R2)		; 5-7
prev	zstore	[srcreg+(iter-1)*srcinc+d1], y0		;; Save R2			; 5
next	vmovapd	y0, [srcreg+(iter+1)*srcinc+srcoff+d1+32] ;; I2

this	vaddpd	y9, y9, y11				;; T1+T2 (final R3)		; 6-8
this	zstore	[srcreg+iter*srcinc], y5		;; Save R1			; 6
next	vmovapd	y5, [srcreg+(iter+1)*srcinc+srcoff+2*d1+32] ;; I3

this	vaddpd	y11, y10, y12				;; T3+T4 (final I2)		; 7-9
prev	zstore	[srcreg+(iter-1)*srcinc+2*d1], y1	;; Save R3			; 7

this	vsubpd	y10, y10, y12				;; T3-T4 (final I3)		; 8-10
this	vmulpd	y12, y8, y13				;; A2 = R2 * cosine/sine	; 8-12
prev	zstore	[srcreg+(iter-1)*srcinc+d1+32], y2	;; Save I2			; 8
this	L1prefetchw srcreg+iter*srcinc+L1pd, L1pt

next	vaddpd	y2, y7, y4				;; R2 + R3			; 9-11
this	vmulpd	y1, y9, y13				;; A3 = R3 * cosine/sine	; 9-13
this	zstore	[srcreg+iter*srcinc+32], y6		;; Save I1			; 9
this no bcast vmovapd y6, [screg1+iter*scinc]		;; sine
this bcast vbroadcastsd y6, Q [screg1+iter*scinc]	;; sine

next	vsubpd	y7, y7, y4				;; R2 - R3			; 10-12
this	vmulpd	y4, y11, y13				;; B2 = I2 * cosine/sine	; 10-14
prev	zstore	[srcreg+(iter-1)*srcinc+2*d1+32], y3	;; Save I3			; 10

next	vaddpd	y3, y0, y5				;; I2 + I3			; 11-13
this	vmulpd	y13, y10, y13				;; B3 = I3 * cosine/sine	; 11-15

next	vsubpd	y0, y0, y5				;; I2 - I3			; 12-14
next	vmulpd	y5, zmm15, y2				;; 0.5 * (R2 + R3)		; 12-16
this	L1prefetchw srcreg+iter*srcinc+d1+L1pd, L1pt

this	vsubpd	y12, y12, y11				;; A2 = A2 - I2			; 13-15
next	vmulpd	y7, zmm14, y7				;; T4 = 0.866 * (R2 - R3)	; 13-17
next	vmovapd	y11, [srcreg+(iter+1)*srcinc+srcoff]	;; R1

this	vaddpd	y1, y1, y10				;; A3 = A3 + I3			; 14-16
next	vmulpd	y10, zmm15, y3				;; 0.5 * (I2 + I3)		; 14-18


this	vaddpd	y4, y4, y8				;; B2 = B2 + R2			; 15-17
next	vmulpd	y0, zmm14, y0				;; T2 = 0.866 * (I2 - I3)	; 15-19
next	vmovapd	y8, [srcreg+(iter+1)*srcinc+srcoff+32]	;; I1

this	vsubpd	y13, y13, y9				;; B3 = B3 - R3			; 16-18
this	vmulpd	y12, y12, y6				;; A2 = A2 * sine (new R2)	; 16-20
this	L1prefetchw srcreg+iter*srcinc+2*d1+L1pd, L1pt

;; Shuffle register assignments so that next call has R2,A3,B2,B3,sine in y0-4 and next R1,I1,R2+R3,I2+I3,0.5*(R2+R3),0.5*(I2+I3),T2,T4 in y5-12.

ytmp	TEXTEQU	y0
y0	TEXTEQU	y12
y12	TEXTEQU	y7
y7	TEXTEQU	y2
y2	TEXTEQU	y4
y4	TEXTEQU	y6
y6	TEXTEQU	y8
y8	TEXTEQU	y3
y3	TEXTEQU	y13
y13	TEXTEQU	y9
y9	TEXTEQU	y5
y5	TEXTEQU	y11
y11	TEXTEQU ytmp

	ENDM


;; Haswell FMA3 version

IF (@INSTR(,%zarch,<FMA3>) NE 0)

zr3_3c_djbfft_cmn_preload MACRO
	vmovapd	zmm15, ZMM_HALF
	vmovapd	zmm14, ZMM_P866
	vmovapd	zmm13, ZMM_ONE
	ENDM

;; uops = 6 loads, 6 stores, 2 s/c loads, 4 muls, 16 fmas, 8 register copies = 42 uops / 4 = 10.5 clocks (above our 10 clock optimum)
;; Timed at just over 11 clocks.
zr3_3c_djbfft_unroll MACRO iter,prev,this,next,srcreg,srcinc,srcoff,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd

;; On first call, init registers

	IF iter EQ -1
y0	TEXTEQU	<zmm0>
y1	TEXTEQU	<zmm1>
y2	TEXTEQU	<zmm2>
y3	TEXTEQU	<zmm3>
y4	TEXTEQU	<zmm4>
y5	TEXTEQU	<zmm5>
y6	TEXTEQU	<zmm6>
y7	TEXTEQU	<zmm7>
y8	TEXTEQU	<zmm8>
y9	TEXTEQU	<zmm9>
y10	TEXTEQU	<zmm10>
y11	TEXTEQU	<zmm11>
y12	TEXTEQU	<zmm12>
	ENDIF

;; On later calls, this R1,I1 are in y0-1.  Previous R2,I2,R3,I3,c/s will be in y2-6.  This T1,T3,R2-R3,I2-I3 will be in y7-10.
;; The remaining registers are free (zmm13, zmm14, and zmm15 are preloaded constants 1.0, 0.866, and 0.5 respectively). 

next	vmovapd	y11, [srcreg+(iter+1)*srcinc+srcoff+d1]	;; R2
prev	zfmsubpd y12, y2, y6, y3			;; A2 = R2 * cosine/sine - I2		; 1-5
prev	zfmaddpd y3, y3, y6, y2				;; B2 = I2 * cosine/sine + R2		; 1-5
this	L1prefetchw srcreg+iter*srcinc+L1pd, L1pt

next	vmovapd	y2, [srcreg+(iter+1)*srcinc+srcoff+2*d1] ;; R3
this	zstore	[srcreg+iter*srcinc], y0		;; Save R1				; 4
prev	zfmaddpd y0, y4, y6, y5				;; A3 = R3 * cosine/sine + I3		; 2-6
prev	zfmsubpd y5, y5, y6, y4				;; B3 = I3 * cosine/sine - R3		; 2-6

next	vmovapd	y6, [srcreg+(iter+1)*srcinc+srcoff+d1+32] ;; I2
next	zfmaddpd y4, y11, zmm13, y2			;; R2 + R3				; 3-7
next	zfmsubpd y11, y11, zmm13, y2			;; R2 - R3				; 3-7

next	vmovapd	y2, [srcreg+(iter+1)*srcinc+srcoff+2*d1+32] ;; I3
this	zstore	[srcreg+iter*srcinc+32], y1		;; Save I1				; 5
next	zfmaddpd y1, y6, zmm13, y2			;; I2 + I3				; 4-8
next	zfmsubpd y6, y6, zmm13, y2			;; I2 - I3				; 4-8

this	zfnmaddpd y2, zmm14, y10, y7, 1			;; T1 - 0.866 * (I2-I3) (newer R2)	; 5-9
this	zfmaddpd y10, zmm14, y10, y7			;; T1 + 0.866 * (I2-I3) (newer R3)	; 5-9
this	L1prefetchw srcreg+iter*srcinc+d1+L1pd, L1pt

this	zfmaddpd y7, zmm14, y9, y8, 1			;; T3 + 0.866 * (R2-R3) (newer I2)	; 6-10
this	zfnmaddpd y9, zmm14, y9, y8			;; T3 - 0.866 * (R2-R3) (newer I3)	; 6-10
this	L1prefetchw srcreg+iter*srcinc+2*d1+L1pd, L1pt

prev no bcast vmovapd y8, [screg1+(iter-1)*scinc]	;; sine
prev bcast vbroadcastsd y8, Q [screg1+(iter-1)*scinc]	;; sine
prev	vmulpd	y12, y12, y8				;; A2 = A2 * sine (final R2)		; 7-11
prev	vmulpd	y3, y3, y8				;; B2 = B2 * sine (final I2)		; 7-11
prev	zstore	[srcreg+(iter-1)*srcinc+d1], y12	;; Save R2				; 12

next	vmovapd	y12, [srcreg+(iter+1)*srcinc+srcoff]	;; R1
prev	vmulpd	y0, y0, y8				;; A3 = A3 * sine (final R3)		; 8-12
prev	vmulpd	y5, y5, y8				;; B3 = B3 * sine (final I3)		; 8-12
next	vmovapd	y8, [srcreg+(iter+1)*srcinc+srcoff+32]	;; I1
prev	zstore	[srcreg+(iter-1)*srcinc+d1+32], y3	;; Save I2				; 12

next	zfmaddpd y3, y12, zmm13, y4			;; R1 + (R2+R3) (newer & final R1)	; 9-13
next	zfnmaddpd y4, zmm15, y4, y12			;; T1 = R1 - 0.5 * (R2+R3)		; 9-13
this next zloop_unrolled_one

this no bcast vmovapd y12, [screg2+iter*scinc]		;; cosine/sine
this bcast vbroadcastsd y12, Q [screg2+iter*scinc]	;; cosine/sine
prev	zstore	[srcreg+(iter-1)*srcinc+2*d1], y0	;; Save R3				; 13
next	zfmaddpd y0, y8, zmm13, y1			;; I1 + (I2+I3) (newer & final I1)	; 10-14
next	zfnmaddpd y1, zmm15, y1, y8			;; T3 = I1 - 0.5 * (I2+I3)		; 10-14
prev	zstore	[srcreg+(iter-1)*srcinc+2*d1+32], y5	;; Save I3				; 13

;; Shuffle register assignments so that next R1,I1 are in y0-1, this R2,I2,R3,I3,c/s in y2-6, and next T1,T3,R2-R3,I2-I3 in y7-10.

ytmp	TEXTEQU	y0
y0	TEXTEQU	y3
y3	TEXTEQU	y7
y7	TEXTEQU	y4
y4	TEXTEQU	y10
y10	TEXTEQU	y6
y6	TEXTEQU	y12
y12	TEXTEQU	y5
y5	TEXTEQU	y9
y9	TEXTEQU	y11
y11	TEXTEQU	y8
y8	TEXTEQU	y1
y1	TEXTEQU ytmp

	ENDM

ENDIF

ENDIF

;;
;; ************************************* three-complex-djbunfft variants ******************************************
;;

;; The standard version
zr3_3cl_three_complex_djbunfft_preload MACRO
	zr3_3c_djbunfft_cmn_preload
	ENDM
zr3_3cl_three_complex_djbunfft MACRO srcreg,srcinc,d1,screg,scinc,maxrpt,L1pt,L1pd
	zr3_3c_djbunfft_cmn srcreg,srcinc,d1,noexec,screg,screg+32,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

; Like standard but uses a six-reals sin/cos table
zr3_r3cl_three_complex_djbunfft_preload MACRO
	zr3_3c_djbunfft_cmn_preload
	ENDM
zr3_r3cl_three_complex_djbunfft MACRO srcreg,srcinc,d1,screg,scinc,maxrpt,L1pt,L1pd
	zr3_3c_djbunfft_cmn srcreg,srcinc,d1,noexec,screg+64,screg+96,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

; Like the standard version except vbroadcastsd is used to reduce sin/cos data
zr3_b3cl_three_complex_djbunfft_preload MACRO
	zr3_3c_djbunfft_cmn_preload
	ENDM
zr3_b3cl_three_complex_djbunfft MACRO srcreg,srcinc,d1,screg,scinc,maxrpt,L1pt,L1pd
	zr3_3c_djbunfft_cmn srcreg,srcinc,d1,exec,screg,screg+8,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

; Like b3cl but extracts the sin/cos data to broadcasts from
; the six-real/three_complex sin/cos table
zr3_rb3cl_three_complex_djbunfft_preload MACRO
	zr3_3c_djbunfft_cmn_preload
	ENDM
zr3_rb3cl_three_complex_djbunfft MACRO srcreg,srcinc,d1,screg,scinc,maxrpt,L1pt,L1pd
	zr3_3c_djbunfft_cmn srcreg,srcinc,d1,exec,screg+8,screg+48,screg,scinc,maxrpt,L1pt,L1pd
	ENDM

;; Do a 3-complex inverse FFT.  The input values are R1+R2i, R3+R4i, R5+R6i
;; First we apply twiddle factors to 2 of the 3 input numbers.
;; A 3-complex inverse FFT is:
;; Res1:  (R1+R2+R3) + (I1+I2+I3)i
;; Res2:  (R1-.5R2+.866I2-.5R3-.866I3) + (I1-.5I2-.866R2-.5I3+.866R3)i
;; Res3:  (R1-.5R2-.866I2-.5R3+.866I3) + (I1-.5I2+.866R2-.5I3-.866R3)i

zr3_3c_djbunfft_cmn_preload MACRO
	ENDM
zr3_3c_djbunfft_cmn MACRO srcreg,srcinc,d1,bcast,screg1,screg2,screg,scinc,maxrpt,L1pt,L1pd
no bcast vmovapd zmm0, [screg2]		;; cosine/sine
bcast	vbroadcastsd zmm0, Q [screg2]	;; cosine/sine
	vmovapd	zmm1, [srcreg+d1]	;; R2
	vmulpd	zmm2, zmm1, zmm0	;; A2 = R2 * cosine/sine
	vmovapd	zmm3, [srcreg+2*d1]	;; R3
	vmulpd	zmm4, zmm3, zmm0	;; A3 = R3 * cosine/sine
	vmovapd	zmm5, [srcreg+d1+32]	;; I2
	vaddpd	zmm2, zmm2, zmm5	;; A2 = A2 + I2
	vmovapd	zmm6, [srcreg+2*d1+32]	;; I3
	vsubpd	zmm4, zmm4, zmm6	;; A3 = A3 - I3
	vmulpd	zmm5, zmm5, zmm0	;; B2 = I2 * cosine/sine
	vmulpd	zmm6, zmm6, zmm0	;; B3 = I3 * cosine/sine
	vsubpd	zmm5, zmm5, zmm1	;; B2 = B2 - R2
	vaddpd	zmm6, zmm6, zmm3	;; B3 = B3 + R3
no bcast vmovapd zmm0, [screg1]		;; sine
bcast	vbroadcastsd zmm0, Q [screg1]	;; sine
	vmulpd	zmm2, zmm2, zmm0	;; A2 = A2 * sine (final R2)
	vmulpd	zmm4, zmm4, zmm0	;; A3 = A3 * sine (final R3)
	vmulpd	zmm5, zmm5, zmm0	;; B2 = B2 * sine (final I2)
	vmulpd	zmm6, zmm6, zmm0	;; B3 = B3 * sine (final I3)

	L1prefetchw srcreg+L1pd, L1pt

	vsubpd	zmm0, zmm2, zmm4	;; R2 - R3
	vaddpd	zmm2, zmm2, zmm4	;; R2 + R3
	vmovapd	zmm7, ZMM_P866
	vmulpd	zmm0, zmm7, zmm0	;; 0.866 * (R2 - R3)
	vsubpd	zmm1, zmm5, zmm6	;; I2 - I3
	vaddpd	zmm5, zmm5, zmm6	;; I2 + I3

	L1prefetchw srcreg+d1+L1pd, L1pt

	vmulpd	zmm1, zmm7, zmm1	;; 0.866 * (I2 - I3)
	vmovapd	zmm7, ZMM_HALF
	vmulpd	zmm4, zmm7, zmm2	;; 0.5 * (R2 + R3)
	vmovapd	zmm6, [srcreg]		;; R1
	vaddpd	zmm2, zmm6, zmm2	;; R1 + R2 + R3 (final R1)
	vsubpd	zmm6, zmm6, zmm4	;; (R1-.5R2-.5R3)

	L1prefetchw srcreg+2*d1+L1pd, L1pt

	vmulpd	zmm7, zmm7, zmm5	;; 0.5 * (I2 + I3)
	vmovapd	zmm3, [srcreg+32]	;; I1
	vaddpd	zmm5, zmm3, zmm5	;; I1 + I2 + I3 (final I1)
	vsubpd	zmm3, zmm3, zmm7	;; (I1-.5I2-.5I3)
	vsubpd	zmm7, zmm6, zmm1	;; Final R3
	vaddpd	zmm6, zmm6, zmm1	;; Final R2
	vsubpd	zmm1, zmm3, zmm0	;; Final I2
	vaddpd	zmm3, zmm3, zmm0	;; Final I3

	zstore	[srcreg], zmm2		;; Save R1
	zstore	[srcreg+32], zmm5	;; Save I1
	zstore	[srcreg+d1], zmm6	;; Save R2
	zstore	[srcreg+d1+32], zmm1	;; Save I2
	zstore	[srcreg+2*d1], zmm7	;; Save R3
	zstore	[srcreg+2*d1+32], zmm3	;; Save I3
	bump	srcreg, srcinc
	bump	screg, scinc
	ENDM

;; 64-bit version

IFDEF X86_64

zr3_3c_djbunfft_cmn_preload MACRO
	vmovapd	zmm15, ZMM_HALF
	vmovapd	zmm14, ZMM_P866
	ENDM

zr3_3c_djbunfft_cmn MACRO srcreg,srcinc,d1,bcast,screg1,screg2,screg,scinc,maxrpt,L1pt,L1pd
	IF maxrpt GE 5 AND maxrpt MOD 5 EQ 0
	zr3_3c_djbunfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbunfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbunfft_unroll 1,exec,exec,exec,srcreg,srcinc,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbunfft_unroll 2,exec,exec,exec,srcreg,srcinc,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbunfft_unroll 3,exec,exec,exec,srcreg,srcinc,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbunfft_unroll 4,exec,exec,noexec,srcreg,srcinc,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbunfft_unroll 5,exec,noexec,noexec,srcreg,srcinc,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	bump	srcreg, 5*srcinc
	bump	screg, 5*scinc
	ELSEIF maxrpt GE 4 AND maxrpt MOD 4 EQ 0
	zr3_3c_djbunfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbunfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbunfft_unroll 1,exec,exec,exec,srcreg,srcinc,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbunfft_unroll 2,exec,exec,exec,srcreg,srcinc,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbunfft_unroll 3,exec,exec,noexec,srcreg,srcinc,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbunfft_unroll 4,exec,noexec,noexec,srcreg,srcinc,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	bump	srcreg, 4*srcinc
	bump	screg, 4*scinc
	ELSEIF maxrpt GE 3 AND maxrpt MOD 3 EQ 0
	zr3_3c_djbunfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbunfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbunfft_unroll 1,exec,exec,exec,srcreg,srcinc,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbunfft_unroll 2,exec,exec,noexec,srcreg,srcinc,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbunfft_unroll 3,exec,noexec,noexec,srcreg,srcinc,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	bump	srcreg, 3*srcinc
	bump	screg, 3*scinc
	ELSEIF maxrpt GE 2 AND maxrpt MOD 2 EQ 0
	zr3_3c_djbunfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbunfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbunfft_unroll 1,exec,exec,noexec,srcreg,srcinc,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbunfft_unroll 2,exec,noexec,noexec,srcreg,srcinc,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	bump	srcreg, 2*srcinc
	bump	screg, 2*scinc
	ELSE
	zr3_3c_djbunfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbunfft_unroll 0,noexec,exec,noexec,srcreg,srcinc,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	zr3_3c_djbunfft_unroll 1,exec,noexec,noexec,srcreg,srcinc,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd
	bump	srcreg, srcinc
	bump	screg, scinc
	ENDIF
	ENDM

zr3_3c_djbunfft_unroll MACRO iter,prev,this,next,srcreg,srcinc,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd

;; On first call, init registers

	IF iter EQ -1
y0	TEXTEQU	<zmm0>
y1	TEXTEQU	<zmm1>
y2	TEXTEQU	<zmm2>
y3	TEXTEQU	<zmm3>
y4	TEXTEQU	<zmm4>
y5	TEXTEQU	<zmm5>
y6	TEXTEQU	<zmm6>
y7	TEXTEQU	<zmm7>
y8	TEXTEQU	<zmm8>
y9	TEXTEQU	<zmm9>
y10	TEXTEQU	<zmm10>
y11	TEXTEQU	<zmm11>
y12	TEXTEQU	<zmm12>
y13	TEXTEQU	<zmm13>
	ENDIF

;; On later calls, previous R1, I1, T1, T2, T3, T4 will be in y0-5.  This (R2,R3,I2,I3)/sine will be in y6-9.
;; The remaining 4 register are free (zmm14 and zmm15 are preloaded constants 0.866 and 0.5).

prev	vsubpd	y10, y2, y3				;; PREVIOUS T1-T2 (final R3)		; 1-3
this no bcast vmovapd y11, [screg1+iter*scinc]		;; sine
this bcast vbroadcastsd y11, Q [screg1+iter*scinc]	;; sine
this	vmulpd	y12, zmm15, y11				;; 0.5 * sine				; 1-5
prev	zstore	[srcreg+(iter-1)*srcinc], y0		;; Save R1				; 1

prev	vaddpd	y2, y2, y3				;; PREVIOUS T1+T2 (final R2)		; 2-4
next no bcast vmovapd y0, [screg2+(iter+1)*scinc]	;; NEXT cosine/sine
next bcast vbroadcastsd y0, Q [screg2+(iter+1)*scinc]	;; NEXT cosine/sine

prev	vsubpd	y13, y4, y5				;; PREVIOUS T3-T4 (final I2)		; 3-5
next	vmovapd	y3, [srcreg+(iter+1)*srcinc+d1]		;; NEXT R2
prev	zstore	[srcreg+(iter-1)*srcinc+32], y1		;; Save I1				; 3
next	vmulpd	y1, y3, y0				;; NEXT A2 = R2 * cosine/sine		; 3-7

prev	vaddpd	y4, y4, y5				;; PREVIOUS T3+T4 (final I3)		; 4-6
next	vmovapd	y5, [srcreg+(iter+1)*srcinc+2*d1]	;; NEXT R3
prev	zstore	[srcreg+(iter-1)*srcinc+2*d1], y10	;; PREVIOUS Save R3			; 4
next	vmulpd	y10, y5, y0				;; NEXT A3 = R3 * cosine/sine		; 4-8

prev	zstore	[srcreg+(iter-1)*srcinc+d1], y2		;; PREVIOUS Save R2			; 5
this	vaddpd	y2, y6, y7				;; R2/sine + R3/sine			; 5-7
prev	zstore	[srcreg+(iter-1)*srcinc+d1+32], y13	;; PREVIOUS Save I2			; 6
next	vmovapd	y13, [srcreg+(iter+1)*srcinc+d1+32]	;; NEXT I2
prev	zstore	[srcreg+(iter-1)*srcinc+2*d1+32], y4	;; PREVIOUS Save I3			; 7
next	vmulpd	y4, y13, y0				;; NEXT B2 = I2 * cosine/sine		; 5-9

this	vsubpd	y6, y6, y7				;; R2/sine - R3/sine			; 6-8
next	vmovapd	y7, [srcreg+(iter+1)*srcinc+2*d1+32]	;; NEXT I3
next	vmulpd	y0, y7, y0				;; NEXT B3 = I3 * cosine/sine		; 6-10

next	vaddpd	y1, y1, y13				;; NEXT A2 = A2 + I2 (new R2/sine)	; 9-11
this	vaddpd	y13, y8, y9				;; I2/sine + I3/sine			; 7-9
this	vsubpd	y8, y8, y9				;; I2/sine - I3/sine			; 8-10
this	vmulpd	y9, zmm14, y11				;; 0.866 * sine				; 7-11
this	L1prefetchw srcreg+iter*srcinc+L1pd, L1pt

next	vsubpd	y10, y10, y7				;; NEXT A3 = A3 - I3 (new R3/sine)	; 10-12
this	vmulpd	y7, y12, y2				;; 0.5 * (R2 + R3)			; 8-12

this	vmulpd	y2, y11, y2				;; R2 + R3				; 9-13
this next zloop_unrolled_one

this	vmulpd	y12, y12, y13				;; 0.5 * (I2 + I3)			; 10-14

next	vsubpd	y4, y4, y3				;; NEXT B2 = B2 - R2 (new I2/sine)	; 11-13
this	vmulpd	y13, y11, y13				;; I2 + I3				; 11-15
this	vmovapd	y3, [srcreg+iter*srcinc]		;; R1
this	L1prefetchw srcreg+iter*srcinc+d1+L1pd, L1pt

next	vaddpd	y0, y0, y5				;; NEXT B3 = B3 + R3 (new I3/sine)	; 12-14
this	vmulpd	y8, y9, y8				;; T2 = 0.866 * (I2 - I3)		; 12-16
this	vmovapd	y11, [srcreg+iter*srcinc+32]		;; I1

this	vsubpd	y7, y3, y7				;; T1 = R1-.5R2-.5R3			; 13-15
this	vmulpd	y6, y9, y6				;; T4 = 0.866 * (R2 - R3)		; 13-17

this	vaddpd	y3, y3, y2				;; R1 + R2 + R3 (final R1)		; 14-16

this	vsubpd	y12, y11, y12				;; T3 = I1-.5I2-.5I3			; 15-17
this	L1prefetchw srcreg+iter*srcinc+2*d1+L1pd, L1pt

this	vaddpd	y11, y11, y13				;; I1 + I2 + I3 (final I1)		; 16-18

;; Shuffle register assignments so that next call has R1, I1, T1, T2, T3, T4 in y0-5 and next (R2,R3,I2,I3)/sine in y6-9.

ytmp	TEXTEQU	y0
y0	TEXTEQU	y3
y3	TEXTEQU	y8
y8	TEXTEQU	y4
y4	TEXTEQU	y12
y12	TEXTEQU	y2
y2	TEXTEQU	y7
y7	TEXTEQU	y10
y10	TEXTEQU	y5
y5	TEXTEQU	y6
y6	TEXTEQU	y1
y1	TEXTEQU	y11
y11	TEXTEQU y9
y9	TEXTEQU	ytmp

	ENDM

;; Haswell FMA3 version

IF (@INSTR(,%zarch,<FMA3>) NE 0)

zr3_3c_djbunfft_cmn_preload MACRO
	vmovapd	zmm15, ZMM_HALF
	vmovapd	zmm14, ZMM_ONE
	ENDM

;; uops = 6 loads, 6 stores, 2 s/c loads, 1 constant load, 2 muls, 16 fmas, 8 register copies = 41 uops / 4 = 10.25 clocks (above our 9 clock optimum)
;; Timed at 10.5 clocks.
zr3_3c_djbunfft_unroll MACRO iter,prev,this,next,srcreg,srcinc,d1,bcast,screg1,screg2,screg,scinc,L1pt,L1pd

;; On first call, init registers

	IF iter EQ -1
y0	TEXTEQU	<zmm0>
y1	TEXTEQU	<zmm1>
y2	TEXTEQU	<zmm2>
y3	TEXTEQU	<zmm3>
y4	TEXTEQU	<zmm4>
y5	TEXTEQU	<zmm5>
y6	TEXTEQU	<zmm6>
y7	TEXTEQU	<zmm7>
y8	TEXTEQU	<zmm8>
y9	TEXTEQU	<zmm9>
y10	TEXTEQU	<zmm10>
y11	TEXTEQU	<zmm11>
y12	TEXTEQU	<zmm12>
y13	TEXTEQU	<zmm13>
	ENDIF

;; On later calls, previous R1,I1,T1,T3,(R2-R3)/sine,(I2-I3)/sine,0.866*sine will be in y0-6.  This R2/sine,R3/sine,I2/sine,I3/sine,sine will be in y7-11.
;; The remaining 4 register are free (zmm14 and zmm15 are preloaded constants 0.866 and 0.5).

this	vmulpd	y12, zmm15, y11				;; 0.5 * sine					; 1-5
this	vmulpd	y13, y11, ZMM_P866			;; 0.866 * sine					; 1-5
prev	zstore	[srcreg+(iter-1)*srcinc], y0		;; Save R1					; 4
this	L1prefetchw srcreg+iter*srcinc+L1pd, L1pt

this	zfmaddpd y0, y7, zmm14, y8			;; R2/sine + R3/sine				; 2-6
this	zfmsubpd y7, y7, zmm14, y8			;; R2/sine - R3/sine				; 2-6
next no bcast vmovapd y8, [screg2+(iter+1)*scinc]	;; cosine/sine
next bcast vbroadcastsd y8, Q [screg2+(iter+1)*scinc]	;; cosine/sine
prev	zstore	[srcreg+(iter-1)*srcinc+32], y1		;; Save I1					; 5

this	zfmaddpd y1, y9, zmm14, y10			;; I2/sine + I3/sine				; 3-7
this	zfmsubpd y9, y9, zmm14, y10			;; I2/sine - I3/sine				; 3-7
this	L1prefetchw srcreg+iter*srcinc+d1+L1pd, L1pt

prev	zfmaddpd y10, y6, y5, y2			;; T1 + 0.866*sine * (I2-I3)/sine (final R2)	; 4-8
prev	zfnmaddpd y5, y6, y5, y2			;; T1 - 0.866*sine * (I2-I3)/sine (final R3)	; 4-8
next	vmovapd	y2, [srcreg+(iter+1)*srcinc+d1]		;; R2
prev	zstore	[srcreg+(iter-1)*srcinc+d1], y10	;; Save R2					; 9

prev	zfmaddpd y10, y6, y4, y3			;; T3 + 0.866*sine * (R2-R3)/sine (final I3)	; 5-9
prev	zfnmaddpd y6, y6, y4, y3			;; T3 - 0.866*sine * (R2-R3)/sine (final I2)	; 5-9
next	vmovapd	y4, [srcreg+(iter+1)*srcinc+d1+32]	;; I2

next	zfmaddpd y3, y2, y8, y4				;; R2 * cosine/sine + I2 (new R2/sine)		; 6-10
prev	zstore	[srcreg+(iter-1)*srcinc+2*d1], y5	;; Save R3					; 9
next	vmovapd	y5, [srcreg+(iter+1)*srcinc+2*d1]	;; R3
prev	zstore	[srcreg+(iter-1)*srcinc+2*d1+32], y10	;; Save I3					; 10
next	vmovapd	y10, [srcreg+(iter+1)*srcinc+2*d1+32]	;; I3
prev	zstore	[srcreg+(iter-1)*srcinc+d1+32], y6	;; Save I2					; 10
next	zfmsubpd y6, y5, y8, y10			;; R3 * cosine/sine - I3 (new R3/sine)		; 6-10

next	zfmsubpd y4, y4, y8, y2				;; I2 * cosine/sine - R2 (new I2/sine)		; 7-11
next	zfmaddpd y10, y10, y8, y5			;; I3 * cosine/sine + R3 (new I3/sine)		; 7-11
this	vmovapd	y2, [srcreg+iter*srcinc]		;; R1
this	L1prefetchw srcreg+iter*srcinc+2*d1+L1pd, L1pt

this	zfnmaddpd y8, y12, y0, y2			;; T1 = R1 - 0.5*sine * (R2+R3)/sine		; 8-12
this	zfmaddpd y0, y11, y0, y2			;; R1 + sine * (R2+R3)/sine (final R1)		; 8-12
this	vmovapd	y5, [srcreg+iter*srcinc+32]		;; I1
this next zloop_unrolled_one

this	zfnmaddpd y12, y12, y1, y5			;; T3 = I1 - 0.5*sine * (I2+I3)/sine		; 9-13
this	zfmaddpd y11, y11, y1, y5			;; I1 + sine * (I2+I3)/sine (final I1)		; 9-13
next no bcast vmovapd y2, [screg1+(iter+1)*scinc]	;; sine
next bcast vbroadcastsd y2, Q [screg1+(iter+1)*scinc]	;; sine

;; Shuffle register assignments so that next call has R1,I1,T1,T3,(R2-R3)/sine,(I2-I3)/sine,0.866*sine in y0-6,
;; and this R2/sine,R3/sine,I2/sine,I3/sine,sine in y7-11.

ytmp	TEXTEQU	y1
y1	TEXTEQU	y11
y11	TEXTEQU	y2
y2	TEXTEQU	y8
y8	TEXTEQU	y6
y6	TEXTEQU	y13
y13	TEXTEQU	y5
y5	TEXTEQU	y9
y9	TEXTEQU	y4
y4	TEXTEQU	y7
y7	TEXTEQU	y3
y3	TEXTEQU	y12
y12	TEXTEQU	ytmp

	ENDM

ENDIF

ENDIF


;;
;; ************************************* six-reals-fft variants ******************************************
;;

; R1 #1 = R1 + R3 + R5
; R1 #2 = R2 + R4 + R6
; R2 = R1 - R4 + 0.5 * (R2 - R3 - R5 + R6)
; I2 = 0.866 * (R2 + R3 - R5 - R6)
; R3 = R1 + R4 - 0.5 * (R2 + R3 + R5 + R6)
; I3 = 0.866 * (R2 - R3 + R5 - R6)

;; The standard version
zr3_3cl_six_reals_fft_preload MACRO
	zr3_6r_fft_cmn_preload
	ENDM
zr3_3cl_six_reals_fft MACRO srcreg,srcinc,d1,screg,scinc,maxrpt,L1pt,L1pd
	zr3_6r_fft_cmn srcreg,srcinc,0,d1,screg,screg+64,screg,scinc,,,maxrpt,L1pt,L1pd
	ENDM

; Like the standard version but uses two sin/cos ptrs
zr3_3cl_2sc_six_reals_fft_preload MACRO
	zr3_6r_fft_cmn_preload
	ENDM
zr3_3cl_2sc_six_reals_fft MACRO srcreg,srcinc,d1,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	zr3_6r_fft_cmn srcreg,srcinc,0,d1,screg2,screg1,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	ENDM

; Like the standard version but the first sin/cos value can be used in a three-complex macro at the same FFT level
zr3_3cl_csc_six_reals_fft_preload MACRO
	zr3_6r_fft_cmn_preload
	ENDM
zr3_3cl_csc_six_reals_fft MACRO srcreg,srcinc,d1,screg,scinc,maxrpt,L1pt,L1pd
	zr3_6r_fft_cmn srcreg,srcinc,0,d1,screg+64,screg,screg,scinc,,,maxrpt,L1pt,L1pd
	ENDM

; Like the standard version but indexes into source using rbx
zr3_f3cl_six_reals_fft_preload MACRO
	zr3_6r_fft_cmn_preload
	ENDM
zr3_f3cl_six_reals_fft MACRO srcreg,srcinc,d1,screg,scinc,maxrpt,L1pt,L1pd
	zr3_6r_fft_cmn srcreg,srcinc,rbx,d1,screg,screg+64,screg,scinc,,,maxrpt,L1pt,L1pd
	ENDM

; Like f3cl but uses 2 sin/cos ptrs
zr3_f3cl_2sc_six_reals_fft_preload MACRO
	zr3_6r_fft_cmn_preload
	ENDM
zr3_f3cl_2sc_six_reals_fft MACRO srcreg,srcinc,d1,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	zr3_6r_fft_cmn srcreg,srcinc,rbx,d1,screg2,screg1,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	ENDM

zr3_6r_fft_cmn_preload MACRO
	ENDM
zr3_6r_fft_cmn MACRO srcreg,srcinc,srcoff,d1,screg1,screg2,scregA,scincA,scregB,scincB,maxrpt,L1pt,L1pd
	vmovapd	zmm4, [srcreg+srcoff+d1]	;; R2
	vmovapd	zmm3, [srcreg+srcoff+2*d1+32]	;; R6
	vaddpd	zmm5, zmm4, zmm3		;; R2 + R6
	vsubpd	zmm4, zmm4, zmm3		;; R2 - R6
	vmovapd	zmm6, [srcreg+srcoff+2*d1]	;; R3
	vmovapd	zmm7, [srcreg+srcoff+d1+32]	;; R5
	vaddpd	zmm0, zmm6, zmm7		;; R3 + R5
	vsubpd	zmm6, zmm6, zmm7		;; R3 - R5

	L1prefetchw srcreg+L1pd, L1pt

	vmovapd	zmm7, ZMM_HALF
	vmulpd	zmm1, zmm7, zmm5		;; 0.5 * (R2 + R6)
	vmovapd	zmm2, [srcreg+srcoff+32]	;; R4
	vaddpd	zmm5, zmm2, zmm5		;; final R1 #2 = R4 + R2 + R6
	vmulpd	zmm7, zmm7, zmm0		;; 0.5 * (R3 + R5)
	vmovapd	zmm3, ZMM_P866
	vmulpd	zmm4, zmm3, zmm4		;; X = 0.866 * (R2 - R6)
	vmulpd	zmm6, zmm3, zmm6		;; Y = 0.866 * (R3 - R5)
	vmovapd	zmm3, [srcreg+srcoff]		;; R1
	vaddpd	zmm0, zmm3, zmm0		;; final R1 #1 = R1 + R3 + R5
	vsubpd	zmm2, zmm2, zmm1		;; B = R4 - 0.5 * (R2 + R6)
	vsubpd	zmm3, zmm3, zmm7		;; A = R1 - 0.5 * (R3 + R5)

	L1prefetchw srcreg+d1+L1pd, L1pt

	vsubpd	zmm7, zmm4, zmm6		;; X - Y (final I3)
	vaddpd	zmm1, zmm4, zmm6		;; X + Y (final I2)
	vaddpd	zmm6, zmm3, zmm2		;; A + B (final R3)
	vsubpd	zmm3, zmm3, zmm2		;; A - B (final R2)

	L1prefetchw srcreg+2*d1+L1pd, L1pt

	vmovapd	zmm2, [screg2+32]		;; cosine/sine
	vmulpd	zmm4, zmm7, zmm2		;; B3 = I3 * cosine/sine
	vaddpd	zmm4, zmm4, zmm6		;; B3 = B3 + R3
	vmulpd	zmm6, zmm6, zmm2		;; A3 = R3 * cosine/sine
	vsubpd	zmm6, zmm6, zmm7		;; A3 = A3 - I3

	vmovapd	zmm2, [screg1+32]		;; cosine/sine
	vmulpd	zmm7, zmm1, zmm2		;; B2 = I2 * cosine/sine
	vaddpd	zmm7, zmm7, zmm3		;; B2 = B2 + R2
	vmulpd	zmm3, zmm3, zmm2		;; A2 = R2 * cosine/sine
	vsubpd	zmm3, zmm3, zmm1		;; A2 = A2 - I2

	vmovapd	zmm1, [screg2]			;; sine
	vmulpd	zmm4, zmm4, zmm1		;; B3 = B3 * sine (new I3)
	vmulpd	zmm6, zmm6, zmm1		;; A3 = A3 * sine (new R3)
	vmovapd	zmm1, [screg1]			;; sine
	vmulpd	zmm7, zmm7, zmm1		;; B2 = B2 * sine (new I2)
	vmulpd	zmm3, zmm3, zmm1		;; A2 = A2 * sine (new R2)

	zstore	[srcreg], zmm0			;; Save R1
	zstore	[srcreg+32], zmm5		;; Save I1
	zstore	[srcreg+d1], zmm3		;; Save R2
	zstore	[srcreg+d1+32], zmm7		;; Save I2
	zstore	[srcreg+2*d1], zmm6		;; Save R3
	zstore	[srcreg+2*d1+32], zmm4		;; Save I3
	bump	srcreg, srcinc
	bump	scregA, scincA
	bump	scregB, scincB
	ENDM

;; 64-bit version

IFDEF X86_64

zr3_6r_fft_cmn_preload MACRO
	vmovapd	zmm14, ZMM_HALF
	vmovapd	zmm15, ZMM_P866
	ENDM
zr3_6r_fft_cmn MACRO srcreg,srcinc,srcoff,d1,screg1,screg2,scregA,scincA,scregB,scincB,maxrpt,L1pt,L1pd
	IF maxrpt GE 4 AND maxrpt MOD 4 EQ 0
	zr3_6r_fft_unroll -1,noexec,noexec,exec,srcreg,srcinc,srcoff,d1,screg1,screg2,scregA,scincA,L1pt,L1pd
	zr3_6r_fft_unroll 0,noexec,exec,exec,srcreg,srcinc,srcoff,d1,screg1,screg2,scregA,scincA,L1pt,L1pd
	zr3_6r_fft_unroll 1,exec,exec,exec,srcreg,srcinc,srcoff,d1,screg1,screg2,scregA,scincA,L1pt,L1pd
	zr3_6r_fft_unroll 2,exec,exec,exec,srcreg,srcinc,srcoff,d1,screg1,screg2,scregA,scincA,L1pt,L1pd
	zr3_6r_fft_unroll 3,exec,exec,noexec,srcreg,srcinc,srcoff,d1,screg1,screg2,scregA,scincA,L1pt,L1pd
	zr3_6r_fft_unroll 4,exec,noexec,noexec,srcreg,srcinc,srcoff,d1,screg1,screg2,scregA,scincA,L1pt,L1pd
	bump	srcreg, 4*srcinc
	bump	scregA, 4*scincA
	bump	scregB, 4*scincB
	ELSEIF maxrpt GE 3 AND maxrpt MOD 3 EQ 0
	zr3_6r_fft_unroll -1,noexec,noexec,exec,srcreg,srcinc,srcoff,d1,screg1,screg2,scregA,scincA,L1pt,L1pd
	zr3_6r_fft_unroll 0,noexec,exec,exec,srcreg,srcinc,srcoff,d1,screg1,screg2,scregA,scincA,L1pt,L1pd
	zr3_6r_fft_unroll 1,exec,exec,exec,srcreg,srcinc,srcoff,d1,screg1,screg2,scregA,scincA,L1pt,L1pd
	zr3_6r_fft_unroll 2,exec,exec,noexec,srcreg,srcinc,srcoff,d1,screg1,screg2,scregA,scincA,L1pt,L1pd
	zr3_6r_fft_unroll 3,exec,noexec,noexec,srcreg,srcinc,srcoff,d1,screg1,screg2,scregA,scincA,L1pt,L1pd
	bump	srcreg, 3*srcinc
	bump	scregA, 3*scincA
	bump	scregB, 3*scincB
	ELSEIF maxrpt GE 2 AND maxrpt MOD 2 EQ 0
	zr3_6r_fft_unroll -1,noexec,noexec,exec,srcreg,srcinc,srcoff,d1,screg1,screg2,scregA,scincB,L1pt,L1pd
	zr3_6r_fft_unroll 0,noexec,exec,exec,srcreg,srcinc,srcoff,d1,screg1,screg2,scregA,scincB,L1pt,L1pd
	zr3_6r_fft_unroll 1,exec,exec,noexec,srcreg,srcinc,srcoff,d1,screg1,screg2,scregA,scincB,L1pt,L1pd
	zr3_6r_fft_unroll 2,exec,noexec,noexec,srcreg,srcinc,srcoff,d1,screg1,screg2,scregA,scincB,L1pt,L1pd
	bump	srcreg, 2*srcinc
	bump	scregA, 2*scincA
	bump	scregB, 2*scincB
	ELSE
	zr3_6r_fft_unroll -1,noexec,noexec,exec,srcreg,srcinc,srcoff,d1,screg1,screg2,scregA,scincA,L1pt,L1pd
	zr3_6r_fft_unroll 0,noexec,exec,noexec,srcreg,srcinc,srcoff,d1,screg1,screg2,scregA,scincA,L1pt,L1pd
	zr3_6r_fft_unroll 1,exec,noexec,noexec,srcreg,srcinc,srcoff,d1,screg1,screg2,scregA,scincA,L1pt,L1pd
	bump	srcreg, srcinc
	bump	scregA, scincA
	bump	scregB, scincB
	ENDIF
	ENDM

;;; WARNING:  Unrolling only works in scincB is blank/zero or scincA equals scincB

zr3_6r_fft_unroll MACRO iter,prev,this,next,srcreg,srcinc,srcoff,d1,screg1,screg2,screg,scinc,L1pt,L1pd

;; On first call, init registers

	IF iter EQ -1
y0	TEXTEQU	<zmm0>
y1	TEXTEQU	<zmm1>
y2	TEXTEQU	<zmm2>
y3	TEXTEQU	<zmm3>
y4	TEXTEQU	<zmm4>
y5	TEXTEQU	<zmm5>
y6	TEXTEQU	<zmm6>
y7	TEXTEQU	<zmm7>
y8	TEXTEQU	<zmm8>
y9	TEXTEQU	<zmm9>
y10	TEXTEQU	<zmm10>
y11	TEXTEQU	<zmm11>
y12	TEXTEQU	<zmm12>
y13	TEXTEQU	<zmm13>
	ENDIF

;; On later calls, previous A2,B2,A3,B3,R2,I2,R3,I3 will be in y0-7.  This R2+R6,R2-R6,R3+R5,R3-R5,0.5*(R2+R6) will be in y8-12.
;; The remaining register is free (zmm14 and zmm15 are preloaded constants 0.866 and 0.5).

this	vmovapd	y13, [srcreg+iter*srcinc+srcoff+32] ;; R4
this	vaddpd	y8, y13, y8			;; final R1 #2 = R4 + R2 + R6	; 1-3
this	vmulpd	y9, zmm15, y9			;; X = 0.866 * (R2 - R6)	;	1-5

prev	vaddpd	y3, y3, y6			;; B3 = B3 + R3			; 2-4
this	vmulpd	y6, zmm14, y10			;; 0.5 * (R3 + R5)		;	2-6
this	L1prefetchw srcreg+iter*srcinc+L1pd, L1pt

this	zstore	[srcreg+iter*srcinc+32], y8	;; Save I1			; 4
this	vmovapd	y8, [srcreg+iter*srcinc+srcoff] ;; R1
this	vaddpd	y10, y8, y10			;; final R1 #1 = R1 + R3 + R5	; 3-5
this	vmulpd	y11, zmm15, y11			;; Y = 0.866 * (R3 - R5)	;	3-7

prev	vaddpd	y1, y1, y4			;; B2 = B2 + R2			; 4-6
prev	vmovapd	y4, [screg2+(iter-1)*scinc]	;; sine

prev	vsubpd	y2, y2, y7			;; A3 = A3 - I3			; 5-7
prev	vmulpd	y3, y3, y4			;; B3 = B3 * sine (new I3)	;	5-9
prev	vmovapd	y7, [screg1+(iter-1)*scinc]	;; sine

this	vsubpd	y13, y13, y12			;; B = R4 - 0.5 * (R2 + R6)	; 6-8
this	vmovapd	y12, [screg2+iter*scinc+32]	;; cosine/sine
this	zstore	[srcreg+iter*srcinc], y10	;; Save R1			; 6

prev	vsubpd	y0, y0, y5			;; A2 = A2 - I2			; 7-9
prev	vmulpd	y1, y1, y7			;; B2 = B2 * sine (new I2)	;	7-11
next	vmovapd	y10, [srcreg+(iter+1)*srcinc+srcoff+d1] ;; R2

this	vsubpd	y8, y8, y6			;; A = R1 - 0.5 * (R3 + R5)	; 8-10
prev	vmulpd	y2, y2, y4			;; A3 = A3 * sine (new R3)	;	8-12
next	vmovapd	y5, [srcreg+(iter+1)*srcinc+srcoff+2*d1+32] ;; R6

this	vsubpd	y4, y9, y11			;; X - Y (final I3)		; 9-11
this	vmovapd	y6, [screg1+iter*scinc+32]	;; cosine/sine

this	vaddpd	y9, y9, y11			;; X + Y (final I2)		; 10-12
prev	vmulpd	y0, y0, y7			;; A2 = A2 * sine (new R2)	;	10-13
next	vmovapd	y11, [srcreg+(iter+1)*srcinc+srcoff+2*d1] ;; R3
prev	zstore	[srcreg+(iter-1)*srcinc+2*d1+32], y3 ;; Save I3			; 10

this	vaddpd	y3, y8, y13			;; A + B (final R3)		; 11-13
next	vmovapd	y7, [srcreg+(iter+1)*srcinc+srcoff+d1+32] ;; R5

this	vsubpd	y8, y8, y13			;; A - B (final R2)		; 12-14
this	vmulpd	y13, y4, y12			;; B3 = I3 * cosine/sine	;	12-16
prev	zstore	[srcreg+(iter-1)*srcinc+d1+32], y1 ;; Save I2			; 12
this	L1prefetchw srcreg+iter*srcinc+d1+L1pd, L1pt

next	vaddpd	y1, y10, y5			;; R2 + R6			; 13-15
prev	zstore	[srcreg+(iter-1)*srcinc+2*d1], y2 ;; Save R3			; 13
this	vmulpd	y2, y9, y6			;; B2 = I2 * cosine/sine	;	13-17

next	vsubpd	y10, y10, y5			;; R2 - R6			; 14-16
this	vmulpd	y12, y3, y12			;; A3 = R3 * cosine/sine	;	14-18
this	L1prefetchw srcreg+iter*srcinc+2*d1+L1pd, L1pt

next	vaddpd	y5, y11, y7			;; R3 + R5			; 15-17
this	vmulpd	y6, y8, y6			;; A2 = R2 * cosine/sine	;	15-19
prev	zstore	[srcreg+(iter-1)*srcinc+d1], y0 ;; Save R2			; 15

next	vsubpd	y11, y11, y7			;; R3 - R5			; 16-18
next	vmulpd	y7, zmm14, y1			;; 0.5 * (R2 + R6)		;	16-20
this next zloop_unrolled_one

;; Shuffle register assignments so that next call has A2,B2,A3,B3,R2,I2,R3,I3 in y0-7 and next R2+R6,R2-R6,R3+R5,R3-R5,0.5*(R2+R6) in y8-12.

ytmp	TEXTEQU	y0
y0	TEXTEQU	y6
y6	TEXTEQU	y3
y3	TEXTEQU	y13
y13	TEXTEQU	ytmp
ytmp	TEXTEQU	y1
y1	TEXTEQU	y2
y2	TEXTEQU	y12
y12	TEXTEQU	y7
y7	TEXTEQU	y4
y4	TEXTEQU	y8
y8	TEXTEQU	ytmp
ytmp	TEXTEQU	y5
y5	TEXTEQU	y9
y9	TEXTEQU	y10
y10	TEXTEQU ytmp

	ENDM


;; Haswell FMA3 version

IF (@INSTR(,%zarch,<FMA3>) NE 0)

zr3_6r_fft_cmn_preload MACRO
	vmovapd	zmm13, ZMM_ONE
	vmovapd	zmm14, ZMM_HALF
	vmovapd	zmm15, ZMM_P866
	ENDM

;; 6 loads, 6 stores, 4 sin/cos loads, 5 muls, 16 FMAs, 8 movs = 45 uops.  Best case is 45/4 = 11.25 clocks.
;; Timed at 12 clocks.
zr3_6r_fft_unroll MACRO iter,prev,this,next,srcreg,srcinc,srcoff,d1,screg1,screg2,screg,scinc,L1pt,L1pd

;; On first call, init registers

	IF iter EQ -1
y0	TEXTEQU	<zmm0>
y1	TEXTEQU	<zmm1>
y2	TEXTEQU	<zmm2>
y3	TEXTEQU	<zmm3>
y4	TEXTEQU	<zmm4>
y5	TEXTEQU	<zmm5>
y6	TEXTEQU	<zmm6>
y7	TEXTEQU	<zmm7>
y8	TEXTEQU	<zmm8>
y9	TEXTEQU	<zmm9>
y10	TEXTEQU	<zmm10>
y11	TEXTEQU	<zmm11>
y12	TEXTEQU	<zmm12>
	ENDIF

;; On later calls, previous R2,I2,R3,I3 will be in y0-3.  This R1,I1,A,B,R2-R6,R3-R5 will be in y4-9.
;; The remaining registers are free (zmm14 and zmm15 are preloaded constants 0.5 and 0.866, zmm13 reserved for Haswell).

prev	vmovapd	y10, [screg1+(iter-1)*scinc+32]	;; cosine/sine for R2/I2
prev	vmovapd	y11, [screg2+(iter-1)*scinc+32]	;; cosine/sine for R3/I3
this	vmulpd	y8, zmm15, y8			;; X = 0.866 * (R2-R6)		; 1-5

next	vmovapd	y12, [srcreg+(iter+1)*srcinc+srcoff+d1] ;; R2
this	zstore	[srcreg+iter*srcinc+32], y5	;; Save I1			; 4
prev	zfmsubpd y5, y0, y10, y1		;; A2 = R2 * cosine/sine - I2	; 2-6
prev	zfmaddpd y1, y1, y10, y0		;; B2 = I2 * cosine/sine + R2	; 2-6

next	vmovapd	y10, [srcreg+(iter+1)*srcinc+srcoff+2*d1+32] ;; R6
prev	zfmsubpd y0, y2, y11, y3		;; A3 = R3 * cosine/sine - I3	; 3-7
prev	zfmaddpd y3, y3, y11, y2		;; B3 = I3 * cosine/sine + R3	; 3-7
this	L1prefetchw srcreg+iter*srcinc+L1pd, L1pt

next	vmovapd	y11, [srcreg+(iter+1)*srcinc+srcoff+2*d1] ;; R3
next	zfmaddpd y2, y12, zmm13, y10		;; R2 + R6			; 4-8
next	zfmsubpd y12, y12, zmm13, y10		;; R2 - R6			; 4-8

next	vmovapd	y10, [srcreg+(iter+1)*srcinc+srcoff+d1+32] ;; R5
this	zstore	[srcreg+iter*srcinc], y4	;; Save R1			; 5
next	zfmaddpd y4, y11, zmm13, y10		;; R3 + R5			; 5-9
next	zfmsubpd y11, y11, zmm13, y10		;; R3 - R5			; 5-9

this	zfmsubpd y10, y6, zmm13, y7		;; A - B (new R2)		; 6-10
this	zfmaddpd y6, y6, zmm13, y7		;; A + B (new R3)		; 6-10
this	L1prefetchw srcreg+iter*srcinc+d1+L1pd, L1pt

this	zfmaddpd y7, zmm15, y9, y8		;; X + 0.866 * (R3-R5) (new I2)	; 7-11
this	zfnmaddpd y9, zmm15, y9, y8		;; X - 0.866 * (R3-R5) (new I3)	; 7-11
this	L1prefetchw srcreg+iter*srcinc+2*d1+L1pd, L1pt

prev	vmovapd	y8, [screg1+(iter-1)*scinc]	;; sine
prev	vmulpd	y5, y5, y8			;; A2 = A2 * sine (final R2)	; 8-12
prev	vmulpd	y1, y1, y8			;; B2 = B2 * sine (final I2)	; 8-12

prev	vmovapd	y8, [screg2+(iter-1)*scinc]	;; sine
prev	vmulpd	y0, y0, y8			;; A3 = A3 * sine (final R3)	; 9-13
prev	vmulpd	y3, y3, y8			;; B3 = B3 * sine (final I3)	; 9-13
this next zloop_unrolled_one

next	vmovapd	y8, [srcreg+(iter+1)*srcinc+srcoff+32] ;; R4
prev	zstore	[srcreg+(iter-1)*srcinc+d1], y5 ;; Save R2			; 13
next	zfmaddpd y5, y8, zmm13, y2		;; final R1 #2 = R4 + (R2+R6)	; 10-14
next	zfnmaddpd y2, zmm14, y2, y8		;; B = R4 - 0.5 * (R2+R6)	; 10-14

next	vmovapd	y8, [srcreg+(iter+1)*srcinc+srcoff] ;; R1
prev	zstore	[srcreg+(iter-1)*srcinc+d1+32], y1 ;; Save I2			; 13+1
next	zfmaddpd y1, y8, zmm13, y4		;; final R1 #1 = R1 + (R3+R5)	; 11-15
next	zfnmaddpd y4, zmm14, y4, y8		;; A = R1 - 0.5 * (R3+R5)	; 11-15
prev	zstore	[srcreg+(iter-1)*srcinc+2*d1], y0 ;; Save R3			; 14+1
prev	zstore	[srcreg+(iter-1)*srcinc+2*d1+32], y3 ;; Save I3			; 14+2

;; Shuffle register assignments so that next call has R2,I2,R3,I3 in y0-3 and R1,I1,A,B,R2-R6,R3-R5 in y4-9.

ytmp	TEXTEQU	y0
y0	TEXTEQU	y10
y10	TEXTEQU	y8
y8	TEXTEQU	y12
y12	TEXTEQU	y3
y3	TEXTEQU	y9
y9	TEXTEQU	y11
y11	TEXTEQU	ytmp
ytmp	TEXTEQU	y1
y1	TEXTEQU	y7
y7	TEXTEQU	y2
y2	TEXTEQU	y6
y6	TEXTEQU	y4
y4	TEXTEQU	ytmp

	ENDM

ENDIF

ENDIF


;;
;; ************************************* six-reals-unfft variants ******************************************
;;

; R1 = R1#1 + (R2 + R3)
; R2 = R1#2 + 0.5 * (R2 - R3) + 0.866 * (I2 + I3)
; R3 = R1#1 - 0.5 * (R2 + R3) + 0.866 * (I2 - I3)
; R4 = R1#2 - (R2 - R3)
; R5 = R1#1 - 0.5 * (R2 + R3) - 0.866 * (I2 - I3)
; R6 = R1#2 + 0.5 * (R2 - R3) - 0.866 * (I2 + I3)

; The standard version
zr3_3cl_six_reals_unfft_preload MACRO
	zr3_6r_unfft_cmn_preload
	ENDM
zr3_3cl_six_reals_unfft MACRO srcreg,srcinc,d1,screg,scinc,maxrpt,L1pt,L1pd
	zr3_6r_unfft_cmn srcreg,srcinc,d1,screg,screg+64,screg,scinc,,,maxrpt,L1pt,L1pd
	ENDM

; Like standard version but uses 2 sin/cos ptrs
zr3_3cl_2sc_six_reals_unfft_preload MACRO
	zr3_6r_unfft_cmn_preload
	ENDM
zr3_3cl_2sc_six_reals_unfft MACRO srcreg,srcinc,d1,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	zr3_6r_unfft_cmn srcreg,srcinc,d1,screg2,screg1,screg1,scinc1,screg2,scinc2,maxrpt,L1pt,L1pd
	ENDM

; Like standard version but the first sin/cos value can be used by a three-complex macro at the same FFT level
zr3_3cl_csc_six_reals_unfft_preload MACRO
	zr3_6r_unfft_cmn_preload
	ENDM
zr3_3cl_csc_six_reals_unfft MACRO srcreg,srcinc,d1,screg,scinc,maxrpt,L1pt,L1pd
	zr3_6r_unfft_cmn srcreg,srcinc,d1,screg+64,screg,screg,scinc,,,maxrpt,L1pt,L1pd
	ENDM

zr3_6r_unfft_cmn_preload MACRO
	ENDM
zr3_6r_unfft_cmn MACRO srcreg,srcinc,d1,screg1,screg2,scregA,scincA,scregB,scincB,maxrpt,L1pt,L1pd
	vmovapd	zmm1, [srcreg+d1]		;; R2
	vmovapd	zmm2, [screg1+32]		;; cosine/sine
	vmulpd	zmm7, zmm1, zmm2		;; A2 = R2 * cosine/sine
	vmovapd	zmm3, [srcreg+2*d1]		;; R3
	vmovapd	zmm6, [screg2+32]		;; cosine/sine
	vmulpd	zmm4, zmm3, zmm6		;; A3 = R3 * cosine/sine
	vmovapd	zmm0, [srcreg+d1+32]		;; I2
	vaddpd	zmm7, zmm7, zmm0		;; A2 = A2 + I2
	vmovapd	zmm5, [srcreg+2*d1+32]		;; I3
	vaddpd	zmm4, zmm4, zmm5		;; A3 = A3 + I3
	vmulpd	zmm0, zmm0, zmm2		;; B2 = I2 * cosine/sine
	vmulpd	zmm5, zmm5, zmm6		;; B3 = I3 * cosine/sine
	vsubpd	zmm0, zmm0, zmm1		;; B2 = B2 - R2
	vsubpd	zmm5, zmm5, zmm3		;; B3 = B3 - R3
	vmovapd	zmm2, [screg1]			;; sine
	vmulpd	zmm7, zmm7, zmm2		;; A2 = A2 * sine (final R2)
	vmovapd	zmm6, [screg2]			;; sine
	vmulpd	zmm4, zmm4, zmm6		;; A3 = A3 * sine (final R3)
	vmulpd	zmm0, zmm0, zmm2		;; B2 = B2 * sine (final I2)
	vmulpd	zmm5, zmm5, zmm6		;; B3 = B3 * sine (final I3)

	L1prefetchw srcreg+L1pd, L1pt

	vsubpd	zmm2, zmm4, zmm7		;; R3 - R2
	vaddpd	zmm1, zmm7, zmm4		;; R2 + R3
	vsubpd	zmm3, zmm0, zmm5		;; I2 - I3
	vaddpd	zmm0, zmm0, zmm5		;; I2 + I3

	L1prefetchw srcreg+d1+L1pd, L1pt

	vmovapd	zmm7, ZMM_P866
	vmulpd	zmm3, zmm7, zmm3		;; B = 0.866 * (I2 - I3)
	vmulpd	zmm4, zmm7, zmm0		;; Y = 0.866 * (I2 + I3)

	vmovapd	zmm7, ZMM_HALF
	vmulpd	zmm5, zmm7, zmm1		;; 0.5 * (R2 + R3)
	vmulpd	zmm0, zmm7, zmm2		;; 0.5 * (R3 - R2)

	L1prefetchw srcreg+2*d1+L1pd, L1pt

	vmovapd	zmm7, [srcreg]			;; R1#1
	vaddpd	zmm1, zmm7, zmm1		;; R1#1 + R2 + R3 (final R1)
	vsubpd	zmm7, zmm7, zmm5		;; A = R1#1 - 0.5 * (R2 + R3)

	vmovapd	zmm5, [srcreg+32]		;; R1#2
	vaddpd	zmm2, zmm5, zmm2		;; R1#2 + (R3 - R2) (final R4)
	vsubpd	zmm5, zmm5, zmm0		;; X = R1#2 - 0.5 * (R3 - R2)
	vsubpd	zmm0, zmm7, zmm3		;; A - B (final R5)
	vaddpd	zmm7, zmm7, zmm3		;; A + B (final R3)
	vsubpd	zmm3, zmm5, zmm4		;; X - Y (final R6)
	vaddpd	zmm5, zmm5, zmm4		;; X + Y (final R2)

	zstore	[srcreg], zmm1			;; Save R1
	zstore	[srcreg+32], zmm2		;; Save I1
	zstore	[srcreg+d1], zmm5		;; Save R2
	zstore	[srcreg+d1+32], zmm0		;; Save I2
	zstore	[srcreg+2*d1], zmm7		;; Save R3
	zstore	[srcreg+2*d1+32], zmm3		;; Save I3
	bump	srcreg, srcinc
	bump	scregA, scincA
	bump	scregB, scincB
	ENDM

;; 64-bit version

IFDEF X86_64

zr3_6r_unfft_cmn_preload MACRO
	vmovapd	zmm14, ZMM_HALF
	vmovapd	zmm15, ZMM_P866
	ENDM
zr3_6r_unfft_cmn MACRO srcreg,srcinc,d1,screg1,screg2,scregA,scincA,scregB,scincB,maxrpt,L1pt,L1pd
	IF maxrpt GE 4 AND maxrpt MOD 4 EQ 0
	zr3_6r_unfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,screg1,screg2,scregA,scincA,L1pt,L1pd
	zr3_6r_unfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,screg1,screg2,scregA,scincA,L1pt,L1pd
	zr3_6r_unfft_unroll 1,exec,exec,exec,srcreg,srcinc,d1,screg1,screg2,scregA,scincA,L1pt,L1pd
	zr3_6r_unfft_unroll 2,exec,exec,exec,srcreg,srcinc,d1,screg1,screg2,scregA,scincA,L1pt,L1pd
	zr3_6r_unfft_unroll 3,exec,exec,noexec,srcreg,srcinc,d1,screg1,screg2,scregA,scincA,L1pt,L1pd
	zr3_6r_unfft_unroll 4,exec,noexec,noexec,srcreg,srcinc,d1,screg1,screg2,scregA,scincA,L1pt,L1pd
	bump	srcreg, 4*srcinc
	bump	scregA, 4*scincA
	bump	scregB, 4*scincB
	ELSEIF maxrpt GE 3 AND maxrpt MOD 3 EQ 0
	zr3_6r_unfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,screg1,screg2,scregA,scincA,L1pt,L1pd
	zr3_6r_unfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,screg1,screg2,scregA,scincA,L1pt,L1pd
	zr3_6r_unfft_unroll 1,exec,exec,exec,srcreg,srcinc,d1,screg1,screg2,scregA,scincA,L1pt,L1pd
	zr3_6r_unfft_unroll 2,exec,exec,noexec,srcreg,srcinc,d1,screg1,screg2,scregA,scincA,L1pt,L1pd
	zr3_6r_unfft_unroll 3,exec,noexec,noexec,srcreg,srcinc,d1,screg1,screg2,scregA,scincA,L1pt,L1pd
	bump	srcreg, 3*srcinc
	bump	scregA, 3*scincA
	bump	scregB, 3*scincB
	ELSEIF maxrpt GE 2 AND maxrpt MOD 2 EQ 0
	zr3_6r_unfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,screg1,screg2,scregA,scincA,L1pt,L1pd
	zr3_6r_unfft_unroll 0,noexec,exec,exec,srcreg,srcinc,d1,screg1,screg2,scregA,scincA,L1pt,L1pd
	zr3_6r_unfft_unroll 1,exec,exec,noexec,srcreg,srcinc,d1,screg1,screg2,scregA,scincA,L1pt,L1pd
	zr3_6r_unfft_unroll 2,exec,noexec,noexec,srcreg,srcinc,d1,screg1,screg2,scregA,scincA,L1pt,L1pd
	bump	srcreg, 2*srcinc
	bump	scregA, 2*scincA
	bump	scregB, 2*scincB
	ELSE
	zr3_6r_unfft_unroll -1,noexec,noexec,exec,srcreg,srcinc,d1,screg1,screg2,scregA,scincA,L1pt,L1pd
	zr3_6r_unfft_unroll 0,noexec,exec,noexec,srcreg,srcinc,d1,screg1,screg2,scregA,scincA,L1pt,L1pd
	zr3_6r_unfft_unroll 1,exec,noexec,noexec,srcreg,srcinc,d1,screg1,screg2,scregA,scincA,L1pt,L1pd
	bump	srcreg, srcinc
	bump	scregA, scincA
	bump	scregB, scincB
	ENDIF
	ENDM

;;; WARNING:  Unrolling only works in scincB is blank/zero or scincA equals scincB

zr3_6r_unfft_unroll MACRO iter,prev,this,next,srcreg,srcinc,d1,screg1,screg2,screg,scinc,L1pt,L1pd

;; On first call, init registers

	IF iter EQ -1
y0	TEXTEQU	<zmm0>
y1	TEXTEQU	<zmm1>
y2	TEXTEQU	<zmm2>
y3	TEXTEQU	<zmm3>
y4	TEXTEQU	<zmm4>
y5	TEXTEQU	<zmm5>
y6	TEXTEQU	<zmm6>
y7	TEXTEQU	<zmm7>
y8	TEXTEQU	<zmm8>
y9	TEXTEQU	<zmm9>
y10	TEXTEQU	<zmm10>
y11	TEXTEQU	<zmm11>
y12	TEXTEQU	<zmm12>
y13	TEXTEQU	<zmm13>
	ENDIF

;; On later calls, previous X,Y.A.B will be in y0-3.  This R1,I1,R2,R3,B2,B3,sine2,sine3 will be in y4-11.
;; The remaining registers are free (zmm14 and zmm15 are preloaded constants 0.5 and 0.866).

prev	vsubpd	y12, y0, y1			;; X - Y (final R6)			; 1-3
this	vmulpd	y8, y8, y10			;; B2 = B2 * sine (final I2)		;	1-5
next	vmovapd	y13, [srcreg+(iter+1)*srcinc+d1] ;; R2

prev	vaddpd	y0, y0, y1			;; X + Y (final R2)			; 2-4
this	vmulpd	y9, y9, y11			;; B3 = B3 * sine (final I3)		;	2-6
next	vmovapd	y10, [screg1+(iter+1)*scinc+32]	;; cosine/sine
prev	zstore	[srcreg+(iter-1)*srcinc+32], y5	;; Save I1				; 2

prev	vsubpd	y5, y2, y3			;; A - B (final R5)			; 3-5
next	vmovapd	y1, [srcreg+(iter+1)*srcinc+2*d1] ;; R3
prev	zstore	[srcreg+(iter-1)*srcinc], y4	;; Save R1				; 3

prev	vaddpd	y2, y2, y3			;; A + B (final R3)			; 4-6
next	vmulpd	y3, y13, y10			;; A2 = R2 * cosine/sine		;	4-8
next	vmovapd	y11, [screg2+(iter+1)*scinc+32]	;; cosine/sine
prev	zstore	[srcreg+(iter-1)*srcinc+2*d1+32], y12 ;; Save I3			; 4

this	vsubpd	y12, y7, y6			;; R3 - R2				; 5-7
next	vmulpd	y4, y1, y11			;; A3 = R3 * cosine/sine		;	5-9
prev	zstore	[srcreg+(iter-1)*srcinc+d1], y0 ;; Save R2				; 5
next	vmovapd	y0, [srcreg+(iter+1)*srcinc+d1+32] ;; I2

this	vaddpd	y6, y6, y7			;; R2 + R3				; 6-8
next	vmulpd	y10, y0, y10			;; B2 = I2 * cosine/sine		;	6-10
next	vmovapd	y7, [srcreg+(iter+1)*srcinc+2*d1+32] ;; I3
prev	zstore	[srcreg+(iter-1)*srcinc+d1+32], y5 ;; Save I2				; 6

this	vaddpd	y5, y8, y9			;; I2 + I3				; 7-9
next	vmulpd	y11, y7, y11			;; B3 = I3 * cosine/sine		;	7-11
prev	zstore	[srcreg+(iter-1)*srcinc+2*d1], y2 ;; Save R3				; 7

this	vsubpd	y8, y8, y9			;; I2 - I3				; 8-10
this	vmulpd	y9, zmm14, y12			;; 0.5 * (R3 - R2)			;	8-12
this	vmovapd	y2, [srcreg+iter*srcinc+32]	;; R1#2

next	vaddpd	y3, y3, y0			;; A2 = A2 + I2				; 9-11
this	vmulpd	y0, zmm14, y6			;; 0.5 * (R2 + R3)			;	9-13
this	L1prefetchw srcreg+iter*srcinc+L1pd, L1pt

next	vaddpd	y4, y4, y7			;; A3 = A3 + I3				; 10-12
this	vmulpd	y5, zmm15, y5			;; Y = 0.866 * (I2 + I3)		;	10-14
this	vmovapd	y7, [srcreg+iter*srcinc]	;; R1#1

next	vsubpd	y10, y10, y13			;; B2 = B2 - R2				; 11-13
this	vmulpd	y8, zmm15, y8			;; B = 0.866 * (I2 - I3)		;	11-15
next	vmovapd	y13, [screg1+(iter+1)*scinc]	;; sine

next	vsubpd	y11, y11, y1			;; B3 = B3 - R3				; 12-14
next	vmovapd	y1, [screg2+(iter+1)*scinc]	;; sine

this	vsubpd	y9, y2, y9			;; X = R1#2 - 0.5 * (R3 - R2)		; 13-15
this	L1prefetchw srcreg+iter*srcinc+d1+L1pd, L1pt

this	vsubpd	y0, y7, y0			;; A = R1#1 - 0.5 * (R2 + R3)		; 14-16

this	vaddpd	y2, y2, y12			;; R1#2 + (R3 - R2) (final R4)		; 15-17
next	vmulpd	y3, y3, y13			;; A2 = A2 * sine (final R2)		;	15-19
this	L1prefetchw srcreg+iter*srcinc+2*d1+L1pd, L1pt

this	vaddpd	y7, y7, y6			;; R1#1 + R2 + R3 (final R1)		; 16-18
next	vmulpd	y4, y4, y1			;; A3 = A3 * sine (final R3)		;	16-20
this next zloop_unrolled_one

;; Shuffle register assignments so that next call has X,Y.A.B in y0-3 and next R1,I1,R2,R3,B2,B3,sine2,sine3 in y4-11.

ytmp	TEXTEQU	y0
y0	TEXTEQU	y9
y9	TEXTEQU	y11
y11	TEXTEQU	y1
y1	TEXTEQU	y5
y5	TEXTEQU	y2
y2	TEXTEQU	ytmp
ytmp	TEXTEQU	y3
y3	TEXTEQU	y8
y8	TEXTEQU	y10
y10	TEXTEQU	y13
y13	TEXTEQU	y6
y6	TEXTEQU	ytmp
ytmp	TEXTEQU	y4
y4	TEXTEQU	y7
y7	TEXTEQU ytmp

	ENDM


;; Haswell FMA3 version

IF (@INSTR(,%zarch,<FMA3>) NE 0)

zr3_6r_unfft_cmn_preload MACRO
	vmovapd	zmm13, ZMM_ONE
	vmovapd	zmm14, ZMM_HALF
	vmovapd	zmm15, ZMM_P866
	ENDM

;; 6 loads, 6 stores, 4 sin/cos loads, 2 muls, 16 FMAs, 8 movs = 42 uops.  Best case is 42/4 = 10.5 clocks.
;; Timed at 11.25 clocks.
zr3_6r_unfft_unroll MACRO iter,prev,this,next,srcreg,srcinc,d1,screg1,screg2,screg,scinc,L1pt,L1pd

;; On first call, init registers

	IF iter EQ -1
y0	TEXTEQU	<zmm0>
y1	TEXTEQU	<zmm1>
y2	TEXTEQU	<zmm2>
y3	TEXTEQU	<zmm3>
y4	TEXTEQU	<zmm4>
y5	TEXTEQU	<zmm5>
y6	TEXTEQU	<zmm6>
y7	TEXTEQU	<zmm7>
y8	TEXTEQU	<zmm8>
y9	TEXTEQU	<zmm9>
y10	TEXTEQU	<zmm10>
y11	TEXTEQU	<zmm11>
y12	TEXTEQU	<zmm12>
	ENDIF

;; On later iterations, previous R1#1,R1#2,R3-R2.R2+R3,I2+I3,I2-I3 are in y0-5.  This R2,I2,A3,B3 are in y6-9.
;; The remaining registers are free (zmm13, zmm14, and zmm15 are preloaded constants 1.0, 0.5, and 0.866).

prev	zfmaddpd y11, y1, zmm13, y2		;; R1#2 + (R3 - R2) (final R4)		; 1-5
prev	zfnmaddpd y2, zmm14, y2, y1		;; X = R1#2 - 0.5 * (R3 - R2)		; 1-5			n 6
next	vmovapd	y10, [srcreg+(iter+1)*srcinc+d1] ;; R2
this	L1prefetchw srcreg+iter*srcinc+d1+L1pd, L1pt

prev	zfmaddpd y12, y0, zmm13, y3		;; R1#1 + R2 + R3 (final R1)		; 2-6
prev	zfnmaddpd y3, zmm14, y3, y0		;; A = R1#1 - 0.5 * (R2 + R3)		; 2-6			n 9
next	vmovapd	y1, [srcreg+(iter+1)*srcinc+d1+32] ;; I2

next	vmovapd	y0, [screg1+(iter+1)*scinc+32]	;; cosine/sine for R2/I2
prev	zstore	[srcreg+(iter-1)*srcinc+32], y11 ;; Save I1				; 6
next	zfmaddpd y11, y10, y0, y1		;; A2 = R2 * cosine/sine + I2		; 3-7			n 8
next	zfmsubpd y1, y1, y0, y10		;; B2 = I2 * cosine/sine - R2		; 3-7
this	L1prefetchw srcreg+iter*srcinc+2*d1+L1pd, L1pt

this	vmovapd	y10, [screg2+iter*scinc]	;; sine
this	zfmsubpd y0, y8, y10, y6		;; A3 * sine - R2			; 4-8			n next 1
this	zfmaddpd y8, y8, y10, y6		;; R2 + A3 * sine			; 4-8			n next 2
next	vmovapd	y6, [srcreg+(iter+1)*srcinc+2*d1] ;; R3

prev	zstore	[srcreg+(iter-1)*srcinc], y12	;; Save R1				; 7
this	zfmaddpd y12, y9, y10, y7		;; I2 + B3 * sine			; 5-9			n next 6
this	zfnmaddpd y9, y9, y10, y7		;; I2 - B3 * sine			; 5-9			n next 9
next	vmovapd	y10, [srcreg+(iter+1)*srcinc+2*d1+32] ;; I3

prev	zfnmaddpd y7, zmm15, y4, y2		;; X - 0.866 * (I2 + I3) (final R6)	; 6-10
prev	zfmaddpd y4, zmm15, y4, y2		;; X + 0.866 * (I2 + I3) (final R2)	; 6-10
this	L1prefetchw srcreg+iter*srcinc+L1pd, L1pt

next	vmovapd	y2, [screg2+(iter+1)*scinc+32]	;; cosine/sine for R3/I3
prev	zstore	[srcreg+(iter-1)*srcinc+2*d1+32], y7 ;; Save I3				; 11
next	zfmaddpd y7, y6, y2, y10		;; A3 = R3 * cosine/sine + I3		; 7-11			n next 4
next	zfmsubpd y10, y10, y2, y6		;; B3 = I3 * cosine/sine - R3		; 7-11			n next 5

next	vmovapd	y2, [screg1+(iter+1)*scinc]	;; sine
next	vmulpd	y11, y11, y2			;; A2 = A2 * sine (new R2)		; 8-12			n next 4
next	vmulpd	y1, y1, y2			;; B2 = B2 * sine (new I2)		; 8-12			n next 5
this	vmovapd	y2, [srcreg+iter*srcinc+32]	;; R1#2
this next zloop_unrolled_one

prev	zfnmaddpd y6, zmm15, y5, y3		;; A - 0.866 * (I2 - I3) (final R5)	; 9-13
prev	zfmaddpd y5, zmm15, y5, y3		;; A + 0.866 * (I2 - I3) (final R3)	; 9-13
this	vmovapd	y3, [srcreg+iter*srcinc]	;; R1#1
prev	zstore	[srcreg+(iter-1)*srcinc+d1], y4 ;; Save R2				; 11+1
prev	zstore	[srcreg+(iter-1)*srcinc+d1+32], y6 ;; Save I2				; 14
prev	zstore	[srcreg+(iter-1)*srcinc+2*d1], y5 ;; Save R3				; 14+1

;; Shuffle register assignments so that this R1#1,R1#2,R3-R2.R2+R3,I2+I3,I2-I3 are in y0-5 and next R2,I2,A3,B3 are in y6-9.

ytmp	TEXTEQU	y0
y0	TEXTEQU	y3
y3	TEXTEQU	y8
y8	TEXTEQU	y7
y7	TEXTEQU	y1
y1	TEXTEQU	y2
y2	TEXTEQU	ytmp
ytmp	TEXTEQU	y4
y4	TEXTEQU	y12
y12	TEXTEQU	y5
y5	TEXTEQU	y9
y9	TEXTEQU	y10
y10	TEXTEQU	y6
y6	TEXTEQU	y11
y11	TEXTEQU	ytmp

	ENDM

ENDIF

ENDIF


;;
;; ************************************* six-reals-three-complex-fft variants ******************************************
;;

;; Macro to do one six_reals_fft and three three_complex_djbfft.
;; The six-reals operation is done in the lower double of the ZMM
;; register.  The three-complex is done in the high doubles of the
;; ZMM register.   This is REALLY funky, as we do both at the same
;; time within the full zmm register whenever possible.

zr3_3cl_six_reals_three_complex_djbfft_preload MACRO
	zr3_o6r_t3c_djbfft_mem_preload
	ENDM

zr3_3cl_six_reals_three_complex_djbfft MACRO srcreg,srcinc,d1,screg,scinc,maxrpt
	zr3_o6r_t3c_djbfft_mem [srcreg],[srcreg+32],[srcreg+d1],[srcreg+d1+32],[srcreg+2*d1],[srcreg+2*d1+32],screg
	zstore	[srcreg], zmm0			;; Save R1
	zstore	[srcreg+32], zmm5		;; Save I1
	zstore	[srcreg+d1], zmm3		;; Save R2
	zstore	[srcreg+d1+32], zmm7		;; Save I2
	zstore	[srcreg+2*d1], zmm6		;; Save R3
	zstore	[srcreg+2*d1+32], zmm4		;; Save I3
	bump	srcreg, srcinc
	bump	screg, scinc
	ENDM

zr3_o6r_t3c_djbfft_mem_preload MACRO
	ENDM

zr3_o6r_t3c_djbfft_mem MACRO mem1,mem2,mem3,mem4,mem5,mem6,screg
					;; Six-reals comments		;; Three complex comments
	vmovapd	zmm0, mem3		;; R2				;; R2
	vmovapd	zmm1, mem4		;; R5				;; I2
;;	vblendpd zmm4, zmm1, zmm0, 1	;; R2				;; I2
	vmovapd	zmm3, mem6		;; R6				;; I3
	vaddpd	zmm5, zmm4, zmm3	;; R2 + R6			;; I2 + I3
	vsubpd	zmm4, zmm4, zmm3	;; R2 - R6			;; I2 - I3
	vmovapd	zmm2, mem5		;; R3				;; R3
;;	vblendpd zmm6, zmm0, zmm2, 1	;; R3				;; R2
;;	vblendpd zmm7, zmm2, zmm1, 1	;; R5				;; R3
	vaddpd	zmm0, zmm6, zmm7	;; R3 + R5			;; R2 + R3
	vsubpd	zmm6, zmm6, zmm7	;; R3 - R5			;; R2 - R3

	vmovapd	zmm7, ZMM_HALF
	vmulpd	zmm1, zmm7, zmm5	;; 0.5 * (R2 + R6)		;; 0.5 * (I2 + I3)
	vmovapd	zmm2, mem2		;; R4				;; I1
	vaddpd	zmm5, zmm2, zmm5	;; final R1 #2 = R4 + R2 + R6	;; I1 + I2 + I3 (final I1)
	vmulpd	zmm7, zmm7, zmm0	;; 0.5 * (R3 + R5)		;; 0.5 * (R2 + R3)
	vmovapd	zmm3, ZMM_P866
	vmulpd	zmm4, zmm3, zmm4	;; X = 0.866 * (R2 - R6)	;; B = 0.866 * (I2 - I3)
	vmulpd	zmm6, zmm3, zmm6	;; Y = 0.866 * (R3 - R5)	;; Y = 0.866 * (R2 - R3)
	vmovapd	zmm3, mem1		;; R1				;; R1
	vaddpd	zmm0, zmm3, zmm0	;; final R1 #1 = R1 + R3 + R5	;; R1 + R2 + R3 (final R1)
	vsubpd	zmm2, zmm2, zmm1	;; B = R4 - 0.5 * (R2 + R6)	;; X = (I1-.5I2-.5I3)
	vsubpd	zmm3, zmm3, zmm7	;; A = R1 - 0.5 * (R3 + R5)	;; A = (R1-.5R2-.5R3)

;;	vblendpd zmm1, zmm2, zmm4, 1	;; X				;; X
	vsubpd	zmm7, zmm1, zmm6	;; X - Y (final I3)		;; X - Y (final I3)
	vaddpd	zmm1, zmm1, zmm6	;; X + Y (final I2)		;; X + Y (final I2)
;;	vblendpd zmm4, zmm4, zmm2, 1	;; B				;; B
	vaddpd	zmm6, zmm3, zmm4	;; A + B (final R3)		;; A + B (final R3)
	vsubpd	zmm3, zmm3, zmm4	;; A - B (final R2)		;; A - B (final R2)

	vmovapd	zmm2, [screg+64+32]	;; cosine/sine
	vmulpd	zmm4, zmm7, zmm2	;; B3 = I3 * cosine/sine
	vaddpd	zmm4, zmm4, zmm6	;; B3 = B3 + R3
	vmulpd	zmm6, zmm6, zmm2	;; A3 = R3 * cosine/sine
	vsubpd	zmm6, zmm6, zmm7	;; A3 = A3 - I3

	vmovapd	zmm2, [screg+32]	;; cosine/sine
	vmulpd	zmm7, zmm1, zmm2	;; B2 = I2 * cosine/sine
	vaddpd	zmm7, zmm7, zmm3	;; B2 = B2 + R2
	vmulpd	zmm3, zmm3, zmm2	;; A2 = R2 * cosine/sine
	vsubpd	zmm3, zmm3, zmm1	;; A2 = A2 - I2

	vmovapd	zmm1, [screg+64]	;; sine
	vmulpd	zmm4, zmm4, zmm1	;; B3 = B3 * sine (new I3)
	vmulpd	zmm6, zmm6, zmm1	;; A3 = A3 * sine (new R3)
	vmovapd	zmm1, [screg]		;; sine
	vmulpd	zmm7, zmm7, zmm1	;; B2 = B2 * sine (new I2)
	vmulpd	zmm3, zmm3, zmm1	;; A2 = A2 * sine (new R2)
	ENDM

;; 64-bit version

IFDEF X86_64

zr3_o6r_t3c_djbfft_mem_preload MACRO
	vmovapd	zmm14, ZMM_HALF
	vmovapd	zmm13, ZMM_P866
	ENDM

zr3_o6r_t3c_djbfft_mem MACRO mem1,mem2,mem3,mem4,mem5,mem6,screg
					;; Six-reals comments		;; Three complex comments
	vmovapd	zmm0, mem3		;; R2				;; R2
	vmovapd	zmm1, mem4		;; R5				;; I2
;;	vblendpd zmm4, zmm1, zmm0, 1	;; R2				;; I2
	vmovapd	zmm3, mem6		;; R6				;; I3
	vaddpd	zmm5, zmm4, zmm3	;; R2 + R6			;; I2 + I3
	vsubpd	zmm4, zmm4, zmm3	;; R2 - R6			;; I2 - I3
	vmovapd	zmm2, mem5		;; R3				;; R3
;;	vblendpd zmm6, zmm0, zmm2, 1	;; R3				;; R2
;;	vblendpd zmm7, zmm2, zmm1, 1	;; R5				;; R3
	vaddpd	zmm0, zmm6, zmm7	;; R3 + R5			;; R2 + R3
	vsubpd	zmm6, zmm6, zmm7	;; R3 - R5			;; R2 - R3

	vmulpd	zmm1, zmm14, zmm5	;; 0.5 * (R2 + R6)		;; 0.5 * (I2 + I3)
	vmovapd	zmm2, mem2		;; R4				;; I1
	vaddpd	zmm5, zmm2, zmm5	;; final R1 #2 = R4 + R2 + R6	;; I1 + I2 + I3 (final I1)
	vmulpd	zmm7, zmm14, zmm0	;; 0.5 * (R3 + R5)		;; 0.5 * (R2 + R3)
	vmulpd	zmm4, zmm13, zmm4	;; X = 0.866 * (R2 - R6)	;; B = 0.866 * (I2 - I3)
	vmulpd	zmm6, zmm13, zmm6	;; Y = 0.866 * (R3 - R5)	;; Y = 0.866 * (R2 - R3)
	vmovapd	zmm3, mem1		;; R1				;; R1
	vaddpd	zmm0, zmm3, zmm0	;; final R1 #1 = R1 + R3 + R5	;; R1 + R2 + R3 (final R1)
	vsubpd	zmm2, zmm2, zmm1	;; B = R4 - 0.5 * (R2 + R6)	;; X = (I1-.5I2-.5I3)
	vsubpd	zmm3, zmm3, zmm7	;; A = R1 - 0.5 * (R3 + R5)	;; A = (R1-.5R2-.5R3)

;;	vblendpd zmm1, zmm2, zmm4, 1	;; X				;; X
	vsubpd	zmm7, zmm1, zmm6	;; X - Y (final I3)		;; X - Y (final I3)
	vaddpd	zmm1, zmm1, zmm6	;; X + Y (final I2)		;; X + Y (final I2)
;;	vblendpd zmm4, zmm4, zmm2, 1	;; B				;; B
	vaddpd	zmm6, zmm3, zmm4	;; A + B (final R3)		;; A + B (final R3)
	vsubpd	zmm3, zmm3, zmm4	;; A - B (final R2)		;; A - B (final R2)

	vmovapd	zmm2, [screg+64+32]	;; cosine/sine
	vmulpd	zmm4, zmm7, zmm2	;; B3 = I3 * cosine/sine
	vaddpd	zmm4, zmm4, zmm6	;; B3 = B3 + R3
	vmulpd	zmm6, zmm6, zmm2	;; A3 = R3 * cosine/sine
	vsubpd	zmm6, zmm6, zmm7	;; A3 = A3 - I3

	vmovapd	zmm2, [screg+32]	;; cosine/sine
	vmulpd	zmm7, zmm1, zmm2	;; B2 = I2 * cosine/sine
	vaddpd	zmm7, zmm7, zmm3	;; B2 = B2 + R2
	vmulpd	zmm3, zmm3, zmm2	;; A2 = R2 * cosine/sine
	vsubpd	zmm3, zmm3, zmm1	;; A2 = A2 - I2

	vmovapd	zmm1, [screg+64]	;; sine
	vmulpd	zmm4, zmm4, zmm1	;; B3 = B3 * sine (new I3)
	vmulpd	zmm6, zmm6, zmm1	;; A3 = A3 * sine (new R3)
	vmovapd	zmm1, [screg]		;; sine
	vmulpd	zmm7, zmm7, zmm1	;; B2 = B2 * sine (new I2)
	vmulpd	zmm3, zmm3, zmm1	;; A2 = A2 * sine (new R2)
	ENDM

;; Haswell FMA3 version

IF (@INSTR(,%zarch,<FMA3>) NE 0)

zr3_o6r_t3c_djbfft_mem_preload MACRO
	vmovapd	zmm15, ZMM_ONE
	vmovapd	zmm14, ZMM_HALF
	vmovapd	zmm13, ZMM_P866
	ENDM

zr3_o6r_t3c_djbfft_mem MACRO mem1,mem2,mem3,mem4,mem5,mem6,screg
					;; Six-reals comments		;; Three complex comments
	vmovapd	zmm0, mem3		;; R2				;; R2
	vmovapd	zmm1, mem4		;; R5				;; I2
;;	vblendpd zmm4, zmm1, zmm0, 1	;; R2				;; I2
	vmovapd	zmm3, mem6		;; R6				;; I3
	zfmaddpd zmm8, zmm4, zmm15, zmm3 ;; R2 + R6			;; I2 + I3
	zfmsubpd zmm4, zmm4, zmm15, zmm3 ;; R2 - R6			;; I2 - I3
	vmovapd	zmm2, mem5		;; R3				;; R3
;;	vblendpd zmm6, zmm0, zmm2, 1	;; R3				;; R2
;;	vblendpd zmm7, zmm2, zmm1, 1	;; R5				;; R3
	zfmaddpd zmm9, zmm6, zmm15, zmm7 ;; R3 + R5			;; R2 + R3
	zfmsubpd zmm6, zmm6, zmm15, zmm7 ;; R3 - R5			;; R2 - R3

	vmovapd	zmm2, mem2		;; R4				;; I1
	zfmaddpd zmm5, zmm2, zmm15, zmm8 ;; final R1 #2 = R4 + R2 + R6	;; I1 + I2 + I3 (final I1)
	vmulpd	zmm4, zmm13, zmm4	;; X = 0.866 * (R2 - R6)	;; B = 0.866 * (I2 - I3)
	vmulpd	zmm6, zmm13, zmm6	;; Y = 0.866 * (R3 - R5)	;; Y = 0.866 * (R2 - R3)
	vmovapd	zmm3, mem1		;; R1				;; R1
	zfmaddpd zmm0, zmm3, zmm15, zmm9 ;; final R1 #1 = R1 + R3 + R5	;; R1 + R2 + R3 (final R1)
	zfnmaddpd zmm2, zmm14, zmm8, zmm2 ;; B = R4 - 0.5 * (R2 + R6)	;; X = (I1-.5I2-.5I3)
	zfnmaddpd zmm3, zmm14, zmm9, zmm3 ;; A = R1 - 0.5 * (R3 + R5)	;; A = (R1-.5R2-.5R3)

;;	vblendpd zmm1, zmm2, zmm4, 1	;; X				;; X
	zfmsubpd zmm7, zmm1, zmm15, zmm6 ;; X - Y (final I3)		;; X - Y (final I3)
	zfmaddpd zmm1, zmm1, zmm15, zmm6 ;; X + Y (final I2)		;; X + Y (final I2)
;;	vblendpd zmm4, zmm4, zmm2, 1	;; B				;; B
	zfmaddpd zmm6, zmm3, zmm15, zmm4 ;; A + B (final R3)		;; A + B (final R3)
	zfmsubpd zmm3, zmm3, zmm15, zmm4 ;; A - B (final R2)		;; A - B (final R2)

	vmovapd	zmm2, [screg+64+32]	;; cosine/sine
	zfmaddpd zmm4, zmm7, zmm2, zmm6	;; B3 = I3 * cosine/sine + R3
	zfmsubpd zmm6, zmm6, zmm2, zmm7	;; A3 = R3 * cosine/sine - I3

	vmovapd	zmm2, [screg+32]	;; cosine/sine
	zfmaddpd zmm7, zmm1, zmm2, zmm3	;; B2 = I2 * cosine/sine + R2
	zfmsubpd zmm3, zmm3, zmm2, zmm1	;; A2 = R2 * cosine/sine - I2

	vmovapd	zmm1, [screg+64]	;; sine
	vmulpd	zmm4, zmm4, zmm1	;; B3 = B3 * sine (new I3)
	vmulpd	zmm6, zmm6, zmm1	;; A3 = A3 * sine (new R3)
	vmovapd	zmm1, [screg]		;; sine
	vmulpd	zmm7, zmm7, zmm1	;; B2 = B2 * sine (new I2)
	vmulpd	zmm3, zmm3, zmm1	;; A2 = A2 * sine (new R2)
	ENDM

ENDIF

ENDIF

;;
;; ************************************* six-reals-three-complex-unfft variants ******************************************
;;

;; Macro to do one six_reals_unfft and three three_complex_djbunfft.
;; The six-reals operation is done in the lower double of the ZMM
;; register.  The three-complex is done in the high doubles of the
;; ZMM register.   This is REALLY funky, as we do both at the same
;; time within the full zmm register whenever possible.

zr3_3cl_six_reals_three_complex_djbunfft_preload MACRO
	zr3_o6r_t3c_djbunfft_mem_preload
	ENDM

zr3_3cl_six_reals_three_complex_djbunfft MACRO srcreg,srcinc,d1,screg,scinc,maxrpt
	zr3_o6r_t3c_djbunfft_mem [srcreg],[srcreg+32],[srcreg+d1],[srcreg+d1+32],[srcreg+2*d1],[srcreg+2*d1+32],screg
	zstore	[srcreg], zmm1		;; Save R1
	zstore	[srcreg+32], zmm2	;; Save I1
	zstore	[srcreg+d1], zmm4	;; Save R2
	zstore	[srcreg+d1+32], zmm5	;; Save I2
	zstore	[srcreg+2*d1], zmm3	;; Save R3
	zstore	[srcreg+2*d1+32], zmm6	;; Save I3
	bump	srcreg, srcinc
	bump	screg, scinc
	ENDM

zr3_o6r_t3c_djbunfft_mem_preload MACRO
	ENDM

zr3_o6r_t3c_djbunfft_mem MACRO mem1,mem2,mem3,mem4,mem5,mem6,screg
					;; Six-reals comments		;; Three complex comments
	vmovapd	zmm1, mem3				;; R2
	vmovapd	zmm7, [screg+32]			;; cosine/sine
	vmulpd	zmm2, zmm1, zmm7			;; A2 = R2 * cosine/sine
	vmovapd	zmm3, mem5				;; R3
	vmovapd	zmm6, [screg+64+32]			;; cosine/sine
	vmulpd	zmm4, zmm3, zmm6			;; A3 = R3 * cosine/sine
	vmovapd	zmm0, mem4				;; I2
	vaddpd	zmm2, zmm2, zmm0			;; A2 = A2 + I2
	vmovapd	zmm5, mem6				;; I3
	vaddpd	zmm4, zmm4, zmm5			;; A3 = A3 + I3
	vmulpd	zmm0, zmm0, zmm7			;; B2 = I2 * cosine/sine
	vmulpd	zmm5, zmm5, zmm6			;; B3 = I3 * cosine/sine
	vsubpd	zmm0, zmm0, zmm1			;; B2 = B2 - R2
	vsubpd	zmm5, zmm5, zmm3			;; B3 = B3 - R3
	vmovapd	zmm7, [screg]				;; sine
	vmulpd	zmm2, zmm2, zmm7			;; A2 = A2 * sine (final R2)
	vmovapd	zmm6, [screg+64]			;; sine
	vmulpd	zmm4, zmm4, zmm6			;; A3 = A3 * sine (final R3)
	vmulpd	zmm0, zmm0, zmm7			;; B2 = B2 * sine (final I2)
	vmulpd	zmm5, zmm5, zmm6			;; B3 = B3 * sine (final I3)

;;	vblendpd zmm1, zmm2, zmm4, 1	;; R3				; R2
;;	vblendpd zmm4, zmm4, zmm2, 1	;; R2				; R3
	vsubpd	zmm2, zmm1, zmm4	;; R3 - R2			; R2 - R3
	vaddpd	zmm1, zmm1, zmm4	;; R2 + R3			; R2 + R3
	vsubpd	zmm3, zmm0, zmm5	;; I2 - I3			; I2 - I3
	vaddpd	zmm0, zmm0, zmm5	;; I2 + I3			; I2 + I3

	vmovapd	zmm7, ZMM_P866
	vmulpd	zmm3, zmm7, zmm3	;; B = 0.866 * (I2 - I3)	; B = 0.866 * (I2 - I3)
;;	vblendpd zmm4, zmm2, zmm0, 1	;; I2 + I3			; R2 - R3
	vmulpd	zmm4, zmm7, zmm4	;; Y = 0.866 * (I2 + I3)	; Y = 0.866 * (R2 - R3)

	vmovapd	zmm7, ZMM_HALF
	vmulpd	zmm5, zmm7, zmm1	;; 0.5 * (R2 + R3)		; 0.5 * (R2 + R3)
;;	vblendpd zmm6, zmm0, zmm2, 1	;; R3 - R2			; I2 + I3
	vmulpd	zmm0, zmm7, zmm6	;; 0.5 * (R3 - R2)		; 0.5 * (I2 + I3)

	vmovapd	zmm7, mem1		;; R1#1				; R1
	vaddpd	zmm1, zmm7, zmm1	;; R1#1 + R2 + R3 (final R1)	; R1 + R2 + R3 (final R1)
	vsubpd	zmm7, zmm7, zmm5	;; A = R1#1 - 0.5 * (R2 + R3)	; A = (R1-.5R2-.5R3)

	vmovapd	zmm5, mem2		;; R1#2				; I1
	vaddpd	zmm2, zmm5, zmm6	;; R1#2 + (R3 - R2) (final R4)	; I1 + I2 + I3 (final I1)
	vsubpd	zmm5, zmm5, zmm0	;; X = R1#2 - 0.5 * (R3 - R2)	; X = (I1-.5I2-.5I3)
	vsubpd	zmm0, zmm7, zmm3	;; A - B (final R5)		; A - B (final R3)
	vaddpd	zmm7, zmm7, zmm3	;; A + B (final R3)		; A + B (final R2)
	vsubpd	zmm3, zmm5, zmm4	;; X - Y (final R6)		; X - Y (final I2)
	vaddpd	zmm5, zmm5, zmm4	;; X + Y (final R2)		; X + Y (final I3)

;;	vblendpd zmm4, zmm7, zmm5, 1	;; final R2			; final R2
;;	vblendpd zmm6, zmm5, zmm3, 1	;; final R6			; final I3
;;	vblendpd zmm5, zmm3, zmm0, 1	;; final R5			; final I2
;;	vblendpd zmm3, zmm0, zmm7, 1	;; final R3			; final R3
	ENDM


;; 64-bit version

IFDEF X86_64

zr3_o6r_t3c_djbunfft_mem_preload MACRO
	vmovapd	zmm14, ZMM_HALF
	vmovapd	zmm13, ZMM_P866
	ENDM

zr3_o6r_t3c_djbunfft_mem MACRO mem1,mem2,mem3,mem4,mem5,mem6,screg
					;; Six-reals comments		;; Three complex comments
	vmovapd	zmm1, mem3				;; R2
	vmovapd	zmm7, [screg+32]			;; cosine/sine
	vmulpd	zmm2, zmm1, zmm7			;; A2 = R2 * cosine/sine
	vmovapd	zmm3, mem5				;; R3
	vmovapd	zmm6, [screg+64+32]			;; cosine/sine
	vmulpd	zmm4, zmm3, zmm6			;; A3 = R3 * cosine/sine
	vmovapd	zmm0, mem4				;; I2
	vaddpd	zmm2, zmm2, zmm0			;; A2 = A2 + I2
	vmovapd	zmm5, mem6				;; I3
	vaddpd	zmm4, zmm4, zmm5			;; A3 = A3 + I3
	vmulpd	zmm0, zmm0, zmm7			;; B2 = I2 * cosine/sine
	vmulpd	zmm5, zmm5, zmm6			;; B3 = I3 * cosine/sine
	vsubpd	zmm0, zmm0, zmm1			;; B2 = B2 - R2
	vsubpd	zmm5, zmm5, zmm3			;; B3 = B3 - R3
	vmovapd	zmm7, [screg]				;; sine
	vmulpd	zmm2, zmm2, zmm7			;; A2 = A2 * sine (final R2)
	vmovapd	zmm6, [screg+64]			;; sine
	vmulpd	zmm4, zmm4, zmm6			;; A3 = A3 * sine (final R3)
	vmulpd	zmm0, zmm0, zmm7			;; B2 = B2 * sine (final I2)
	vmulpd	zmm5, zmm5, zmm6			;; B3 = B3 * sine (final I3)

;;	vblendpd zmm1, zmm2, zmm4, 1	;; R3				; R2
;;	vblendpd zmm4, zmm4, zmm2, 1	;; R2				; R3
	vsubpd	zmm2, zmm1, zmm4	;; R3 - R2			; R2 - R3
	vaddpd	zmm1, zmm1, zmm4	;; R2 + R3			; R2 + R3
	vsubpd	zmm3, zmm0, zmm5	;; I2 - I3			; I2 - I3
	vaddpd	zmm0, zmm0, zmm5	;; I2 + I3			; I2 + I3

	vmulpd	zmm3, zmm13, zmm3	;; B = 0.866 * (I2 - I3)	; B = 0.866 * (I2 - I3)
;;	vblendpd zmm4, zmm2, zmm0, 1	;; I2 + I3			; R2 - R3
	vmulpd	zmm4, zmm13, zmm4	;; Y = 0.866 * (I2 + I3)	; Y = 0.866 * (R2 - R3)

	vmulpd	zmm5, zmm14, zmm1	;; 0.5 * (R2 + R3)		; 0.5 * (R2 + R3)
;;	vblendpd zmm6, zmm0, zmm2, 1	;; R3 - R2			; I2 + I3
	vmulpd	zmm0, zmm14, zmm6	;; 0.5 * (R3 - R2)		; 0.5 * (I2 + I3)

	vmovapd	zmm7, mem1		;; R1#1				; R1
	vaddpd	zmm1, zmm7, zmm1	;; R1#1 + R2 + R3 (final R1)	; R1 + R2 + R3 (final R1)
	vsubpd	zmm7, zmm7, zmm5	;; A = R1#1 - 0.5 * (R2 + R3)	; A = (R1-.5R2-.5R3)

	vmovapd	zmm5, mem2		;; R1#2				; I1
	vaddpd	zmm2, zmm5, zmm6	;; R1#2 + (R3 - R2) (final R4)	; I1 + I2 + I3 (final I1)
	vsubpd	zmm5, zmm5, zmm0	;; X = R1#2 - 0.5 * (R3 - R2)	; X = (I1-.5I2-.5I3)
	vsubpd	zmm0, zmm7, zmm3	;; A - B (final R5)		; A - B (final R3)
	vaddpd	zmm7, zmm7, zmm3	;; A + B (final R3)		; A + B (final R2)
	vsubpd	zmm3, zmm5, zmm4	;; X - Y (final R6)		; X - Y (final I2)
	vaddpd	zmm5, zmm5, zmm4	;; X + Y (final R2)		; X + Y (final I3)

;;	vblendpd zmm4, zmm7, zmm5, 1	;; final R2			; final R2
;;	vblendpd zmm6, zmm5, zmm3, 1	;; final R6			; final I3
;;	vblendpd zmm5, zmm3, zmm0, 1	;; final R5			; final I2
;;	vblendpd zmm3, zmm0, zmm7, 1	;; final R3			; final R3
	ENDM

;; Haswell FMA3 version

IF (@INSTR(,%zarch,<FMA3>) NE 0)

zr3_o6r_t3c_djbunfft_mem_preload MACRO
	vmovapd	zmm15, ZMM_ONE
	vmovapd	zmm14, ZMM_HALF
	vmovapd	zmm13, ZMM_P866
	ENDM

zr3_o6r_t3c_djbunfft_mem MACRO mem1,mem2,mem3,mem4,mem5,mem6,screg
					;; Six-reals comments		;; Three complex comments
	vmovapd	zmm1, mem3				;; R2
	vmovapd	zmm0, mem4				;; I2
	vmovapd	zmm7, [screg+32]			;; cosine/sine
	zfmaddpd zmm2, zmm1, zmm7, zmm0			;; A2 = R2 * cosine/sine + I2
	zfmsubpd zmm0, zmm0, zmm7, zmm1			;; B2 = I2 * cosine/sine - R2
	vmovapd	zmm3, mem5				;; R3
	vmovapd	zmm5, mem6				;; I3
	vmovapd	zmm6, [screg+64+32]			;; cosine/sine
	zfmaddpd zmm4, zmm3, zmm6, zmm5			;; A3 = R3 * cosine/sine + I3
	zfmsubpd zmm5, zmm5, zmm6, zmm3			;; B3 = I3 * cosine/sine - R3
	vmovapd	zmm7, [screg]				;; sine
	vmulpd	zmm2, zmm2, zmm7			;; A2 = A2 * sine (final R2)
	vmovapd	zmm6, [screg+64]			;; sine
	vmulpd	zmm4, zmm4, zmm6			;; A3 = A3 * sine (final R3)
	vmulpd	zmm0, zmm0, zmm7			;; B2 = B2 * sine (final I2)
	vmulpd	zmm5, zmm5, zmm6			;; B3 = B3 * sine (final I3)

;;	vblendpd zmm1, zmm2, zmm4, 1	;; R3				; R2
;;	vblendpd zmm4, zmm4, zmm2, 1	;; R2				; R3
	zfmsubpd zmm2, zmm1, zmm15, zmm4 ;; R3 - R2			; R2 - R3
	zfmaddpd zmm1, zmm1, zmm15, zmm4 ;; R2 + R3			; R2 + R3
	zfmsubpd zmm3, zmm0, zmm15, zmm5 ;; I2 - I3			; I2 - I3
	zfmaddpd zmm0, zmm0, zmm15, zmm5 ;; I2 + I3			; I2 + I3

;;	vblendpd zmm4, zmm2, zmm0, 1	;; I2 + I3			; R2 - R3
;;	vblendpd zmm6, zmm0, zmm2, 1	;; R3 - R2			; I2 + I3

	vmovapd	zmm5, mem1		;; R1#1				; R1
	zfnmaddpd zmm7, zmm14, zmm1, zmm5 ;; A = R1#1 - 0.5 * (R2 + R3)	; A = (R1-.5R2-.5R3)
	zfmaddpd zmm1, zmm5, zmm15, zmm1 ;; R1#1 + R2 + R3 (final R1)	; R1 + R2 + R3 (final R1)

	vmovapd	zmm5, mem2		;; R1#2				; I1
	zfmaddpd zmm2, zmm5, zmm15, zmm6 ;; R1#2 + (R3 - R2) (final R4)	; I1 + I2 + I3 (final I1)
	zfnmaddpd zmm5, zmm14, zmm6, zmm5 ;; X = R1#2 - 0.5 * (R3 - R2)	; X = (I1-.5I2-.5I3)
	zfnmaddpd zmm0, zmm13, zmm3, zmm7 ;; A - 0.866 * (I2 - I3) (final R5) ; A - 0.866 * (I2 - I3) (final R3)
	zfmaddpd zmm7, zmm13, zmm3, zmm7 ;; A + 0.866 * (I2 - I3) (final R3) ; A + 0.866 * (I2 - I3) (final R2)
	zfnmaddpd zmm3, zmm13, zmm4, zmm5 ;; X - 0.866 * (I2 + I3) (final R6) ; X - 0.866 * (R2 - R3) (final I2)
	zfmaddpd zmm5, zmm13, zmm4, zmm5 ;; X + 0.866 * (I2 + I3) (final R2) ; X + 0.866 * (R2 - R3) (final I3)

;;	vblendpd zmm4, zmm7, zmm5, 1	;; final R2			; final R2
;;	vblendpd zmm6, zmm5, zmm3, 1	;; final R6			; final I3
;;	vblendpd zmm5, zmm3, zmm0, 1	;; final R5			; final I2
;;	vblendpd zmm3, zmm0, zmm7, 1	;; final R3			; final R3
	ENDM

ENDIF

ENDIF

;;
;; Macros for a radix-6 first step in an AVX real FFT.
;;

;;
;; ************************************* 24-reals-first-fft variants ******************************************
;;

;; These macros operate on 24 reals doing 4.585 levels of the FFT, applying
;; the sin/cos multipliers afterwards.  The output is 2 reals and 11 complex numbers.

;; To calculate a 24-reals FFT, we calculate 24 complex values in a brute force way (using a shorthand notation):
;; r1 + r2 + ... + r24	*  w^0000000000...
;; r1 + r2 + ... + r24	*  w^0123456789A...
;; r1 + r2 + ... + r24	*  w^02468ACE....
;;    ...
;; r1 + r2 + ... + r24	*  w^...A987654321
;; Note that Hermetian szmmetry means we won't need to calculate the last 12 complex values.
;;
;; The sin/cos values (w = 24th root of unity) are:
;; w^1 = .966 + .259i
;; w^2 = .866 + .5i
;; w^3 = .707 + .707i
;; w^4 = .5   + .866i
;; w^5 = .259 + .966i
;; w^6 = 0 + 1i
;; w^7 = -.259 + .966i
;; w^8 = -.5   + .866i
;; w^9 = -.707 + .707i
;; w^10 = -.866 + .5i
;; w^11 = -.966 + .259i
;; w^12 = -1
;;
;; Applying the sin/cos values above (and noting that combining r2 and r24, r3 and r23, etc. will simplify calculations):
;; reals:
;; r1     +(r2+r24)     +(r3+r23)     +(r4+r22)     +(r5+r21)     +(r6+r20) + (r7+r19)     +(r8+r18)     +(r9+r17)     +(r10+r16)     +(r11+r15)     +(r12+r14) + r13
;; r1 +.966(r2+r24) +.866(r3+r23) +.707(r4+r22) +.500(r5+r21) +.259(r6+r20)            -.259(r8+r18) -.500(r9+r17) -.707(r10+r16) -.866(r11+r15) -.966(r12+r14) - r13
;; r1 +.866(r2+r24) +.500(r3+r23)               -.500(r5+r21) -.866(r6+r20) - (r7+r19) -.866(r8+r18) -.500(r9+r17)                +.500(r11+r15) +.866(r12+r14) + r13
;; r1 +.707(r2+r24)               -.707(r4+r22)     -(r5+r21) -.707(r6+r20)            +.707(r8+r18)     +(r9+r17) +.707(r10+r16)                -.707(r12+r14) - r13
;; r1 +.500(r2+r24) -.500(r3+r23)     -(r4+r22) -.500(r5+r21) +.500(r6+r20) + (r7+r19) +.500(r8+r18) -.500(r9+r17)     -(r10+r16) -.500(r11+r15) +.500(r12+r14) + r13
;; r1 +.259(r2+r24) -.866(r3+r23) -.707(r4+r22) +.500(r5+r21) +.966(r6+r20)            -.966(r8+r18) -.500(r9+r17) +.707(r10+r16) +.866(r11+r15) -.259(r12+r14) - r13
;; r1                   -(r3+r23)                   +(r5+r21)               - (r7+r19)                   +(r9+r17)                    -(r11+r15)                + r13
;; r1 -.259(r2+r24) -.866(r3+r23) +.707(r4+r22) +.500(r5+r21) -.966(r6+r20)            +.966(r8+r18) -.500(r9+r17) -.707(r10+r16) +.866(r11+r15) +.259(r12+r14) - r13
;; r1 -.500(r2+r24) -.500(r3+r23)     +(r4+r22) -.500(r5+r21) -.500(r6+r20) + (r7+r19) -.500(r8+r18) -.500(r9+r17)     +(r10+r16) -.500(r11+r15) -.500(r12+r14) + r13
;; r1 -.707(r2+r24)               +.707(r4+r22)     -(r5+r21) +.707(r6+r20)            -.707(r8+r18)     +(r9+r17) -.707(r10+r16)                +.707(r12+r14) - r13
;; r1 -.866(r2+r24) +.500(r3+r23)               -.500(r5+r21) +.866(r6+r20) - (r7+r19) +.866(r8+r18) -.500(r9+r17)                +.500(r11+r15) -.866(r12+r14) + r13
;; r1 -.966(r2+r24) +.866(r3+r23) -.707(r4+r22) +.500(r5+r21) -.259(r6+r20)            +.259(r8+r18) -.500(r9+r17) +.707(r10+r16) -.866(r11+r15) +.966(r12+r14) - r13
;; r1     -(r2+r24)     +(r3+r23)     -(r4+r22)     +(r5+r21)     -(r6+r20) + (r7+r19)     -(r8+r18)     +(r9+r17)     -(r10+r16)     +(r11+r15)     -(r12+r14) + r13
;;
;; imaginarys:
;; 0
;; +.259(r2-r24) +.500(r3-r23) +.707(r4-r22) +.866(r5-r21) +.966(r6-r20) + (r7-r19) +.966(r8-r18) +.866(r9-r17) +.707(r10-r16) +.500(r11-r15) +.259(r12-r14)
;; +.500(r2-r24) +.866(r3-r23)     +(r4-r22) +.866(r5-r21) +.500(r6-r20)            -.500(r8-r18) -.866(r9-r17)     -(r10-r16) -.866(r11-r15) -.500(r12-r14)
;; +.707(r2-r24)     +(r3-r23) +.707(r4-r22)               -.707(r6-r20) - (r7-r19) -.707(r8-r18)               +.707(r10-r16)     +(r11-r15) +.707(r12-r14)
;; +.866(r2-r24) +.866(r3-r23)               -.866(r5-r21) -.866(r6-r20)            +.866(r8-r18) +.866(r9-r17)                -.866(r11-r15) -.866(r12-r14)
;; +.966(r2-r24) +.500(r3-r23) -.707(r4-r22) -.866(r5-r21) +.259(r6-r20) + (r7-r19) +.259(r8-r18) -.866(r9-r17) -.707(r10-r16) +.500(r11-r15) +.966(r12-r14)
;;      (r2-r24)                   -(r4-r22)                   +(r6-r20)                -(r8-r18)                   +(r10-r16)                    -(r12-r14)
;; +.966(r2-r24) -.500(r3-r23) -.707(r4-r22) +.866(r5-r21) +.259(r6-r20) - (r7-r19) +.259(r8-r18) +.866(r9-r17) -.707(r10-r16) -.500(r11-r15) +.966(r12-r14)
;; +.866(r2-r24) -.866(r3-r23)               +.866(r5-r21) -.866(r6-r20)            +.866(r8-r18) -.866(r9-r17)                +.866(r11-r15) -.866(r12-r14)
;; +.707(r2-r24)     -(r3-r23) +.707(r4-r22)               -.707(r6-r20) + (r7-r19) -.707(r8-r18)               +.707(r10-r16)     -(r11-r15) +.707(r12-r14)
;; +.500(r2-r24) -.866(r3-r23)     +(r4-r22) -.866(r5-r21) +.500(r6-r20)            -.500(r8-r18) +.866(r9-r17)     -(r10-r16) +.866(r11-r15) -.500(r12-r14)
;; +.259(r2-r24) -.500(r3-r23) +.707(r4-r22) -.866(r5-r21) +.966(r6-r20) - (r7-r19) +.966(r8-r18) -.866(r9-r17) +.707(r10-r16) -.500(r11-r15) +.259(r12-r14)
;; 0
;;
;; There are many more szmmetries we can take advantage of.   For example, the (r2+/-r24) column
;; always has the same multiplier as the (r12+/-r14) column.  This is true for all the "even" columns.
;; Also the computations for the 2nd row are very similar to the computations for the 12th row,
;; the 3rd row are similar to the 11th, etc.  Finally, note that for the odd columns, there are
;; only three multipliers to apply and can be combined with every fourth column.
;;
;; Lastly, output would normally be 11 complex and 2 reals.  but the users of this routine
;; expect us to "back up" the 2 reals by one level.  That is:
;;	real #1A:  r1 + r3+r23 + r5+r21 + ...
;;	real #1B:  r2+r24 + r4+r22 + ...

;; Simplifying, we get:
;; r2/12  = r2o +/- .707*r2e	r3/11 = r3o +/- .866 r3e	r4/10 = r4o +/- .707*r4e	r5/9 = r5o +/- r5e	r6/8 = r6o +/- .707*r6e
;; r1a = r17a + r17b,  r7 = r17a - r17b
;;
;; r15eA =         +((r2+r24) + (r12+r14))                                  +((r6+r20) + (r8+r18))
;; r1b =  r15eA                            +((r4+r22) + (r10+r16))
;; r2e =  +.966/.707((r2+r24) - (r12+r14)) +((r4+r22) - (r10+r16)) +.259/.707((r6+r20) - (r8+r18))
;; r3e =           +((r2+r24) + (r12+r14))                                  -((r6+r20) + (r8+r18))
;; r4e =           +((r2+r24) - (r12+r14)) -((r4+r22) - (r10+r16))          -((r6+r20) - (r8+r18))                                        
;; r5e =  +.500*r15eA                      -((r4+r22) + (r10+r16))
;; r6e =  +.259/.707((r2+r24) - (r12+r14)) -((r4+r22) - (r10+r16)) +.966/.707((r6+r20) - (r8+r18))
;; r17a=   r1+r13     +((r5+r21) + (r9+r17)) 
;; r17b=                                         +((r3+r23) + (r11+r15)) + (r7+r19)
;; r26oA = r1-r13 +.500((r5+r21) - (r9+r17))
;; r26oB =                                   +.866((r3+r23) - (r11+r15))
;; r35oA = r1+r13 -.500((r5+r21) + (r9+r17))
;; r35oB =                                   +.500((r3+r23) + (r11+r15)) - (r7+r19)
;; r2o =   r26oA                             + r26oB
;; r3o =   r35oA                             + r35oB
;; r4o =   r1-r13     -((r5+r21) - (r9+r17)) 
;; r5o =   r35oA                             - r35oB
;; r6o =   r26oA                             - r26oB

;; i2/12 = .707*i2e +/- i2o		i3/11 = i3e +/- .866 i3o	i4/10 = .707*i4e +/- i4o
;; i5/9  = .866*i5e +/- .866*i5o	i6/8  = .707*i6e +/- i6o
;;
;; i26oA = +.5((r3-r23) + (r11-r15))                        + (r7-r19)
;; i26oB =                           +.866((r5-r21) + (r9-r17))
;; i2o = i26oA + io26B
;; i3o =     +((r3-r23) - (r11-r15))     +((r5-r21) - (r9-r17))
;; i4o =     +((r3-r23) + (r11-r15))                        - (r7-r19)
;; i5o =     +((r3-r23) - (r11-r15))     -((r5-r21) - (r9-r17))
;; i6o = i26oA - io26B
;; i37eA =         ((r2-r24) - (r12-r14))                                  +((r6-r20) - (r8-r18))
;; i2e = +.259/.707((r2-r24) + (r12-r14)) +((r4-r22) + (r10-r16)) +.966/.707((r6-r20) + (r8-r18))
;; i3e = .500*i37eA                       +((r4-r22) - (r10-r16))
;; i4e =          +((r2-r24) + (r12-r14)) +((r4-r22) + (r10-r16))          -((r6-r20) + (r8-r18))
;; i5e =          +((r2-r24) - (r12-r14))                                  -((r6-r20) - (r8-r18))
;; i6e = +.966/.707((r2-r24) + (r12-r14)) -((r4-r22) + (r10-r16)) +.259/.707((r6-r20) + (r8-r18))
;; i7 =  i37eA                            -((r4-r22) - (r10-r16))

yr6_12cl_24_reals_fft_preload MACRO
	ENDM

yr6_12cl_24_reals_fft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd

	;; Do the odd columns for the real results

	vmovapd	zmm0, [srcreg+2*d2]		;; r5
	vaddpd	zmm0, zmm0, [srcreg+4*d2+32]	;; r5+r21						; 1-3
	vmovapd	zmm1, [srcreg+4*d2]		;; r9
	vaddpd	zmm1, zmm1, [srcreg+2*d2+32]	;; r9+r17						; 2-4
	vmovapd	zmm3, [srcreg]			;; r1
	vmovapd	zmm4, [srcreg+32]		;; r13

	vaddpd	zmm2, zmm3, zmm4		;; r1+r13						; 3-5
	vsubpd	zmm3, zmm3, zmm4		;; r1-r13						; 4-6

	vsubpd	zmm4, zmm0, zmm1		;; (r5+r21)-(r9+r17)					; 5-7
	vaddpd	zmm0, zmm0, zmm1		;; (r5+r21)+(r9+r17)					; 6-8

	vmovapd	zmm1, [srcreg+d2]		;; r3
	vaddpd	zmm1, zmm1, [srcreg+5*d2+32]	;; r3+r23						; 7-9

	vsubpd	zmm5, zmm3, zmm4		;; r1-r13 - ((r5+r21)-(r9+r17)) (r4o)			; 8-10
	vmovapd zmm7, ZMM_HALF
	vmulpd	zmm4, zmm7, zmm4		;; .5*((r5+r21)-(r9+r17))				; 8-12

	vaddpd	zmm6, zmm2, zmm0		;; r1+r13 + ((r5+r21)+(r9+r17)) (r17a)			; 9-11
	vmulpd	zmm0, zmm7, zmm0		;; .5*((r5+r21)+(r9+r17))				; 9-13

	vmovapd	zmm7, [srcreg+5*d2]		;; r11
	vaddpd	zmm7, zmm7, [srcreg+d2+32]	;; r11+r15						; 10-12

	zstore	ZMM_TMPS[0*32], zmm5		;; Real odd-cols row #4
	vmovapd	zmm5, [srcreg+3*d2]		;; r7
	vaddpd	zmm5, zmm5, [srcreg+3*d2+32]	;; r7+r19						; 11-13

	vaddpd	zmm3, zmm3, zmm4		;; r1-r13 + .5*((r5+r21)-(r9+r17)) (r26oA)		; 13-15
	L1prefetchw srcreg+L1pd, L1pt

	vsubpd	zmm2, zmm2, zmm0		;; r1+r13 - .5*((r5+r21)+(r9+r17)) (r35oA)		; 14-16

	vaddpd	zmm4, zmm1, zmm7		;; (r3+r23)+(r11+r15)					; 15-17
	L1prefetchw srcreg+d1+L1pd, L1pt

	vsubpd	zmm1, zmm1, zmm7		;; (r3+r23)-(r11+r15)					; 16-18

	vaddpd	zmm0, zmm4, zmm5		;; ((r3+r23)+(r11+r15)) + r7+r19 (r17b)			; 18-20
	vmulpd	zmm4, zmm4, ZMM_HALF		;; .5*((r3+r23)+(r11+r15))				; 18-22

	vmulpd	zmm1, zmm1, ZMM_P866		;; .866*((r3+r23)-(r11+r15)) (r26oB)			; 19-23
	L1prefetchw srcreg+d2+L1pd, L1pt

	vaddpd	zmm7, zmm6, zmm0		;; r17a + r17b (r1a)					; 21-23

	vsubpd	zmm6, zmm6, zmm0		;; r17a - r17b (r7)					; 22-24
	vmovapd	zmm0, [srcreg+d1]		;; r2

	vsubpd	zmm4, zmm4, zmm5		;; .5*((r3+r23)+(r11+r15)) - (r7+r19) (r35oB)		; 23-25
	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	vaddpd	zmm5, zmm3, zmm1		;; r26oA + r26oB (r2o)					; 24-26

	vsubpd	zmm3, zmm3, zmm1		;; r26oA - r26oB (r6o)					; 25-27
	L1prefetchw srcreg+2*d2+L1pd, L1pt

	vaddpd	zmm1, zmm2, zmm4		;; r35oA + r35oB (r3o)					; 26-28

	vsubpd	zmm2, zmm2, zmm4		;; r35oA - r35oB (r5o)					; 27-29
	L1prefetchw srcreg+2*d2+d1+L1pd, L1pt

	;; Do the even columns for the real results

	vaddpd	zmm0, zmm0, [srcreg+5*d2+d1+32]	;; r2+r24						; 28-30

	vmovapd	zmm4, [srcreg+5*d2+d1]		;; r12
	vaddpd	zmm4, zmm4, [srcreg+d1+32]	;; r12+r14						; 29-31

	zstore	[srcreg], zmm7			;; Final real #1A					; 24
	vmovapd	zmm7, [srcreg+2*d2+d1]		;; r6
	vaddpd	zmm7, zmm7, [srcreg+3*d2+d1+32]	;; r6+r20						; 30-32

	zstore	ZMM_TMPS[1*32], zmm6		;; Real row #7						; 25
	vmovapd	zmm6, [srcreg+3*d2+d1]		;; r8
	vaddpd	zmm6, zmm6, [srcreg+2*d2+d1+32]	;; r8+r18						; 31-33

	zstore	ZMM_TMPS[2*32], zmm5		;; Real odd-cols row #2					; 27
	vaddpd	zmm5, zmm0, zmm4		;; (r2+r24)+(r12+r14)					; 32-34

	vsubpd	zmm0, zmm0, zmm4		;; (r2+r24)-(r12+r14)					; 33-35
	L1prefetchw srcreg+3*d2+L1pd, L1pt

	vaddpd	zmm4, zmm7, zmm6		;; (r6+r20)+(r8+r18)					; 34-36

	vsubpd	zmm7, zmm7, zmm6		;; (r6+r20)-(r8+r18)					; 35-37

	vmovapd	zmm6, [srcreg+d2+d1]		;; r4
	vaddpd	zmm6, zmm6, [srcreg+4*d2+d1+32]	;; r4+r22						; 36-38

	zstore	ZMM_TMPS[3*32], zmm3		;; Real odd-cols row #6					; 28
	vmovapd	zmm3, [srcreg+4*d2+d1]		;; r10
	vaddpd	zmm3, zmm3, [srcreg+d2+d1+32]	;; r10+r16						; 37-39

	zstore	ZMM_TMPS[4*32], zmm1		;; Real odd-cols row #3					; 29
	vaddpd	zmm1, zmm5, zmm4		;; ((r2+r24)+(r12+r14)) + ((r6+r20)+(r8+r18)) (r15eA)	; 38-40

	vsubpd	zmm5, zmm5, zmm4		;; ((r2+r24)+(r12+r14)) - ((r6+r20)+(r8+r18)) (r3e)	; 39-41
	L1prefetchw srcreg+3*d2+d1+L1pd, L1pt

	vaddpd	zmm4, zmm6, zmm3		;; (r4+r22)+(r10+r16)					; 40-42

	vsubpd	zmm6, zmm6, zmm3		;; (r4+r22)-(r10+r16)					; 41-43
	vmulpd	zmm3, zmm1, ZMM_HALF		;; .5*r15eA						; 41-45

	zstore	ZMM_TMPS[5*32], zmm2		;; Real odd-cols row #5					; 30
	vsubpd	zmm2, zmm0, zmm7		;; ((r2+r24)-(r12+r14))-((r6+r20)-(r8+r18))		; 42-44
	vmulpd	zmm5, zmm5, ZMM_P866		;; .866*r3e						; 42-46

	vaddpd	zmm1, zmm1, zmm4		;; r15eA + ((r4+r22)+(r10+r16)) (r1b)			; 43-45
	zstore	[srcreg+32], zmm1		;; Save final real #1B (real even-cols row #1)		; 46
	vmulpd	zmm1, zmm0, ZMM_P966		;; .966*((r2+r24)-(r12+r14))				; 43-47

	zstore	ZMM_TMPS[6*32], zmm5		;; Real even-cols row #3				; 47
	vmulpd	zmm5, zmm6, ZMM_SQRTHALF	;; .707*((r4+r22)-(r10+r16))				; 44-48

	vsubpd	zmm2, zmm2, zmm6		;; ((r2+r24)-(r12+r14))-((r6+r20)-(r8+r18))-((r4+r22)-(r10+r16)) (r4e) ; 45-47
	vmulpd	zmm0, zmm0, ZMM_P259		;; .259*((r2+r24)-(r12+r14))				; 45-49

	vsubpd	zmm3, zmm3, zmm4		;; .5*r15eA - ((r4+r22)+(r10+r16)) (r5e)		; 46-48
	vmulpd	zmm6, zmm7, ZMM_P259		;; .259*((r6+r20)-(r8+r18))				; 46-50

	vmovapd	zmm4, [srcreg+d2]		;; r3
	vsubpd	zmm4, zmm4, [srcreg+5*d2+32]	;; r3-r23						; 47-49
	vmulpd	zmm7, zmm7, ZMM_P966		;; .966*((r6+r20)-(r8+r18))				; 47-51

	vmulpd	zmm2, zmm2, ZMM_SQRTHALF	;; .707*r4e						; 48-52
	zstore	ZMM_TMPS[7*32], zmm3		;; Real even-cols row #5				; 49
	vmovapd	zmm3, [srcreg+5*d2]		;; r11
	vsubpd	zmm3, zmm3, [srcreg+d2+32]	;; r11-r15						; 48-50

	vaddpd	zmm1, zmm1, zmm5		;; .966*((r2+r24)-(r12+r14))+.707*((r4+r22)-(r10+r16))	; 49-51

	vsubpd	zmm0, zmm0, zmm5		;; .259*((r2+r24)-(r12+r14))-.707*((r4+r22)-(r10+r16))	; 50-52

	vmovapd	zmm5, [srcreg+2*d2]		;; r5
	vsubpd	zmm5, zmm5, [srcreg+4*d2+32]	;; r5-r21						; 51-53

	vaddpd	zmm1, zmm1, zmm6		;; .966*((r2+r24)-(r12+r14))+.707*((r4+r22)-(r10+r16))+.259*((r6+r20)-(r8+r18)) (r2e) ; 52-54
	L1prefetchw srcreg+4*d2+L1pd, L1pt

	vaddpd	zmm0, zmm0, zmm7		;; .259*((r2+r24)-(r12+r14))-.707*((r4+r22)-(r10+r16))+.966*((r6+r20)-(r8+r18)) (r6e) ; 53-55

	;; Do the odd columns for the imaginary results

	vmovapd	zmm6, [srcreg+4*d2]		;; r9
	vsubpd	zmm6, zmm6, [srcreg+2*d2+32]	;; r9-r17						; 54-56

	vaddpd	zmm7, zmm4, zmm3		;; (r3-r23)+(r11-r15)					; 55-57
	L1prefetchw srcreg+4*d2+d1+L1pd, L1pt

	vsubpd	zmm4, zmm4, zmm3		;; (r3-r23)-(r11-r15)					; 56-58

	vsubpd	zmm3, zmm5, zmm6		;; (r5-r21)-(r9-r17)					; 57-59

	vaddpd	zmm5, zmm5, zmm6		;; (r5-r21)+(r9-r17)					; 58-60
	vmulpd	zmm6, zmm7, ZMM_HALF		;; .5*((r3-r23)+(r11-r15))				; 58-62

	zstore	ZMM_TMPS[8*32], zmm2		;; Real even-cols row #4				; 53
	vmovapd	zmm2, [srcreg+3*d2]		;; r7
	vsubpd	zmm2, zmm2, [srcreg+3*d2+32]	;; r7-r19						; 59-61

	zstore	ZMM_TMPS[9*32], zmm1		;; Real even-cols row #2				; 55
	vaddpd	zmm1, zmm4, zmm3		;; ((r3-r23)-(r11-r15)) + ((r5-r21)-(r9-r17)) (i3o)	; 60-62

	vsubpd	zmm3, zmm4, zmm3		;; ((r3-r23)-(r11-r15)) - ((r5-r21)-(r9-r17)) (i5o)	; 61-63
	vmovapd	zmm4, ZMM_P866
	vmulpd	zmm5, zmm4, zmm5		;; .866*((r5-r21)+(r9-r17)) (i26oB)			; 61-65

	vsubpd	zmm7, zmm7, zmm2		;; ((r3-r23)+(r11-r15)) - (r7-r19) (i4o)		; 62-64
	L1prefetchw srcreg+5*d2+L1pd, L1pt

	vaddpd	zmm6, zmm6, zmm2		;; .5*((r3-r23)+(r11-r15)) + (r7-r19) (i26oA)		; 63-65
	vmulpd	zmm1, zmm4, zmm1		;; .866*i3o						; 63-67

	vmovapd	zmm2, [srcreg+d1]		;; r2
	vsubpd	zmm2, zmm2, [srcreg+5*d2+d1+32]	;; r2-r24						; 64-66
	vmulpd	zmm3, zmm4, zmm3		;; .866*i5o						; 64-68

	zstore	ZMM_TMPS[10*32], zmm0		;; Real even-cols row #6				; 56
	vmovapd	zmm0, [srcreg+5*d2+d1]		;; r12
	vsubpd	zmm0, zmm0, [srcreg+d1+32]	;; r12-r14						; 65-67

	zstore	ZMM_TMPS[11*32], zmm7		;; Imag odd-cols row #4					; 65
	vaddpd	zmm7, zmm6, zmm5		;; i26oA + i26oB (i2o)					; 66-68

	vsubpd	zmm6, zmm6, zmm5		;; i26oA - i26oB (i6o)					; 67-69

	;; Do the even columns for the imaginary results

	vmovapd	zmm5, [srcreg+2*d2+d1]		;; r6
	vsubpd	zmm5, zmm5, [srcreg+3*d2+d1+32]	;; r6-r20						; 68-70

	zstore	ZMM_TMPS[12*32], zmm1		;; Imag odd-cols row #3					; 68
	vmovapd	zmm1, [srcreg+3*d2+d1]		;; r8
	vsubpd	zmm1, zmm1, [srcreg+2*d2+d1+32]	;; r8-r18						; 69-71

	zstore	ZMM_TMPS[13*32], zmm3		;; Imag odd-cols row #5					; 69
	vaddpd	zmm3, zmm2, zmm0		;; (r2-r24)+(r12-r14)					; 70-72

	vsubpd	zmm2, zmm2, zmm0		;; (r2-r24)-(r12-r14)					; 71-73
	L1prefetchw srcreg+5*d2+d1+L1pd, L1pt

	vaddpd	zmm0, zmm5, zmm1		;; (r6-r20)+(r8-r18)					; 72-74

	vsubpd	zmm5, zmm5, zmm1		;; (r6-r20)-(r8-r18)					; 73-75

	vmovapd	zmm1, [srcreg+d2+d1]		;; r4
	vsubpd	zmm1, zmm1, [srcreg+4*d2+d1+32]	;; r4-r22						; 74-76

	zstore	ZMM_TMPS[14*32], zmm7		;; Imag odd-cols row #2					; 69+1
	vmovapd	zmm7, [srcreg+4*d2+d1]		;; r10
	vsubpd	zmm7, zmm7, [srcreg+d2+d1+32]	;; r10-r16						; 75-77

	zstore	ZMM_TMPS[15*32], zmm6		;; Imag odd-cols row #6					; 70+1
	vaddpd	zmm6, zmm2, zmm5		;; ((r2-r24)-(r12-r14)) + ((r6-r20)-(r8-r18)) (i37eA)	; 76-78

	vsubpd	zmm2, zmm2, zmm5		;; ((r2-r24)-(r12-r14)) - ((r6-r20)-(r8-r18)) (i5e)	; 77-79

	vsubpd	zmm5, zmm1, zmm7		;; (r4-r22)-(r10-r16)					; 78-80

	vaddpd	zmm1, zmm1, zmm7		;; (r4-r22)+(r10-r16)					; 79-81
	vmulpd	zmm7, zmm6, ZMM_HALF		;; .5*i37eA						; 79-83

	vmulpd	zmm2, zmm4, zmm2		;; .866*i5e						; 80-84
	vsubpd	zmm4, zmm3, zmm0		;; ((r2-r24)+(r12-r14))-((r6-r20)+(r8-r18))		; 80-82

	vsubpd	zmm6, zmm6, zmm5		;; i37eA - ((r4-r22)-(r10-r16)) (i7)			; 81-83
	zstore	ZMM_TMPS[16*32], zmm6		;; Imag cols row #7					; 84
	vmulpd	zmm6, zmm3, ZMM_P259		;; .259*((r2-r24)+(r12-r14))				; 73-77

	zstore	ZMM_TMPS[17*32], zmm2		;; Imag even-cols row #5				; 85
	vmulpd	zmm2, zmm0, ZMM_P966		;; .966*((r6-r20)+(r8-r18))				; 75-79

	vaddpd	zmm4, zmm4, zmm1		;; ((r2-r24)+(r12-r14))-((r6-r20)+(r8-r18))+((r4-r22)+(r10-r16)) (i4e) ; 83-85
	vmulpd	zmm3, zmm3, ZMM_P966		;; .966*((r2-r24)+(r12-r14))				; 74-78

	vaddpd	zmm7, zmm7, zmm5		;; .5*i37eA + ((r4-r22)-(r10-r16)) (i3e)		; 84-86
	vmulpd	zmm0, zmm0, ZMM_P259		;; .259*((r6-r20)+(r8-r18))				; 76-80

	vaddpd	zmm6, zmm6, zmm2		;; .259*((r2-r24)+(r12-r14))+.966*((r6-r20)+(r8-r18))	; 85-87  (80 at earliest)
	vmulpd	zmm1, zmm1, ZMM_SQRTHALF	;; .707*((r4-r22)+(r10-r16))				; 81-85

	vaddpd	zmm3, zmm3, zmm0		;; .966*((r2-r24)+(r12-r14))+.259*((r6-r20)+(r8-r18))	; 86-88 (81 at earliest)
	vmulpd	zmm4, zmm4, ZMM_SQRTHALF	;; .707*i4e						; 86-90

	zstore	ZMM_TMPS[18*32], zmm7		;; Imag even-cols row #3				; 87

	vaddpd	zmm6, zmm6, zmm1		;; .259*((r2-r24)+(r12-r14))+.966*((r6-r20)+(r8-r18))+.707*((r4-r22)+(r10-r16)) (i2e) ; 88-90 (83 at earliest)

	vsubpd	zmm3, zmm3, zmm1		;; .966*((r2-r24)+(r12-r14))+.259*((r6-r20)+(r8-r18))-.707*((r4-r22)+(r10-r16)) (i6e) ; 89-91 (84 at earliest)

	zstore	ZMM_TMPS[19*32], zmm4		;; Imag even-cols row #4				; 91
;;	zstore	ZMM_TMPS[?*32], zmm6		;; Imag even-cols row #2				; 91+1
	zstore	ZMM_TMPS[20*32], zmm3		;; Imag even-cols row #6				; 92

	;; Now combine the even and odd columns then do the post-multiply by twiddle factors.

	vmovapd	zmm3, ZMM_TMPS[2*32]		;; Real odd-cols row #2
	vmovapd	zmm7, ZMM_TMPS[9*32]		;; Real even-cols row #2
	vsubpd	zmm0, zmm3, zmm7		;; Real #12
	vaddpd	zmm1, zmm3, zmm7		;; Real #2
;;	vmovapd	zmm6, ZMM_TMPS[?*32]		;; Imag even-cols row #2
	vmovapd	zmm5, ZMM_TMPS[14*32]		;; Imag odd-cols row #2
	vsubpd	zmm2, zmm6, zmm5		;; Imag #12
	vaddpd	zmm5, zmm6, zmm5		;; Imag #2

	vmovapd	zmm3, [screg+10*64+32]		;; cosine/sine
	vmulpd	zmm6, zmm0, zmm3		;; A12 = R12 * cosine/sine
	vmovapd	zmm4, [screg+32]		;; cosine/sine
	vmulpd	zmm7, zmm1, zmm4		;; A2 = R2 * cosine/sine
	vsubpd	zmm6, zmm6, zmm2		;; A12 = A12 - I12
	vmulpd	zmm2, zmm2, zmm3		;; B12 = I12 * cosine/sine
	vsubpd	zmm7, zmm7, zmm5		;; A2 = A2 - I2
	vmulpd	zmm5, zmm5, zmm4		;; B2 = I2 * cosine/sine
	vaddpd	zmm2, zmm2, zmm0		;; B12 = B12 + R12
	vmovapd	zmm3, [screg+10*64]		;; sine
	vmulpd	zmm6, zmm6, zmm3		;; A12 = A12 * sine (final R12)
	vaddpd	zmm5, zmm5, zmm1		;; B2 = B2 + R2
	vmovapd	zmm4, [screg]			;; sine
	vmulpd	zmm7, zmm7, zmm4		;; A2 = A2 * sine (final R2)
	vmulpd	zmm2, zmm2, zmm3		;; B12 = B12 * sine (final I12)
	vmulpd	zmm5, zmm5, zmm4		;; B2 = B2 * sine (final I2)
	zstore	[srcreg+5*d2+d1], zmm6		;; Save final R12
	zstore	[srcreg+5*d2+d1+32], zmm2	;; Save final I12
	zstore	[srcreg+d1], zmm7		;; Save final R2
	zstore	[srcreg+d1+32], zmm5		;; Save final I2

	vmovapd	zmm6, ZMM_TMPS[4*32]		;; Real odd-cols row #3
	vmovapd	zmm7, ZMM_TMPS[6*32]		;; Real even-cols row #3
	vsubpd	zmm0, zmm6, zmm7		;; Real #11
	vaddpd	zmm1, zmm6, zmm7		;; Real #3
	vmovapd	zmm6, ZMM_TMPS[18*32]		;; Imag even-cols row #3
	vmovapd	zmm7, ZMM_TMPS[12*32]		;; Imag odd-cols row #3
	vsubpd	zmm2, zmm6, zmm7		;; Imag #11
	vaddpd	zmm3, zmm6, zmm7		;; Imag #3

	vmovapd	zmm5, [screg+9*64+32]		;; cosine/sine
	vmulpd	zmm6, zmm0, zmm5		;; A11 = R11 * cosine/sine
	vmovapd	zmm4, [screg+64+32]		;; cosine/sine
	vmulpd	zmm7, zmm1, zmm4		;; A3 = R3 * cosine/sine
	vsubpd	zmm6, zmm6, zmm2		;; A11 = A11 - I11
	vmulpd	zmm2, zmm2, zmm5		;; B11 = I11 * cosine/sine
	vsubpd	zmm7, zmm7, zmm3		;; A3 = A3 - I3
	vmulpd	zmm3, zmm3, zmm4		;; B3 = I3 * cosine/sine
	vaddpd	zmm2, zmm2, zmm0		;; B11 = B11 + R11
	vmovapd	zmm5, [screg+9*64]		;; sine
	vmulpd	zmm6, zmm6, zmm5		;; A11 = A11 * sine (final R11)
	vaddpd	zmm3, zmm3, zmm1		;; B3 = B3 + R3
	vmovapd	zmm4, [screg+64]		;; sine
	vmulpd	zmm7, zmm7, zmm4		;; A3 = A3 * sine (final R3)
	vmulpd	zmm2, zmm2, zmm5		;; B11 = B11 * sine (final I11)
	vmulpd	zmm3, zmm3, zmm4		;; B3 = B3 * sine (final I3)
	zstore	[srcreg+5*d2], zmm6		;; Save final R11
	zstore	[srcreg+5*d2+32], zmm2		;; Save final I11
	zstore	[srcreg+d2], zmm7		;; Save final R3
	zstore	[srcreg+d2+32], zmm3		;; Save final I3

	vmovapd	zmm6, ZMM_TMPS[0*32]		;; Real odd-cols row #4
	vmovapd	zmm7, ZMM_TMPS[8*32]		;; Real even-cols row #4
	vsubpd	zmm0, zmm6, zmm7		;; Real #10
	vaddpd	zmm1, zmm6, zmm7		;; Real #4
	vmovapd	zmm6, ZMM_TMPS[19*32]		;; Imag even-cols row #4
	vmovapd	zmm7, ZMM_TMPS[11*32]		;; Imag odd-cols row #4
	vsubpd	zmm2, zmm6, zmm7		;; Imag #10
	vaddpd	zmm3, zmm6, zmm7		;; Imag #4

	vmovapd	zmm5, [screg+8*64+32]		;; cosine/sine
	vmulpd	zmm6, zmm0, zmm5		;; A10 = R10 * cosine/sine
	vmovapd	zmm4, [screg+2*64+32]		;; cosine/sine
	vmulpd	zmm7, zmm1, zmm4		;; A4 = R4 * cosine/sine
	vsubpd	zmm6, zmm6, zmm2		;; A10 = A10 - I10
	vmulpd	zmm2, zmm2, zmm5		;; B10 = I10 * cosine/sine
	vsubpd	zmm7, zmm7, zmm3		;; A4 = A4 - I4
	vmulpd	zmm3, zmm3, zmm4		;; B4 = I4 * cosine/sine
	vaddpd	zmm2, zmm2, zmm0		;; B10 = B10 + R10
	vmovapd zmm5, [screg+8*64]		;; sine
	vmulpd	zmm6, zmm6, zmm5		;; A10 = A10 * sine (final R10)
	vaddpd	zmm3, zmm3, zmm1		;; B4 = B4 + R4
	vmovapd	zmm4, [screg+2*64]		;; sine
	vmulpd	zmm7, zmm7, zmm4		;; A4 = A4 * sine (final R4)
	vmulpd	zmm2, zmm2, zmm5		;; B10 = B10 * sine (final I10)
	vmulpd	zmm3, zmm3, zmm4		;; B4 = B4 * sine (final I4)
	zstore	[srcreg+4*d2+d1], zmm6		;; Save final R10
	zstore	[srcreg+4*d2+d1+32], zmm2	;; Save final I10
	zstore	[srcreg+d2+d1], zmm7		;; Save final R4
	zstore	[srcreg+d2+d1+32], zmm3		;; Save final I4

	vmovapd	zmm6, ZMM_TMPS[5*32]		;; Real odd-cols row #5
	vmovapd	zmm7, ZMM_TMPS[7*32]		;; Real even-cols row #5
	vsubpd	zmm0, zmm6, zmm7		;; Real #9
	vaddpd	zmm1, zmm6, zmm7		;; Real #5
	vmovapd	zmm6, ZMM_TMPS[17*32]		;; Imag even-cols row #5
	vmovapd	zmm7, ZMM_TMPS[13*32]		;; Imag odd-cols row #5
	vsubpd	zmm2, zmm6, zmm7		;; Imag #9
	vaddpd	zmm3, zmm6, zmm7		;; Imag #5

	vmovapd	zmm5, [screg+7*64+32]		;; cosine/sine
	vmulpd	zmm6, zmm0, zmm5		;; A9 = R9 * cosine/sine
	vmovapd	zmm4, [screg+3*64+32]		;; cosine/sine
	vmulpd	zmm7, zmm1, zmm4		;; A5 = R5 * cosine/sine
	vsubpd	zmm6, zmm6, zmm2		;; A9 = A9 - I9
	vmulpd	zmm2, zmm2, zmm5		;; B9 = I9 * cosine/sine
	vsubpd	zmm7, zmm7, zmm3		;; A5 = A5 - I5
	vmulpd	zmm3, zmm3, zmm4		;; B5 = I5 * cosine/sine
	vaddpd	zmm2, zmm2, zmm0		;; B9 = B9 + R9
	vmovapd	zmm5, [screg+7*64]		;; sine
	vmulpd	zmm6, zmm6, zmm5		;; A9 = A9 * sine (final R9)
	vaddpd	zmm3, zmm3, zmm1		;; B5 = B5 + R5
	vmovapd	zmm4, [screg+3*64]		;; sine
	vmulpd	zmm7, zmm7, zmm4		;; A5 = A5 * sine (final R5)
	vmulpd	zmm2, zmm2, zmm5		;; B9 = B9 * sine (final I9)
	vmulpd	zmm3, zmm3, zmm4		;; B5 = B5 * sine (final I5)
	zstore	[srcreg+4*d2], zmm6		;; Save final R9
	zstore	[srcreg+4*d2+32], zmm2		;; Save final I9
	zstore	[srcreg+2*d2], zmm7		;; Save final R5
	zstore	[srcreg+2*d2+32], zmm3		;; Save final I5

	vmovapd	zmm6, ZMM_TMPS[3*32]		;; Real odd-cols row #6
	vmovapd	zmm7, ZMM_TMPS[10*32]		;; Real even-cols row #6
	vsubpd	zmm0, zmm6, zmm7		;; Real #8
	vaddpd	zmm1, zmm6, zmm7		;; Real #6
	vmovapd	zmm6, ZMM_TMPS[20*32]		;; Imag even-cols row #6
	vmovapd	zmm7, ZMM_TMPS[15*32]		;; Imag odd-cols row #6
	vsubpd	zmm2, zmm6, zmm7		;; Imag #8
	vaddpd	zmm3, zmm6, zmm7		;; Imag #6

	vmovapd	zmm5, [screg+6*64+32]		;; cosine/sine
	vmulpd	zmm6, zmm0, zmm5		;; A8 = R8 * cosine/sine
	vmovapd	zmm4, [screg+4*64+32]		;; cosine/sine
	vmulpd	zmm7, zmm1, zmm4		;; A6 = R6 * cosine/sine
	vsubpd	zmm6, zmm6, zmm2		;; A8 = A8 - I8
	vmulpd	zmm2, zmm2, zmm5		;; B8 = I8 * cosine/sine
	vsubpd	zmm7, zmm7, zmm3		;; A6 = A6 - I6
	vmulpd	zmm3, zmm3, zmm4		;; B6 = I6 * cosine/sine
	vaddpd	zmm2, zmm2, zmm0		;; B8 = B8 + R8
	vmovapd	zmm5, [screg+6*64]		;; sine
	vmulpd	zmm6, zmm6, zmm5		;; A8 = A8 * sine (final R8)
	vaddpd	zmm3, zmm3, zmm1		;; B6 = B6 + R6
	vmovapd	zmm4, [screg+4*64]		;; sine
	vmulpd	zmm7, zmm7, zmm4		;; A6 = A6 * sine (final R6)
	vmulpd	zmm2, zmm2, zmm5		;; B8 = B8 * sine (final I8)
	vmulpd	zmm3, zmm3, zmm4		;; B6 = B6 * sine (final I6)
	zstore	[srcreg+3*d2+d1], zmm6		;; Save final R8
	zstore	[srcreg+3*d2+d1+32], zmm2	;; Save final I8
	zstore	[srcreg+2*d2+d1], zmm7		;; Save final R6
	zstore	[srcreg+2*d2+d1+32], zmm3	;; Save final I6

	vmovapd	zmm0, ZMM_TMPS[1*32]		;; Real #7
	vmovapd	zmm1, ZMM_TMPS[16*32]		;; Imag #7
	vmovapd	zmm5, [screg+5*64+32]		;; cosine/sine
	vmulpd	zmm4, zmm0, zmm5		;; A7 = R7 * cosine/sine
	vsubpd	zmm4, zmm4, zmm1		;; A7 = A7 - I7
	vmulpd	zmm1, zmm1, zmm5		;; B7 = I7 * cosine/sine
	vaddpd	zmm1, zmm1, zmm0		;; B7 = B7 + R7
	vmovapd	zmm5, [screg+5*64]		;; sine
	vmulpd	zmm4, zmm4, zmm5		;; A7 = A7 * sine (final R7)
	vmulpd	zmm1, zmm1, zmm5		;; B7 = B7 * sine (final I7)
	zstore	[srcreg+3*d2], zmm4		;; Save final R7
	zstore	[srcreg+3*d2+32], zmm1		;; Save final I7

	bump	srcreg, srcinc
	bump	screg, scinc
	ENDM

;; 64-bit version

IFDEF X86_64

yr6_12cl_24_reals_fft_preload MACRO
	ENDM

yr6_12cl_24_reals_fft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd

	;; Do the odd columns for the real results

	vmovapd	zmm0, [srcreg+2*d2]		;; r5
	vaddpd	zmm0, zmm0, [srcreg+4*d2+32]	;; r5+r21						; 1-3

	vmovapd	zmm1, [srcreg+4*d2]		;; r9
	vaddpd	zmm1, zmm1, [srcreg+2*d2+32]	;; r9+r17						; 2-4

	vmovapd	zmm3, [srcreg]			;; r1
	vmovapd	zmm4, [srcreg+32]		;; r13
	vaddpd	zmm2, zmm3, zmm4		;; r1+r13						; 3-5

	vsubpd	zmm3, zmm3, zmm4		;; r1-r13						; 4-6

	vsubpd	zmm4, zmm0, zmm1		;; (r5+r21)-(r9+r17)					; 5-7

	vaddpd	zmm0, zmm0, zmm1		;; (r5+r21)+(r9+r17)					; 6-8

	vmovapd	zmm1, [srcreg+d2]		;; r3
	vaddpd	zmm1, zmm1, [srcreg+5*d2+32]	;; r3+r23						; 7-9

	vsubpd	zmm5, zmm3, zmm4		;; r1-r13 - ((r5+r21)-(r9+r17)) (r4o)			; 8-10
	vmovapd zmm15, ZMM_HALF
	vmulpd	zmm4, zmm15, zmm4		;; .5*((r5+r21)-(r9+r17))				; 8-12

	vaddpd	zmm6, zmm2, zmm0		;; r1+r13 + ((r5+r21)+(r9+r17)) (r17a)			; 9-11
	vmulpd	zmm0, zmm15, zmm0		;; .5*((r5+r21)+(r9+r17))				; 9-13

	vmovapd	zmm7, [srcreg+5*d2]		;; r11
	vaddpd	zmm7, zmm7, [srcreg+d2+32]	;; r11+r15						; 10-12

	vmovapd	zmm8, [srcreg+3*d2]		;; r7
	vaddpd	zmm8, zmm8, [srcreg+3*d2+32]	;; r7+r19						; 11-13

	vmovapd	zmm9, [srcreg+d1]		;; r2
	vaddpd	zmm9, zmm9, [srcreg+5*d2+d1+32]	;; r2+r24						; 12-14

	vaddpd	zmm3, zmm3, zmm4		;; r1-r13 + .5*((r5+r21)-(r9+r17)) (r26oA)		; 13-15
	vmovapd	zmm10, [srcreg+5*d2+d1]		;; r12

	vsubpd	zmm2, zmm2, zmm0		;; r1+r13 - .5*((r5+r21)+(r9+r17)) (r35oA)		; 14-16
	vmovapd	zmm0, [srcreg+2*d2+d1]		;; r6

	vaddpd	zmm4, zmm1, zmm7		;; (r3+r23)+(r11+r15)					; 15-17
	L1prefetchw srcreg+L1pd, L1pt

	vsubpd	zmm1, zmm1, zmm7		;; (r3+r23)-(r11+r15)					; 16-18
	zstore	ZMM_TMPS[0*32], zmm5		;; Real odd-cols row #4					; 11

	vaddpd	zmm10, zmm10, [srcreg+d1+32]	;; r12+r14						; 17-19
	L1prefetchw srcreg+d1+L1pd, L1pt

	vaddpd	zmm7, zmm4, zmm8		;; ((r3+r23)+(r11+r15)) + r7+r19 (r17b)			; 18-20
	vmulpd	zmm4, zmm15, zmm4		;; .5*((r3+r23)+(r11+r15))				; 18-22

	vaddpd	zmm0, zmm0, [srcreg+3*d2+d1+32]	;; r6+r20						; 19-21
	vmovapd	zmm14, ZMM_P866
	vmulpd	zmm1, zmm14, zmm1		;; .866*((r3+r23)-(r11+r15)) (r26oB)			; 19-23

	vmovapd	zmm5, [srcreg+3*d2+d1]		;; r8
	vaddpd	zmm5, zmm5, [srcreg+2*d2+d1+32]	;; r8+r18						; 20-22

	vaddpd	zmm11, zmm6, zmm7		;; r17a + r17b (r1a)					; 21-23
	L1prefetchw srcreg+d2+L1pd, L1pt

	vsubpd	zmm6, zmm6, zmm7		;; r17a - r17b (r7)					; 22-24

	vsubpd	zmm4, zmm4, zmm8		;; .5*((r3+r23)+(r11+r15)) - (r7+r19) (r35oB)		; 23-25
	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	vaddpd	zmm8, zmm3, zmm1		;; r26oA + r26oB (r2o)					; 24-26
	zstore	[srcreg], zmm11			;; Final real #1A					; 24

	vsubpd	zmm3, zmm3, zmm1		;; r26oA - r26oB (r6o)					; 25-27
	zstore	ZMM_TMPS[1*32], zmm6		;; Real row #7						; 25

	vaddpd	zmm1, zmm2, zmm4		;; r35oA + r35oB (r3o)					; 26-28
	L1prefetchw srcreg+2*d2+L1pd, L1pt

	vsubpd	zmm2, zmm2, zmm4		;; r35oA - r35oB (r5o)					; 27-29
	zstore	ZMM_TMPS[2*32], zmm8		;; Real odd-cols row #2					; 27

	;; Do the even columns for the real results

	vaddpd	zmm4, zmm9, zmm10		;; (r2+r24)+(r12+r14)					; 28-30
	zstore	ZMM_TMPS[3*32], zmm3		;; Real odd-cols row #6					; 28

	vsubpd	zmm9, zmm9, zmm10		;; (r2+r24)-(r12+r14)					; 29-31
	vmovapd	zmm6, [srcreg+d2+d1]		;; r4

	vaddpd	zmm10, zmm0, zmm5		;; (r6+r20)+(r8+r18)					; 30-32
	zstore	ZMM_TMPS[4*32], zmm1		;; Real odd-cols row #3					; 29

	vsubpd	zmm0, zmm0, zmm5		;; (r6+r20)-(r8+r18)					; 31-33
	zstore	ZMM_TMPS[5*32], zmm2		;; Real odd-cols row #5					; 30

	vaddpd	zmm6, zmm6, [srcreg+4*d2+d1+32]	;; r4+r22						; 32-34

	vmovapd	zmm3, [srcreg+4*d2+d1]		;; r10
	vaddpd	zmm3, zmm3, [srcreg+d2+d1+32]	;; r10+r16						; 33-35

	vaddpd	zmm1, zmm4, zmm10		;; ((r2+r24)+(r12+r14)) + ((r6+r20)+(r8+r18)) (r15eA)	; 34-36
	vmovapd	zmm13, ZMM_P966

	vsubpd	zmm4, zmm4, zmm10		;; ((r2+r24)+(r12+r14)) - ((r6+r20)+(r8+r18)) (r3e)	; 35-37
	vmovapd	zmm7, [srcreg+5*d2]		;; r11

	vaddpd	zmm8, zmm6, zmm3		;; (r4+r22)+(r10+r16)					; 36-38
	L1prefetchw srcreg+2*d2+d1+L1pd, L1pt

	vsubpd	zmm6, zmm6, zmm3		;; (r4+r22)-(r10+r16)					; 37-39
	vmulpd	zmm3, zmm15, zmm1		;; .5*r15eA						; 37-41

	vsubpd	zmm2, zmm9, zmm0		;; ((r2+r24)-(r12+r14))-((r6+r20)-(r8+r18))		; 38-40
	vmulpd	zmm4, zmm14, zmm4		;; .866*r3e						; 38-42
	L1prefetchw srcreg+3*d2+L1pd, L1pt

	vaddpd	zmm1, zmm1, zmm8		;; r15eA + ((r4+r22)+(r10+r16)) (r1b)			; 39-41
	vmulpd	zmm10, zmm13, zmm9		;; .966*((r2+r24)-(r12+r14))				; 39-43

	vsubpd	zmm7, zmm7, [srcreg+d2+32]	;; r11-r15						; 40-42
	vmovapd	zmm12, ZMM_SQRTHALF
	vmulpd	zmm5, zmm12, zmm6		;; .707*((r4+r22)-(r10+r16))				; 40-44

	vsubpd	zmm2, zmm2, zmm6		;; ((r2+r24)-(r12+r14))-((r6+r20)-(r8+r18))-((r4+r22)-(r10+r16)) (r4e) ; 41-43
	vmovapd	zmm11, ZMM_P259
	vmulpd	zmm9, zmm11, zmm9		;; .259*((r2+r24)-(r12+r14))				; 41-45

	vsubpd	zmm3, zmm3, zmm8		;; .5*r15eA - ((r4+r22)+(r10+r16)) (r5e)		; 42-44
	vmulpd	zmm8, zmm11, zmm0		;; .259*((r6+r20)-(r8+r18))				; 42-46
	zstore	[srcreg+32], zmm1		;; Save final real #1B (real even-cols row #1)		; 42

	vmovapd	zmm1, [srcreg+d2]		;; r3
	vsubpd	zmm1, zmm1, [srcreg+5*d2+32]	;; r3-r23						; 43-45
	vmulpd	zmm0, zmm13, zmm0		;; .966*((r6+r20)-(r8+r18))				; 43-47
	zstore	ZMM_TMPS[6*32], zmm4		;; Real even-cols row #3				; 43

	vmovapd	zmm4, [srcreg+2*d2]		;; r5
	vsubpd	zmm4, zmm4, [srcreg+4*d2+32]	;; r5-r21						; 44-46
	vmulpd	zmm2, zmm12, zmm2		;; .707*r4e						; 44-48

	vaddpd	zmm10, zmm10, zmm5		;; .966*((r2+r24)-(r12+r14))+.707*((r4+r22)-(r10+r16))	; 45-47
	L1prefetchw srcreg+3*d2+d1+L1pd, L1pt

	vsubpd	zmm9, zmm9, zmm5		;; .259*((r2+r24)-(r12+r14))-.707*((r4+r22)-(r10+r16))	; 46-48

	vmovapd	zmm5, [srcreg+4*d2]		;; r9
	vsubpd	zmm5, zmm5, [srcreg+2*d2+32]	;; r9-r17						; 47-49

	vaddpd	zmm10, zmm10, zmm8		;; .966*((r2+r24)-(r12+r14))+.707*((r4+r22)-(r10+r16))+.259*((r6+r20)-(r8+r18)) (r2e) ; 48-50
	L1prefetchw srcreg+4*d2+L1pd, L1pt

	vaddpd	zmm9, zmm9, zmm0		;; .259*((r2+r24)-(r12+r14))-.707*((r4+r22)-(r10+r16))+.966*((r6+r20)-(r8+r18)) (r6e) ; 49-51
	zstore	ZMM_TMPS[7*32], zmm3		;; Real even-cols row #5				; 49

	;; Do the odd columns for the imaginary results

	vaddpd	zmm0, zmm1, zmm7		;; (r3-r23)+(r11-r15)					; 50-52
	L1prefetchw srcreg+4*d2+d1+L1pd, L1pt

	vsubpd	zmm1, zmm1, zmm7		;; (r3-r23)-(r11-r15)					; 51-53
	zstore	ZMM_TMPS[8*32], zmm2		;; Real even-cols row #4				; 49+1

	vsubpd	zmm7, zmm4, zmm5		;; (r5-r21)-(r9-r17)					; 52-54
	vmovapd	zmm2, [srcreg+3*d2]		;; r7

	vaddpd	zmm4, zmm4, zmm5		;; (r5-r21)+(r9-r17)					; 53-55
	vmulpd	zmm5, zmm15, zmm0		;; .5*((r3-r23)+(r11-r15))				; 53-57

	vsubpd	zmm2, zmm2, [srcreg+3*d2+32]	;; r7-r19						; 54-56
	zstore	ZMM_TMPS[9*32], zmm9		;; Real even-cols row #6				; 52

	vaddpd	zmm9, zmm1, zmm7		;; ((r3-r23)-(r11-r15)) + ((r5-r21)-(r9-r17)) (i3o)	; 55-57

	vsubpd	zmm1, zmm1, zmm7		;; ((r3-r23)-(r11-r15)) - ((r5-r21)-(r9-r17)) (i5o)	; 56-58
	vmulpd	zmm4, zmm14, zmm4		;; .866*((r5-r21)+(r9-r17)) (i26oB)			; 56-60
	vmovapd	zmm6, [srcreg+d1]		;; r2

	vsubpd	zmm0, zmm0, zmm2		;; ((r3-r23)+(r11-r15)) - (r7-r19) (i4o)		; 57-59
	L1prefetchw srcreg+5*d2+L1pd, L1pt

	vaddpd	zmm5, zmm5, zmm2		;; .5*((r3-r23)+(r11-r15)) + (r7-r19) (i26oA)		; 58-60
	vmulpd	zmm9, zmm14, zmm9		;; .866*i3o						; 58-62

	vsubpd	zmm6, zmm6, [srcreg+5*d2+d1+32]	;; r2-r24						; 59-61
	vmulpd	zmm1, zmm14, zmm1		;; .866*i5o						; 59-63

	vmovapd	zmm2, [srcreg+5*d2+d1]		;; r12
	vsubpd	zmm2, zmm2, [srcreg+d1+32]	;; r12-r14						; 60-62

	vaddpd	zmm7, zmm5, zmm4		;; i26oA + i26oB (i2o)					; 61-63
	vmovapd	zmm8, [srcreg+2*d2+d1]		;; r6

	vsubpd	zmm5, zmm5, zmm4		;; i26oA - i26oB (i6o)					; 62-64
	zstore	ZMM_TMPS[10*32], zmm0		;; Imag odd-cols row #4					; 60

	;; Do the even columns for the imaginary results

	vsubpd	zmm8, zmm8, [srcreg+3*d2+d1+32]	;; r6-r20						; 63-65

	vmovapd	zmm3, [srcreg+3*d2+d1]		;; r8
	vsubpd	zmm3, zmm3, [srcreg+2*d2+d1+32]	;; r8-r18						; 64-66

	vaddpd	zmm4, zmm6, zmm2		;; (r2-r24)+(r12-r14)					; 65-67
	zstore	ZMM_TMPS[11*32], zmm1		;; Imag odd-cols row #5					; 64

	vsubpd	zmm6, zmm6, zmm2		;; (r2-r24)-(r12-r14)					; 66-68
	vmovapd	zmm1, [srcreg+d2+d1]		;; r4

	vaddpd	zmm2, zmm8, zmm3		;; (r6-r20)+(r8-r18)					; 67-69
	L1prefetchw srcreg+5*d2+d1+L1pd, L1pt

	vsubpd	zmm8, zmm8, zmm3		;; (r6-r20)-(r8-r18)					; 68-70
	zstore	ZMM_TMPS[12*32], zmm5		;; Imag odd-cols row #6					; 65

	vsubpd	zmm1, zmm1, [srcreg+4*d2+d1+32]	;; r4-r22						; 69-71

	vmovapd	zmm0, [srcreg+4*d2+d1]		;; r10
	vsubpd	zmm0, zmm0, [srcreg+d2+d1+32]	;; r10-r16						; 70-72
	vmulpd	zmm3, zmm11, zmm4		;; .259*((r2-r24)+(r12-r14))				; 70-74

	vaddpd	zmm5, zmm6, zmm8		;; ((r2-r24)-(r12-r14)) + ((r6-r20)-(r8-r18)) (i37eA)	; 71-73
	vmulpd	zmm14, zmm13, zmm2		;; .966*((r6-r20)+(r8-r18))				; 71-75

	vsubpd	zmm6, zmm6, zmm8		;; ((r2-r24)-(r12-r14)) - ((r6-r20)-(r8-r18)) (i5e)	; 72-74
	vmulpd	zmm13, zmm13, zmm4		;; .966*((r2-r24)+(r12-r14))				; 72-76

	vaddpd	zmm8, zmm1, zmm0		;; (r4-r22)+(r10-r16)					; 73-75
	vmulpd	zmm11, zmm11, zmm2		;; .259*((r6-r20)+(r8-r18))				; 73-77

	vsubpd	zmm1, zmm1, zmm0		;; (r4-r22)-(r10-r16)					; 74-76
	vmulpd	zmm0, zmm15, zmm5		;; .5*i37eA						; 74-78

	vsubpd	zmm4, zmm4, zmm2		;; ((r2-r24)+(r12-r14))-((r6-r20)+(r8-r18))		; 75-77
	vmulpd	zmm6, zmm6, ZMM_P866		;; .866*i5e						; 75-79

	vaddpd	zmm3, zmm3, zmm14		;; .259*((r2-r24)+(r12-r14))+.966*((r6-r20)+(r8-r18))	; 76-78
	vmulpd	zmm2, zmm12, zmm8		;; .707*((r4-r22)+(r10-r16))				; 76-80
	vmovapd	zmm14, ZMM_TMPS[1*32]		;; Real #7

	vsubpd	zmm5, zmm5, zmm1		;; i37eA - ((r4-r22)-(r10-r16)) (i7)			; 77-79

	vaddpd	zmm13, zmm13, zmm11		;; .966*((r2-r24)+(r12-r14))+.259*((r6-r20)+(r8-r18))	; 78-80
	vmovapd	zmm11, [screg+5*64+32]		;; cosine/sine for R7/I7

	vaddpd	zmm0, zmm0, zmm1		;; .5*i37eA + ((r4-r22)-(r10-r16)) (i3e)		; 79-81

	vaddpd	zmm4, zmm4, zmm8		;; ((r2-r24)+(r12-r14))-((r6-r20)+(r8-r18))+((r4-r22)+(r10-r16)) (i4e) ; 80-82
	vmovapd	zmm8, ZMM_TMPS[2*32]		;; Real odd-cols row #2

	vaddpd	zmm3, zmm3, zmm2		;; .259*((r2-r24)+(r12-r14))+.966*((r6-r20)+(r8-r18))+.707*((r4-r22)+(r10-r16)) (i2e) ; 81-83
	vmulpd	zmm1, zmm14, zmm11		;; A7 = R7 * cosine/sine				;  81-85

	vsubpd	zmm13, zmm13, zmm2		;; .966*((r2-r24)+(r12-r14))+.259*((r6-r20)+(r8-r18))-.707*((r4-r22)+(r10-r16)) (i6e) ; 82-84
	vmulpd	zmm11, zmm5, zmm11		;; B7 = I7 * cosine/sine				;  82-86

	;; Now combine the even and odd columns then do the post-multiply by twiddle factors.

	vsubpd	zmm15, zmm8, zmm10		;; (ro2 - re2) Real #12					; 83-85
	vmulpd	zmm4, zmm12, zmm4		;; .707*i4e						; 83-87

	vsubpd	zmm2, zmm3, zmm7		;; (ie2 - io2) Imag #12					; 84-86
	vmovapd	zmm12, [screg+10*64+32]		;; cosine/sine for R12/I12

	vaddpd	zmm8, zmm8, zmm10		;; (ro2 + re2) Real #2					; 85-87
	zstore	ZMM_TMPS[1*32], zmm6		;; Imag even-cols row #5				; 80

	vaddpd	zmm3, zmm3, zmm7		;; (ie2 + io2) Imag #2					; 86-88
	vmulpd	zmm10, zmm15, zmm12		;; A12 = R12 * cosine/sine				;  86-90
	vmovapd	zmm6, [screg+32]		;; cosine/sine for R2/I2

	vsubpd	zmm1, zmm1, zmm5		;; A7 = A7 - I7						; 87-89
	vmulpd	zmm12, zmm2, zmm12		;; B12 = I12 * cosine/sine				;  87-91
	vmovapd	zmm5, [screg+5*64]		;; sine for R7/I7

	vaddpd	zmm11, zmm11, zmm14		;; B7 = B7 + R7						; 88-90
	vmulpd	zmm7, zmm8, zmm6		;; A2 = R2 * cosine/sine				;  88-92

	vmulpd	zmm6, zmm3, zmm6		;; B2 = I2 * cosine/sine				;  89-93
	vmovapd	zmm14, [screg+10*64]		;; sine for R12/I12

	vmulpd	zmm1, zmm1, zmm5		;; A7 = A7 * sine (final R7)				;  90-94

	vsubpd	zmm10, zmm10, zmm2		;; A12 = A12 - I12					; 91-93
	vmulpd	zmm11, zmm11, zmm5		;; B7 = B7 * sine (final I7)				;  91-95

	vaddpd	zmm12, zmm12, zmm15		;; B12 = B12 + R12					; 92-94
	vmovapd	zmm2, ZMM_TMPS[4*32]		;; Real odd-cols row #3

	vsubpd	zmm7, zmm7, zmm3		;; A2 = A2 - I2						; 93-95
	vmovapd	zmm5, ZMM_TMPS[6*32]		;; Real even-cols row #3

	vaddpd	zmm6, zmm6, zmm8		;; B2 = B2 + R2						; 94-96
	vmulpd	zmm10, zmm10, zmm14		;; A12 = A12 * sine (final R12)				;  94-98
	vmovapd	zmm8, [screg]			;; sine for R2/I2

	vsubpd	zmm15, zmm2, zmm5		;; Real #11						; 95-97
	vmulpd	zmm12, zmm12, zmm14		;; B12 = B12 * sine (final I12)				;  95-99
	vmovapd	zmm14, [screg+9*64+32]		;; cosine/sine for R11/I11

	vsubpd	zmm3, zmm0, zmm9		;; Imag #11						; 96-98
	vmulpd	zmm7, zmm7, zmm8		;; A2 = A2 * sine (final R2)				;  96-100
	zstore	[srcreg+3*d2], zmm1		;; Save final R7					; 95

	vaddpd	zmm2, zmm2, zmm5		;; Real #3						; 97-99
	vmulpd	zmm6, zmm6, zmm8		;; B2 = B2 * sine (final I2)				;  97-101
	vmovapd	zmm1, ZMM_TMPS[0*32]		;; Real odd-cols row #4

	vaddpd	zmm0, zmm0, zmm9		;; Imag #3						; 98-100
	vmulpd	zmm9, zmm15, zmm14		;; A11 = R11 * cosine/sine				;  98-102
	vmovapd	zmm5, ZMM_TMPS[8*32]		;; Real even-cols row #4

	vsubpd	zmm8, zmm1, zmm5		;; Real #10						; 99-101
	vmulpd	zmm14, zmm3, zmm14		;; B11 = I11 * cosine/sine				;  99-103
	zstore	[srcreg+3*d2+32], zmm11		;; Save final I7					; 96

	vmovapd	zmm11, ZMM_TMPS[10*32]		;; Imag odd-cols row #4
	zstore	[srcreg+5*d2+d1], zmm10		;; Save final R12					; 99
	vsubpd	zmm10, zmm4, zmm11		;; Imag #10						; 100-102
	zstore	[srcreg+5*d2+d1+32], zmm12	;; Save final I12					; 100
	vmovapd	zmm12, [screg+64+32]		;; cosine/sine for R3/I3
	zstore	[srcreg+d1], zmm7		;; Save final R2					; 101
	vmulpd	zmm7, zmm2, zmm12		;; A3 = R3 * cosine/sine				;  100-104

	vaddpd	zmm1, zmm1, zmm5		;; Real #4						; 101-103
	vmulpd	zmm12, zmm0, zmm12		;; B3 = I3 * cosine/sine				;  101-105
	vmovapd	zmm5, [screg+8*64+32]		;; cosine/sine for R10/I10

	vaddpd	zmm4, zmm4, zmm11		;; Imag #4						; 102-104
	vmulpd	zmm11, zmm8, zmm5		;; A10 = R10 * cosine/sine				;  102-106
	zstore	[srcreg+d1+32], zmm6		;; Save final I2					; 102

	vsubpd	zmm9, zmm9, zmm3		;; A11 = A11 - I11					; 103-105
	vmulpd	zmm5, zmm10, zmm5		;; B10 = I10 * cosine/sine				;  103-107
	vmovapd	zmm3, [screg+2*64+32]		;; cosine/sine for R4/I4

	vaddpd	zmm14, zmm14, zmm15		;; B11 = B11 + R11					; 104-106
	vmulpd	zmm15, zmm1, zmm3		;; A4 = R4 * cosine/sine				;  104-108
	vmovapd	zmm6, [screg+9*64]		;; sine for R11/I11

	vsubpd	zmm7, zmm7, zmm0		;; A3 = A3 - I3						; 105-107
	vmulpd	zmm3, zmm4, zmm3		;; B4 = I4 * cosine/sine				;  105-109
	vmovapd	zmm0, [screg+64]		;; sine for R3/I3

	vaddpd	zmm12, zmm12, zmm2		;; B3 = B3 + R3						; 106-108
	vmulpd	zmm9, zmm9, zmm6		;; A11 = A11 * sine (final R11)				;  106-110
	vmovapd zmm2, [screg+8*64]		;; sine for R10/I10

	vsubpd	zmm11, zmm11, zmm10		;; A10 = A10 - I10					; 107-109
	vmulpd	zmm14, zmm14, zmm6		;; B11 = B11 * sine (final I11)				;  107-111
	vmovapd	zmm10, [screg+2*64]		;; sine for R4/I4

	vaddpd	zmm5, zmm5, zmm8		;; B10 = B10 + R10					; 108-110
	vmulpd	zmm7, zmm7, zmm0		;; A3 = A3 * sine (final R3)				;  108-112
	vmovapd	zmm6, ZMM_TMPS[5*32]		;; Real odd-cols row #5

	vsubpd	zmm15, zmm15, zmm4		;; A4 = A4 - I4						; 109-111
	vmulpd	zmm12, zmm12, zmm0		;; B3 = B3 * sine (final I3)				;  109-113
	vmovapd	zmm8, ZMM_TMPS[7*32]		;; Real even-cols row #5

	vaddpd	zmm3, zmm3, zmm1		;; B4 = B4 + R4						; 110-112
	vmulpd	zmm11, zmm11, zmm2		;; A10 = A10 * sine (final R10)				;  110-114
	vmovapd	zmm0, ZMM_TMPS[1*32]		;; Imag even-cols row #5

	vsubpd	zmm4, zmm6, zmm8		;; Real #9						; 111-113
	vmulpd	zmm5, zmm5, zmm2		;; B10 = B10 * sine (final I10)				;  111-115
	vmovapd	zmm1, ZMM_TMPS[11*32]		;; Imag odd-cols row #5

	vsubpd	zmm2, zmm0, zmm1		;; Imag #9						; 112-114
	vmulpd	zmm15, zmm15, zmm10		;; A4 = A4 * sine (final R4)				;  112-116
	zstore	[srcreg+5*d2], zmm9		;; Save final R11					; 111
	vmovapd	zmm9, [screg+7*64+32]		;; cosine/sine for R9/I9

	vaddpd	zmm6, zmm6, zmm8		;; Real #5						; 113-115
	vmulpd	zmm3, zmm3, zmm10		;; B4 = B4 * sine (final I4)				;  113-117
	vmovapd	zmm10, ZMM_TMPS[3*32]		;; Real odd-cols row #6

	vaddpd	zmm0, zmm0, zmm1		;; Imag #5						; 114-116
	vmulpd	zmm8, zmm4, zmm9		;; A9 = R9 * cosine/sine				;  114-118
	vmovapd	zmm1, ZMM_TMPS[9*32]		;; Real even-cols row #6
	zstore	[srcreg+5*d2+32], zmm14		;; Save final I11					; 112

	vsubpd	zmm14, zmm10, zmm1		;; Real #8						; 115-117
	vmulpd	zmm9, zmm2, zmm9		;; B9 = I9 * cosine/sine				;  115-119
	zstore	[srcreg+d2], zmm7		;; Save final R3					; 113

	vmovapd	zmm7, ZMM_TMPS[12*32]		;; Imag odd-cols row #6
	zstore	[srcreg+d2+32], zmm12		;; Save final I3					; 114
	vsubpd	zmm12, zmm13, zmm7		;; Imag #8						; 116-118
	zstore	[srcreg+4*d2+d1], zmm11		;; Save final R10					; 115
	vmovapd	zmm11, [screg+3*64+32]		;; cosine/sine for R5/I5
	zstore	[srcreg+4*d2+d1+32], zmm5	;; Save final I10					; 116
	vmulpd	zmm5, zmm6, zmm11		;; A5 = R5 * cosine/sine				;  116-120

	vaddpd	zmm10, zmm10, zmm1		;; Real #6						; 117-119
	vmulpd	zmm11, zmm0, zmm11		;; B5 = I5 * cosine/sine				;  117-121
	vmovapd	zmm1, [screg+6*64+32]		;; cosine/sine for R8/I8

	vaddpd	zmm13, zmm13, zmm7		;; Imag #6						; 118-120
	vmulpd	zmm7, zmm14, zmm1		;; A8 = R8 * cosine/sine				;  118-122
	zstore	[srcreg+d2+d1], zmm15		;; Save final R4					; 117

	vsubpd	zmm8, zmm8, zmm2		;; A9 = A9 - I9						; 119-121
	vmulpd	zmm1, zmm12, zmm1		;; B8 = I8 * cosine/sine				;  119-123
	vmovapd	zmm15, [screg+4*64+32]		;; cosine/sine for R6/I6

	vaddpd	zmm9, zmm9, zmm4		;; B9 = B9 + R9						; 120-122
	vmulpd	zmm2, zmm10, zmm15		;; A6 = R6 * cosine/sine				;  120-124
	vmovapd	zmm4, [screg+7*64]		;; sine for R9/I9

	vsubpd	zmm5, zmm5, zmm0		;; A5 = A5 - I5						; 121-123
	vmulpd	zmm15, zmm13, zmm15		;; B6 = I6 * cosine/sine				;  121-125
	vmovapd	zmm0, [screg+3*64]		;; sine for R5/I5

	vaddpd	zmm11, zmm11, zmm6		;; B5 = B5 + R5						; 122-124
	vmulpd	zmm8, zmm8, zmm4		;; A9 = A9 * sine (final R9)				;  122-126
	vmovapd	zmm6, [screg+6*64]		;; sine for R8/I8

	vsubpd	zmm7, zmm7, zmm12		;; A8 = A8 - I8						; 123-125
	vmulpd	zmm9, zmm9, zmm4		;; B9 = B9 * sine (final I9)				;  123-127
	vmovapd	zmm12, [screg+4*64]		;; sine for R6/I6

	vaddpd	zmm1, zmm1, zmm14		;; B8 = B8 + R8						; 124-126
	vmulpd	zmm5, zmm5, zmm0		;; A5 = A5 * sine (final R5)				;  124-128
	zstore	[srcreg+d2+d1+32], zmm3		;; Save final I4					; 118

	vsubpd	zmm2, zmm2, zmm13		;; A6 = A6 - I6						; 125-127
	vmulpd	zmm11, zmm11, zmm0		;; B5 = B5 * sine (final I5)				;  125-129

	vaddpd	zmm15, zmm15, zmm10		;; B6 = B6 + R6						; 126-128
	vmulpd	zmm7, zmm7, zmm6		;; A8 = A8 * sine (final R8)				;  126-130

	vmulpd	zmm1, zmm1, zmm6		;; B8 = B8 * sine (final I8)				;  127-131
	zstore	[srcreg+4*d2], zmm8		;; Save final R9					; 127

	vmulpd	zmm2, zmm2, zmm12		;; A6 = A6 * sine (final R6)				;  128-132
	zstore	[srcreg+4*d2+32], zmm9		;; Save final I9					; 128

	vmulpd	zmm15, zmm15, zmm12		;; B6 = B6 * sine (final I6)				;  129-133
	zstore	[srcreg+2*d2], zmm5		;; Save final R5					; 129

	zstore	[srcreg+2*d2+32], zmm11		;; Save final I5					; 130
	zstore	[srcreg+3*d2+d1], zmm7		;; Save final R8					; 131
	zstore	[srcreg+3*d2+d1+32], zmm1	;; Save final I8					; 132
	zstore	[srcreg+2*d2+d1], zmm2		;; Save final R6					; 133
	zstore	[srcreg+2*d2+d1+32], zmm15	;; Save final I6					; 134

	bump	srcreg, srcinc
	bump	screg, scinc
	ENDM

;; Haswell FMA3 version
;; This is the original Bulldozer version timed at 85.5 clocks.  Converting adds and subs to FMA3 did not improve the timings.

IF (@INSTR(,%zarch,<FMA3>) NE 0)

yr6_12cl_24_reals_fft_preload MACRO
	vmovapd	zmm15, ZMM_ONE
	ENDM

yr6_12cl_24_reals_fft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd

	;; Do the odd columns for the real results

	vmovapd	zmm0, [srcreg+2*d2]		;; r5
	vaddpd	zmm0, zmm0, [srcreg+4*d2+32]	;; r5+r21						; 1-5		n 7
	vmovapd	zmm1, [srcreg+4*d2]		;; r9
	zfmaddpd zmm1, zmm1, zmm15, [srcreg+2*d2+32] ;; r9+r17						; 1-5		n 7

	vmovapd	zmm2, [srcreg+d2]		;; r3
	vaddpd	zmm2, zmm2, [srcreg+5*d2+32]	;; r3+r23						; 2-6		n 8
	vmovapd	zmm3, [srcreg+5*d2]		;; r11
	zfmaddpd zmm3, zmm3, zmm15, [srcreg+d2+32] ;; r11+r15						; 2-6		n 8

	vmovapd	zmm4, [srcreg+d1]		;; r2
	vaddpd	zmm4, zmm4, [srcreg+5*d2+d1+32]	;; r2+r24						; 3-7		n 9
	vmovapd	zmm5, [srcreg+5*d2+d1]		;; r12
	zfmaddpd zmm5, zmm5, zmm15, [srcreg+d1+32] ;; r12+r14						; 3-7		n 9

	vmovapd	zmm6, [srcreg+2*d2+d1]		;; r6
	vaddpd	zmm6, zmm6, [srcreg+3*d2+d1+32]	;; r6+r20						; 4-8		n 10
	vmovapd	zmm7, [srcreg+3*d2+d1]		;; r8
	zfmaddpd zmm7, zmm7, zmm15, [srcreg+2*d2+d1+32]	;; r8+r18					; 4-8		n 10

	vmovapd	zmm8, [srcreg+d2+d1]		;; r4
	vaddpd	zmm8, zmm8, [srcreg+4*d2+d1+32]	;; r4+r22						; 5-9		n 11
	vmovapd	zmm9, [srcreg+4*d2+d1]		;; r10
	zfmaddpd zmm9, zmm9, zmm15, [srcreg+d2+d1+32] ;; r10+r16					; 5-9		n 11

	vmovapd	zmm10, [srcreg]			;; r1
	vmovapd	zmm11, [srcreg+32]		;; r13
	vsubpd	zmm12, zmm10, zmm11		;; r1-r13						; 6-10		n 12
	zfmaddpd zmm10, zmm10, zmm15, zmm11	;; r1+r13						; 6-10		n 13

	vsubpd	zmm11, zmm0, zmm1		;; (r5+r21)-(r9+r17)					; 7-11		n 12
	zfmaddpd zmm0, zmm0, zmm15, zmm1	;; (r5+r21)+(r9+r17)					; 7-11		n 13

	vmovapd	zmm13, [srcreg+3*d2]		;; r7
	vaddpd	zmm13, zmm13, [srcreg+3*d2+32]	;; r7+r19						; 8-12		n 14
	zfmaddpd zmm1, zmm2, zmm15, zmm3	;; (r3+r23)+(r11+r15)					; 8-12		n 14

	vsubpd	zmm2, zmm2, zmm3		;; (r3+r23)-(r11+r15) (r26oB/.866)			; 9-13
	zfmaddpd zmm3, zmm4, zmm15, zmm5	;; (r2+r24)+(r12+r14)					; 9-13		n 15
	vmovapd zmm14, ZMM_HALF

	vsubpd	zmm4, zmm4, zmm5		;; (r2+r24)-(r12+r14)					; 10-14		n 16
	zfmaddpd zmm5, zmm6, zmm15, zmm7	;; (r6+r20)+(r8+r18)					; 10-14		n 15
	L1prefetchw srcreg+L1pd, L1pt

	vsubpd	zmm6, zmm6, zmm7		;; (r6+r20)-(r8+r18)					; 11-15		n 21
	zfmsubpd zmm7, zmm8, zmm15, zmm9	;; (r4+r22)-(r10+r16)					; 11-15		n 16

	vaddpd	zmm8, zmm8, zmm9		;; (r4+r22)+(r10+r16)					; 12-16		n 32
	zfmaddpd zmm9, zmm14, zmm11, zmm12	;; r1-r13 + .5*((r5+r21)-(r9+r17)) (r26oA)		; 12-16
	L1prefetchw srcreg+d1+L1pd, L1pt

	vsubpd	zmm12, zmm12, zmm11		;; r1-r13 - ((r5+r21)-(r9+r17)) (r4o)			; 13-17
	zfmaddpd zmm11, zmm10, zmm15, zmm0	;; r1+r13 + ((r5+r21)+(r9+r17)) (r17a)			; 13-17		n 19

	zfnmaddpd zmm0, zmm14, zmm0, zmm10	;; r1+r13 - .5*((r5+r21)+(r9+r17)) (r35oA)		; 14-18		n 20
	vaddpd	zmm10, zmm1, zmm13		;; ((r3+r23)+(r11+r15)) + r7+r19 (r17b)			; 14-18		n 19
	zstore	ZMM_TMPS[0*32], zmm2		;; r26oB / .866						; 14

	zfmsubpd zmm1, zmm14, zmm1, zmm13	;; .5*((r3+r23)+(r11+r15)) - (r7+r19) (r35oB)		; 15-19		n 20
	vsubpd	zmm13, zmm3, zmm5		;; ((r2+r24)+(r12+r14)) - ((r6+r20)+(r8+r18)) (r3e/.866) ; 15-19
	vmovapd	zmm2, ZMM_P966_P707

	vaddpd	zmm3, zmm3, zmm5		;; ((r2+r24)+(r12+r14)) + ((r6+r20)+(r8+r18)) (r15eA)	; 16-20		n 32
	zfmsubpd zmm5, zmm4, zmm15, zmm7	;; ((r2+r24)-(r12+r14))-((r4+r22)-(r10+r16))		; 16-20		n 21

	zfmaddpd zmm14, zmm2, zmm4, zmm7	;; .966/.707*((r2+r24)-(r12+r14))+((r4+r22)-(r10+r16))	; 17-21		n 22
	zstore	ZMM_TMPS[1*32], zmm9		;; r26oA						; 17
	vmovapd	zmm9, ZMM_P259_P707
	zfmsubpd zmm4, zmm9, zmm4, zmm7		;; .259/.707*((r2+r24)-(r12+r14))-((r4+r22)-(r10+r16))	; 17-21		n 22

	vmovapd	zmm7, [srcreg+d1]		;; r2
	vsubpd	zmm7, zmm7, [srcreg+5*d2+d1+32]	;; r2-r24						; 18-22		n 27
	zstore	ZMM_TMPS[2*32], zmm12		;; Real odd-cols row #4					; 18
	vmovapd	zmm12, [srcreg+5*d2+d1]		;; r12
	zfmsubpd zmm12, zmm12, zmm15, [srcreg+d1+32] ;; r12-r14						; 18-22		n 27

	zstore	ZMM_TMPS[3*32], zmm13		;; Real even-cols row #3 / .866				; 20
	vaddpd	zmm13, zmm11, zmm10		;; r17a + r17b (r1a)					; 19-23
	zfmsubpd zmm11, zmm11, zmm15, zmm10	;; r17a - r17b (r7)					; 19-23

	vaddpd	zmm10, zmm0, zmm1		;; r35oA + r35oB (r3o)					; 20-24
	zfmsubpd zmm0, zmm0, zmm15, zmm1	;; r35oA - r35oB (r5o)					; 20-24

	vmovapd	zmm1, [srcreg+2*d2+d1]		;; r6
	vsubpd	zmm1, zmm1, [srcreg+3*d2+d1+32]	;; r6-r20						; 21-25		n 28
	zfmsubpd zmm5, zmm5, zmm15, zmm6	;; ((r2+r24)-(r12+r14))-((r4+r22)-(r10+r16))-((r6+r20)-(r8+r18)) (r4e/.707) ; 21-25

	zfmaddpd zmm14, zmm9, zmm6, zmm14	;; .966/.707*((r2+r24)-(r12+r14))+((r4+r22)-(r10+r16))+.259/.707*((r6+r20)-(r8+r18)) (r2e/.707) ; 22-26
	vmovapd	zmm9, [srcreg+3*d2+d1]		;; r8
	vsubpd	zmm9, zmm9, [srcreg+2*d2+d1+32] ;; r8-r18						; 22-26		n 28

	zfmaddpd zmm4, zmm2, zmm6, zmm4		;; .259/.707*((r2+r24)-(r12+r14))-((r4+r22)-(r10+r16))+.966/.707*((r6+r20)-(r8+r18)) (r6e/.707) ; 23-27
	;; There is room for a 1/2 clock stall here.  We'll probably use it as the next few clocks each require 4 loads.

	vmovapd	zmm2, [srcreg+d2+d1]		;; r4
	vsubpd	zmm2, zmm2, [srcreg+4*d2+d1+32]	;; r4-r22						; 24-28		n 29
	vmovapd	zmm6, [srcreg+4*d2+d1]		;; r10
	zfmsubpd zmm6, zmm6, zmm15, [srcreg+d2+d1+32] ;; r10-r16					; 24-28		n 29
	zstore	[srcreg], zmm13			;; Final real #1A					; 24

	vmovapd	zmm13, [srcreg+d2]		;; r3
	vsubpd	zmm13, zmm13, [srcreg+5*d2+32]	;; r3-r23						; 25-29		n 30
	zstore	ZMM_TMPS[4*32], zmm11		;; Real row #7						; 24+1
	vmovapd	zmm11, [srcreg+5*d2]		;; r11
	zfmsubpd zmm11, zmm11, zmm15, [srcreg+d2+32] ;; r11-r15						; 25-29		n 30

	zstore	ZMM_TMPS[5*32], zmm10		;; Real odd-cols row #3					; 25+1
	vmovapd	zmm10, [srcreg+2*d2]		;; r5
	vsubpd	zmm10, zmm10, [srcreg+4*d2+32]	;; r5-r21						; 26-30		n 31
	zstore	ZMM_TMPS[6*32], zmm0		;; Real odd-cols row #5					; 25+2
	vmovapd	zmm0, [srcreg+4*d2]		;; r9
	zfmsubpd zmm0, zmm0, zmm15, [srcreg+2*d2+32] ;; r9-r17						; 26-30		n 31

	zstore	ZMM_TMPS[7*32], zmm5		;; Real even-cols row #4 / .707				; 26+2
	vsubpd	zmm5, zmm7, zmm12		;; (r2-r24)-(r12-r14)					; 27-31		n 33
	zfmaddpd zmm7, zmm7, zmm15, zmm12	;; (r2-r24)+(r12-r14)					; 27-31		n 34
	L1prefetchw srcreg+d2+L1pd, L1pt

	vsubpd	zmm12, zmm1, zmm9		;; (r6-r20)-(r8-r18)					; 28-32		n 33
	zfmaddpd zmm1, zmm1, zmm15, zmm9	;; (r6-r20)+(r8-r18)					; 28-32		n 39

	vaddpd	zmm9, zmm2, zmm6		;; (r4-r22)+(r10-r16)					; 29-33		n 34
	zfmsubpd zmm2, zmm2, zmm15, zmm6	;; (r4-r22)-(r10-r16)					; 29-33		n 38
	vmovapd	zmm6, [srcreg+3*d2]		;; r7
	zstore	ZMM_TMPS[8*32], zmm14		;; Real even-cols row #2 / .707				; 27+2

	vsubpd	zmm6, zmm6, [srcreg+3*d2+32]	;; r7-r19						; 30-34		n 35
	zfmaddpd zmm14, zmm13, zmm15, zmm11	;; (r3-r23)+(r11-r15)					; 30-34		n 35
	zstore	ZMM_TMPS[9*32], zmm4		;; Real even-cols row #6 / .707				; 27+3

	vsubpd	zmm13, zmm13, zmm11		;; (r3-r23)-(r11-r15)					; 31-35		n 37
	zfmsubpd zmm11, zmm10, zmm15, zmm0	;; (r5-r21)-(r9-r17)					; 31-35		n 37
	vmovapd	zmm4, ZMM_HALF

	vaddpd	zmm10, zmm10, zmm0		;; (r5-r21)+(r9-r17)					; 32-36		n 40
	zfmaddpd zmm0, zmm3, zmm15, zmm8	;; r15eA + ((r4+r22)+(r10+r16)) (r1b)			; 32-36
	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	zfmsubpd zmm3, zmm4, zmm3, zmm8		;; .5*r15eA - ((r4+r22)+(r10+r16)) (r5e)		; 33-37		n 44
	vaddpd	zmm8, zmm5, zmm12		;; ((r2-r24)-(r12-r14)) + ((r6-r20)-(r8-r18)) (i37eA)	; 33-37		n 38

	vsubpd	zmm5, zmm5, zmm12		;; ((r2-r24)-(r12-r14)) - ((r6-r20)-(r8-r18)) (i5e)	; 34-38		n 39
	zfmaddpd zmm12, zmm7, zmm15, zmm9	;; ((r2-r24)+(r12-r14))+((r4-r22)+(r10-r16))		; 34-38		n 39
	L1prefetchw srcreg+2*d2+L1pd, L1pt

	zstore	[srcreg+32], zmm0		;; Save final real #1B (real even-cols row #1)		; 37
	zfmaddpd zmm0, zmm4, zmm14, zmm6	;; .5*((r3-r23)+(r11-r15)) + (r7-r19) (i26oA)		; 35-39		n 40
	vsubpd	zmm14, zmm14, zmm6		;; ((r3-r23)+(r11-r15)) - (r7-r19) (i4o)		; 35-39

	zfmaddpd zmm6, zmm7, ZMM_P259_P707, zmm9 ;; .259/.707*((r2-r24)+(r12-r14))+((r4-r22)+(r10-r16)) ; 36-40		n 41
	zstore	ZMM_TMPS[10*32], zmm14		;; Imag odd-cols row #4					; 40
	vmovapd	zmm14, ZMM_P966_P707
	zfmsubpd zmm7, zmm14, zmm7, zmm9	;; .966/.707*((r2-r24)+(r12-r14))-((r4-r22)+(r10-r16))	; 36-40		n 41

	vaddpd	zmm9, zmm13, zmm11		;; ((r3-r23)-(r11-r15)) + ((r5-r21)-(r9-r17)) (i3o/.866) ; 37-41	n 43
	zfmsubpd zmm13, zmm13, zmm15, zmm11	;; ((r3-r23)-(r11-r15)) - ((r5-r21)-(r9-r17)) (i5o/.866) ; 37-41	n 45
	L1prefetchw srcreg+2*d2+d1+L1pd, L1pt

	zfmaddpd zmm11, zmm4, zmm8, zmm2	;; .5*i37eA + ((r4-r22)-(r10-r16)) (i3e)		; 38-42		n 43
	vsubpd	zmm8, zmm8, zmm2		;; i37eA - ((r4-r22)-(r10-r16)) (i7)			; 38-42		n 46

	vmovapd	zmm4, ZMM_P866
	vmulpd	zmm5, zmm4, zmm5		;; .866*i5e						; 39-43		n 45
	vsubpd	zmm12, zmm12, zmm1		;; ((r2-r24)+(r12-r14))+((r4-r22)+(r10-r16))-((r6-r20)+(r8-r18)) (i4e) ; 39-43

	zfmaddpd zmm2, zmm4, zmm10, zmm0	;; i26oA + .866*((r5-r21)+(r9-r17)) (i2o)		; 40-44
	zfnmaddpd zmm0, zmm4, zmm10, zmm0	;; i26oA - .866*((r5-r21)+(r9-r17)) (i6o)		; 40-44

	zfmaddpd zmm6, zmm14, zmm1, zmm6	;; .259/.707*((r2-r24)+(r12-r14))+((r4-r22)+(r10-r16))+.966/.707*((r6-r20)+(r8-r18)) (i2e/.707) ; 41-45
	zfmaddpd zmm7, zmm1, ZMM_P259_P707, zmm7 ;; .966/.707*((r2-r24)+(r12-r14))-((r4-r22)+(r10-r16))+.259/.707*((r6-r20)+(r8-r18)) (i6e/.707) ; 41-45

	vmovapd	zmm10, ZMM_TMPS[5*32]		;; Real odd-cols row #3
	vmovapd	zmm14, ZMM_TMPS[3*32]		;; Real even-cols row #3 / .866
	zfnmaddpd zmm1, zmm4, zmm14, zmm10	;; (ro3 - .866*re3) Real #11				; 42-46
	zfmaddpd zmm14, zmm4, zmm14, zmm10	;; (ro3 + .866*re3) Real #3				; 42-46

	zfnmaddpd zmm10, zmm4, zmm9, zmm11	;; (i3e - .866*i3o) Imag #11				; 43-47
	zfmaddpd zmm9, zmm4, zmm9, zmm11	;; (i3e + .866*i3o) Imag #3				; 43-47

	vmovapd	zmm11, ZMM_TMPS[6*32]		;; Real odd-cols row #5
	zstore	ZMM_TMPS[3*32], zmm12		;; Imag even-cols row #4				; 44
	vsubpd	zmm12, zmm11, zmm3		;; (ro5 - re5) Real #9					; 44-48
	zfmaddpd zmm11, zmm11, zmm15, zmm3	;; (ro5 + re5) Real #5					; 44-48

	zfnmaddpd zmm3, zmm4, zmm13, zmm5	;; (ie5 - .866*io5) Imag #9				; 45-49
	zfmaddpd zmm13, zmm4, zmm13, zmm5	;; (ie5 + .866*io5) Imag #5				; 45-49
	vmovapd	zmm5, ZMM_TMPS[4*32]		;; Real #7
	zstore	ZMM_TMPS[4*32], zmm2		;; Imag odd-cols row #2					; 45

	vmovapd	zmm2, [screg+5*64+32]		;; cosine/sine for R7/I7
	zstore	ZMM_TMPS[5*32], zmm0		;; Imag odd-cols row #6					; 45+1
	zfmsubpd zmm0, zmm5, zmm2, zmm8		;; A7 = R7 * cosine/sine - I7				; 46-50
	zfmaddpd zmm8, zmm8, zmm2, zmm5		;; B7 = I7 * cosine/sine + R7				; 46-50

	vmovapd	zmm2, ZMM_TMPS[1*32]		;; r26oA
	vmovapd	zmm5, ZMM_TMPS[0*32]		;; r26oB / .866
	zstore	ZMM_TMPS[0*32], zmm6		;; Imag even-cols row #2 / .707				; 46+1
	zfmaddpd zmm6, zmm4, zmm5, zmm2		;; r26oA + .866*r26oB (r2o)				; 47-51
	zfnmaddpd zmm4, zmm4, zmm5, zmm2	;; r26oA - .866*r26oB (r6o)				; 47-51

	vmovapd	zmm2, [screg+9*64+32]		;; cosine/sine for R11/I11
	zfmsubpd zmm5, zmm1, zmm2, zmm10	;; A11 = R11 * cosine/sine - I11			; 48-52
	zfmaddpd zmm10, zmm10, zmm2, zmm1	;; B11 = I11 * cosine/sine + R11			; 48-52
	zstore	ZMM_TMPS[1*32], zmm7		;; Imag even-cols row #6 / .707				; 46+2

	vmovapd	zmm2, [screg+64+32]		;; cosine/sine for R3/I3
	zfmsubpd zmm1, zmm14, zmm2, zmm9	;; A3 = R3 * cosine/sine - I3				; 49-53
	zfmaddpd zmm9, zmm9, zmm2, zmm14	;; B3 = I3 * cosine/sine + R3				; 49-53

	vmovapd	zmm2, [screg+7*64+32]		;; cosine/sine for R9/I9
	zfmsubpd zmm14, zmm12, zmm2, zmm3	;; A9 = R9 * cosine/sine - I9				; 50-54
	zfmaddpd zmm3, zmm3, zmm2, zmm12	;; B9 = I9 * cosine/sine + R9				; 50-54
	L1prefetchw srcreg+3*d2+L1pd, L1pt

	vmovapd	zmm2, [screg+3*64+32]		;; cosine/sine for R5/I5
	zfmsubpd zmm12, zmm11, zmm2, zmm13	;; A5 = R5 * cosine/sine - I5				; 51-55
	zfmaddpd zmm13, zmm13, zmm2, zmm11	;; B5 = I5 * cosine/sine + R5				; 51-55

	vmovapd	zmm2, [screg+5*64]		;; sine for R7/I7
	vmulpd	zmm0, zmm0, zmm2		;; A7 = A7 * sine (final R7)				; 52-56
	vmulpd	zmm8, zmm8, zmm2		;; B7 = B7 * sine (final I7)				; 52-56

	vmovapd	zmm2, [screg+9*64]		;; sine for R11/I11
	vmulpd	zmm5, zmm5, zmm2		;; A11 = A11 * sine (final R11)				; 53-57
	vmulpd	zmm10, zmm10, zmm2		;; B11 = B11 * sine (final I11)				; 53-57
	L1prefetchw srcreg+3*d2+d1+L1pd, L1pt

	vmovapd	zmm2, [screg+64]		;; sine for R3/I3
	vmulpd	zmm1, zmm1, zmm2		;; A3 = A3 * sine (final R3)				; 54-58
	vmulpd	zmm9, zmm9, zmm2		;; B3 = B3 * sine (final I3)				; 54-58

	vmovapd	zmm2, [screg+7*64]		;; sine for R9/I9
	vmulpd	zmm14, zmm14, zmm2		;; A9 = A9 * sine (final R9)				; 55-59
	vmulpd	zmm3, zmm3, zmm2		;; B9 = B9 * sine (final I9)				; 55-59

	vmovapd	zmm2, [screg+3*64]		;; sine for R5/I5
	vmulpd	zmm12, zmm12, zmm2		;; A5 = A5 * sine (final R5)				; 56-60
	vmulpd	zmm13, zmm13, zmm2		;; B5 = B5 * sine (final I5)				; 56-60

	vmovapd	zmm11, ZMM_SQRTHALF
	vmovapd	zmm7, ZMM_TMPS[2*32]		;; Real odd-cols row #4
	vmovapd	zmm2, ZMM_TMPS[7*32]		;; Real even-cols row #4 / .707
	zstore	[srcreg+3*d2], zmm0		;; Save final R7					; 57
	zfnmaddpd zmm0, zmm11, zmm2, zmm7	;; (ro4 - .707*re4) Real #10				; 57-61
	zfmaddpd zmm2, zmm11, zmm2, zmm7	;; (ro4 + .707*re4) Real #4				; 57-61

	vmovapd	zmm7, ZMM_TMPS[3*32]		;; Imag even-cols row #4 / .707
	zstore	[srcreg+3*d2+32], zmm8		;; Save final I7					; 57+1
	vmovapd	zmm8, ZMM_TMPS[10*32]		;; Imag odd-cols row #4
	zstore	[srcreg+5*d2], zmm5		;; Save final R11					; 58+1
	zfmsubpd zmm5, zmm11, zmm7, zmm8	;; (.707*i4e - i4o) Imag #10				; 58-62
	zfmaddpd zmm7, zmm11, zmm7, zmm8	;; (.707*i4e + i4o) Imag #4				; 58-62

	vmovapd	zmm8, ZMM_TMPS[8*32]		;; Real even-cols row #2
	zstore	[srcreg+5*d2+32], zmm10		;; Save final I11					; 58+2
	zfnmaddpd zmm10, zmm11, zmm8, zmm6	;; (ro2 - .707*re2) Real #12				; 59-63
	zfmaddpd zmm8, zmm11, zmm8, zmm6	;; (ro2 + .707*re2) Real #2				; 59-63

	vmovapd	zmm6, ZMM_TMPS[0*32]		;; Imag even-cols row #2 / .707
	zstore	[srcreg+d2], zmm1		;; Save final R3					; 59+2
	vmovapd	zmm1, ZMM_TMPS[4*32]		;; Imag odd-cols row #2
	zstore	[srcreg+d2+32], zmm9		;; Save final I3					; 59+3
	zfmsubpd zmm9, zmm11, zmm6, zmm1	;; (.707*ie2 - io2) Imag #12				; 60-64
	zfmaddpd zmm6, zmm11, zmm6, zmm1	;; (.707*ie2 + io2) Imag #2				; 60-64

	vmovapd	zmm1, ZMM_TMPS[9*32]		;; Real even-cols row #6 / .707
	zstore	[srcreg+4*d2], zmm14		;; Save final R9					; 60+3
	zfnmaddpd zmm14, zmm11, zmm1, zmm4	;; (ro6 - .707*re6) Real #8				; 61-65
	zfmaddpd zmm1, zmm11, zmm1, zmm4	;; (ro6 + .707*re6) Real #6				; 61-65

	vmovapd	zmm4, ZMM_TMPS[1*32]		;; Imag even-cols row #6 / .707
	zstore	[srcreg+4*d2+32], zmm3		;; Save final I9					; 60+4
	vmovapd	zmm3, ZMM_TMPS[5*32]		;; Imag odd-cols row #6
	zstore	[srcreg+2*d2], zmm12		;; Save final R5					; 61+4
	zfmsubpd zmm12, zmm11, zmm4, zmm3	;; (.707*ie6 - io6) Imag #8				; 62-66
	zfmaddpd zmm4, zmm11, zmm4, zmm3	;; (.707*ie6 + io6) Imag #6				; 62-66

	vmovapd	zmm3, [screg+8*64+32]		;; cosine/sine for R10/I10
	zfmsubpd zmm11, zmm0, zmm3, zmm5	;; A10 = R10 * cosine/sine - I10			; 63-67
	zfmaddpd zmm5, zmm5, zmm3, zmm0		;; B10 = I10 * cosine/sine + R10			; 63-67
	L1prefetchw srcreg+4*d2+L1pd, L1pt

	vmovapd	zmm3, [screg+2*64+32]		;; cosine/sine for R4/I4
	zfmsubpd zmm0, zmm2, zmm3, zmm7		;; A4 = R4 * cosine/sine - I4				; 64-68
	zfmaddpd zmm7, zmm7, zmm3, zmm2		;; B4 = I4 * cosine/sine + R4				; 64-68

	vmovapd	zmm3, [screg+10*64+32]		;; cosine/sine for R12/I12
	zfmsubpd zmm2, zmm10, zmm3, zmm9	;; A12 = R12 * cosine/sine - I12			; 65-69
	zfmaddpd zmm9, zmm9, zmm3, zmm10	;; B12 = I12 * cosine/sine + R12			; 65-69
	L1prefetchw srcreg+4*d2+d1+L1pd, L1pt

	vmovapd	zmm3, [screg+32]		;; cosine/sine for R2/I2
	zfmsubpd zmm10, zmm8, zmm3, zmm6	;; A2 = R2 * cosine/sine - I2				; 66-70
	zfmaddpd zmm6, zmm6, zmm3, zmm8		;; B2 = I2 * cosine/sine + R2				; 66-70
	zstore	[srcreg+2*d2+32], zmm13		;; Save final I5					; 61+5

	vmovapd	zmm3, [screg+6*64+32]		;; cosine/sine for R8/I8
	zfmsubpd zmm8, zmm14, zmm3, zmm12	;; A8 = R8 * cosine/sine - I8				; 67-71
	zfmaddpd zmm12, zmm12, zmm3, zmm14	;; B8 = I8 * cosine/sine + R8				; 67-71

	vmovapd	zmm3, [screg+4*64+32]		;; cosine/sine for R6/I6
	zfmsubpd zmm14, zmm1, zmm3, zmm4	;; A6 = R6 * cosine/sine - I6				; 68-72
	zfmaddpd zmm4, zmm4, zmm3, zmm1		;; B6 = I6 * cosine/sine + R6				; 68-72
	L1prefetchw srcreg+5*d2+L1pd, L1pt

	vmovapd zmm3, [screg+8*64]		;; sine for R10/I10
	vmulpd	zmm11, zmm11, zmm3		;; A10 = A10 * sine (final R10)				; 69-73
	vmulpd	zmm5, zmm5, zmm3		;; B10 = B10 * sine (final I10)				; 69-73

	vmovapd	zmm3, [screg+2*64]		;; sine for R4/I4
	vmulpd	zmm0, zmm0, zmm3		;; A4 = A4 * sine (final R4)				; 70-74
	vmulpd	zmm7, zmm7, zmm3		;; B4 = B4 * sine (final I4)				; 70-74
	L1prefetchw srcreg+5*d2+d1+L1pd, L1pt

	vmovapd	zmm3, [screg+10*64]		;; sine for R12/I12
	vmulpd	zmm2, zmm2, zmm3		;; A12 = A12 * sine (final R12)				; 71-75
	vmulpd	zmm9, zmm9, zmm3		;; B12 = B12 * sine (final I12)				; 71-75

	vmovapd	zmm3, [screg]			;; sine for R2/I2
	vmulpd	zmm10, zmm10, zmm3		;; A2 = A2 * sine (final R2)				; 72-76
	vmulpd	zmm6, zmm6, zmm3		;; B2 = B2 * sine (final I2)				; 72-76

	vmovapd	zmm3, [screg+6*64]		;; sine for R8/I8
	vmulpd	zmm8, zmm8, zmm3		;; A8 = A8 * sine (final R8)				; 73-77
	vmulpd	zmm12, zmm12, zmm3		;; B8 = B8 * sine (final I8)				; 73-77

	vmovapd	zmm3, [screg+4*64]		;; sine for R6/I6
	vmulpd	zmm14, zmm14, zmm3		;; A6 = A6 * sine (final R6)				; 74-78
	vmulpd	zmm4, zmm4, zmm3		;; B6 = B6 * sine (final I6)				; 74-78

	zstore	[srcreg+4*d2+d1], zmm11		;; Save final R10					; 74
	zstore	[srcreg+4*d2+d1+32], zmm5	;; Save final I10					; 74+1
	zstore	[srcreg+d2+d1], zmm0		;; Save final R4					; 75+1
	zstore	[srcreg+d2+d1+32], zmm7		;; Save final I4					; 75+2
	zstore	[srcreg+5*d2+d1], zmm2		;; Save final R12					; 76+2
	zstore	[srcreg+5*d2+d1+32], zmm9	;; Save final I12					; 76+3
	zstore	[srcreg+d1], zmm10		;; Save final R2					; 77+3
	zstore	[srcreg+d1+32], zmm6		;; Save final I2					; 77+4
	zstore	[srcreg+3*d2+d1], zmm8		;; Save final R8					; 78+4
	zstore	[srcreg+3*d2+d1+32], zmm12	;; Save final I8					; 78+5
	zstore	[srcreg+2*d2+d1], zmm14		;; Save final R6					; 79+5
	zstore	[srcreg+2*d2+d1+32], zmm4	;; Save final I6					; 79+6

	bump	srcreg, srcinc
	bump	screg, scinc
	ENDM

ENDIF

ENDIF

;;
;; ************************************* 24-reals-last-unfft variants ******************************************
;;

;; These macros produce 24 reals after doing 4.585 levels of the inverse FFT applying
;; the sin/cos multipliers beforehand.  The input is 2 real and 11 complex numbers.

;; To calculate a 24-reals inverse FFT, we calculate 24 real values from 24 complex inputs in a brute force way.
;; First we note that the 24 complex values are computed from the 11 complex and 2 real inputs using Hermetian szmmetry, thusly:
;; c1 = r1A + 0*i
;; c2 = r2 + i2*i
;; ...
;; c12 = r12 + i12*i
;; c13 = r1B + 0*i
;; c14 = r12 - i12*i
;; ...
;; c24 = r2 - i2*i 
;;
;; The brute force calculations are:
;;
;; c1 + c2 + ... + c24	*  w^-0000000000...
;; c1 + c2 + ... + c24	*  w^-0123456789A...
;; c1 + c2 + ... + c24	*  w^-02468ACE....
;;    ...
;; c1 + c2 + ... + c24	*  w^-...A987654321
;;
;; The sin/cos values (w = 24th root of unity) are:
;; w^-1 = .966 - .259i
;; w^-2 = .866 - .5i
;; w^-3 = .707 - .707i
;; w^-4 = .5 - .866i
;; w^-5 = .259 - .966i
;; w^-6 = 0 - 1i
;; w^-7 = -.259 - .966i
;; w^-8 = -.5 - .866i
;; w^-9 = -.707 - .707i
;; w^-10 = -.866 - .5i
;; w^-11 = -.966 - .259i
;; w^-12 = -1

;;
;; Applying the sin/cos values above, taking advantage of szmmetry, and ignoring a lot of multiplies by 2:
;; r1     +(r2+r12)     +(r3+r11)     +(r4+r10)     +(r5+r9)     +(r6+r8) + r7 + r13
;; r1 +.966(r2-r12) +.866(r3-r11) +.707(r4-r10) +.500(r5-r9) +.259(r6-r8)      - r13 +.259(i2+i12) +.500(i3+i11) +.707(i4+i10) +.866(i5+i9) +.966(i6+i8) + i7
;; r1 +.866(r2+r12) +.500(r3+r11)               -.500(r5+r9) -.866(r6+r8) - r7 + r13 +.500(i2-i12) +.866(i3-i11)     +(i4-i10) +.866(i5-i9) +.500(i6-i8)
;; r1 +.707(r2-r12)               -.707(r4-r10)     -(r5-r9) -.707(r6-r8)      - r13 +.707(i2+i12)     +(i3+i11) +.707(i4+i10)              -.707(i6+i8) - i7
;; r1 +.500(r2+r12) -.500(r3+r11)     -(r4+r10) -.500(r5+r9) +.500(r6+r8) + r7 + r13 +.866(i2-i12) +.866(i3-i11)               -.866(i5-i9) -.866(i6-i8)
;; r1 +.259(r2-r12) -.866(r3-r11) -.707(r4-r10) +.500(r5-r9) +.966(r6-r8)      - r13 +.966(i2+i12) +.500(i3+i11) -.707(i4+i10) -.866(i5+i9) +.259(i6+i8) + i7
;; r1                   -(r3+r11)                   +(r5+r9)              - r7 + r13     +(i2-i12)                   -(i4-i10)                  +(i6-i8)
;; r1 -.259(r2-r12) -.866(r3-r11) +.707(r4-r10) +.500(r5-r9) -.966(r6-r8)      - r13 +.966(i2+i12) -.500(i3+i11) -.707(i4+i10) +.866(i5+i9) +.259(i6+i8) - i7
;; r1 -.500(r2+r12) -.500(r3+r11)     +(r4+r10) -.500(r5+r9) -.500(r6+r8) + r7 + r13 +.866(i2-i12) -.866(i3-i11)               +.866(i5-i9) -.866(i6-i8)
;; r1 -.707(r2-r12)               +.707(r4-r10)     -(r5-r9) +.707(r6-r8)      - r13 +.707(i2+i12)     -(i3+i11) +.707(i4+i10)              -.707(i6+i8) + i7
;; r1 -.866(r2+r12) +.500(r3+r11)               -.500(r5+r9) +.866(r6+r8) - r7 + r13 +.500(i2-i12) -.866(i3-i11)     +(i4-i10) -.866(i5-i9) +.500(i6-i8)
;; r1 -.966(r2-r12) +.866(r3-r11) -.707(r4-r10) +.500(r5-r9) -.259(r6-r8)      - r13 +.259(i2+i12) -.500(i3+i11) +.707(i4+i10) -.866(i5+i9) +.966(i6+i8) - i7
;; r1     -(r2+r12)     +(r3+r11)     -(r4+r10)     +(r5+r9)     -(r6+r8) + r7 + r13
;; ... r14 thru r24 are the same as r12 through r2 but with the sign of the imaginary component changed.
;;
;; Also remember that due to the funny way we do things reals input r1A = r1+r13 and r1B = r1-13

;; Simplifying yields:
;;
;; r17oA = r1+r13     +(r5+r9)
;; r26oA = r1-r13 +.500(r5-r9)
;; r35oA = r1+r13 -.500(r5+r9)
;; r4o   = r1-r13     -(r5-r9)
;;
;; r17oB =     +(r3+r11) + r7
;; r26oB = +.866(r3-r11)
;; r35oB = +.500(r3+r11) - r7
;;
;; r1/7o = r17oA +/- r17oB
;; r2/6o = r26oA +/- r26oB
;; r3/5o = r35oA +/- r35oB
;;
;; r1e =     +(r2+r12)     +(r4+r10)     +(r6+r8)
;; r2e = +.966(r2-r12) +.707(r4-r10) +.259(r6-r8)
;; r3e =     +(r2+r12)                   -(r6+r8)
;; r4e =     +(r2-r12)     -(r4-r10)     -(r6-r8)
;; r5e = +.500(r2+r12)     -(r4+r10) +.500(r6+r8)
;; r6e = +.259(r2-r12) -.707(r4-r10) +.966(r6-r8)
;;
;; r1/13 = r1o +/- r1e
;; r2/12 = r2o +/- r2e
;; r3/11 = r3o +/- .866*r3e
;; r4/10 = r4o +/- .707*r4e
;; r5/9 = r5o +/- r5e
;; r6/8 = r6o +/- r6e
;;
;; i2e = +.259(i2+i12) +.707(i4+i10) +.966(i6+i8)
;; i3e = +.500(i2-i12)     +(i4-i10) +.500(i6-i8)
;; i4e =     +(i2+i12)     +(i4+i10)     -(i6+i8)
;; i5e =     +(i2-i12)                   -(i6-i8)
;; i6e = +.966(i2+i12) -.707(i4+i10) +.259(i6+i8)
;; i7  =     +(i2-i12)     -(i4-i10)     +(i6-i8)
;;
;; i2o = +.500(i3+i11) +.866(i5+i9) + i7
;; i3o =     +(i3-i11)     +(i5-i9)
;; i4o =     +(i3+i11)              - i7
;; i5o =     +(i3-i11)     -(i5-i9)
;; i6o = +.500(i3+i11) -.866(i5+i9) + i7
;;
;; i2/12 = i2e +- i2o
;; i3/11 = i3e +- .866*i3o
;; i4/10 = .707*i4e +- i4o
;; i5/9  = i5e +- i5o
;; i6/8  = i6e +- i6o
;;
;; r2/24 = r2 +- i2
;; r3/23 = r3 +- i3
;; r4/22 = r4 +- i4
;; r5/21 = r5 +- .866*i5
;; r6/20 = r6 +- i6
;; r7/19 = r7 +- i7
;; r8/18 = r8 +- i8
;; r9/17 = r9 +- .866*i9
;; r10/16 = r10 +- i10
;; r11/15 = r11 +- i11
;; r12/14 = r12 +- i12

yr6_12cl_24_reals_unfft_preload MACRO
	ENDM

yr6_12cl_24_reals_unfft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd

;; Apply the 11 twiddle factors.  Also do the first +/- on the reals so that
;; we have a perfect balance of adds and multiplies.

	vmovapd	zmm0, [screg+5*64+32]		;; cosine/sine for R7/I7
	vmovapd	zmm1, [srcreg+3*d2]		;; R7
	vmulpd	zmm2, zmm1, zmm0		;; A7 = R7 * cosine/sine
	vmovapd	zmm3, [srcreg+3*d2+32]		;; I7
	vaddpd	zmm2, zmm2, zmm3		;; A7 = A7 + I7
	vmulpd	zmm3, zmm3, zmm0		;; B7 = I7 * cosine/sine
	vsubpd	zmm3, zmm3, zmm1		;; B7 = B7 - R7
	vmovapd	zmm0, [screg+5*64]		;; sine for R7/I7
	vmulpd	zmm2, zmm2, zmm0		;; R7 = A7 * sine
	vmulpd	zmm3, zmm3, zmm0		;; I7 = B7 * sine
	zstore	ZMM_TMPS[0*32], zmm2		;; Save R7
	zstore	ZMM_TMPS[1*32], zmm3		;; Save I7

	vmovapd	zmm0, [screg+32]		;; cosine/sine for R2/I2
	vmovapd	zmm1, [srcreg+d1]		;; R2
	vmulpd	zmm2, zmm1, zmm0		;; A2 = R2 * cosine/sine
	vmovapd	zmm4, [screg+10*64+32]		;; cosine/sine for R12/I12
	vmovapd	zmm5, [srcreg+5*d2+d1]		;; R12
	vmulpd	zmm6, zmm5, zmm4		;; A12 = R12 * cosine/sine
	vmovapd	zmm3, [srcreg+d1+32]		;; I2
	vaddpd	zmm2, zmm2, zmm3		;; A2 = A2 + I2
	vmulpd	zmm3, zmm3, zmm0		;; B2 = I2 * cosine/sine
	vmovapd	zmm7, [srcreg+5*d2+d1+32]	;; I12
	vaddpd	zmm6, zmm6, zmm7		;; A12 = A12 + I12
	vmulpd	zmm7, zmm7, zmm4		;; B12 = I12 * cosine/sine
	vsubpd	zmm3, zmm3, zmm1		;; B2 = B2 - R2
	vmovapd	zmm0, [screg]			;; sine for R2/I2
	vmulpd	zmm2, zmm2, zmm0		;; R2 = A2 * sine
	vsubpd	zmm7, zmm7, zmm5		;; B12 = B12 - R12
	vmovapd	zmm4, [screg+10*64]		;; sine for R12/I12
	vmulpd	zmm6, zmm6, zmm4		;; R12 = A12 * sine
	vmulpd	zmm3, zmm3, zmm0		;; I2 = B2 * sine
	vmulpd	zmm7, zmm7, zmm4		;; I12 = B12 * sine
	vaddpd	zmm0, zmm2, zmm6		;; R2+R12
	vsubpd	zmm2, zmm2, zmm6		;; R2-R12
	L1prefetchw srcreg+L1pd, L1pt
	vaddpd	zmm1, zmm3, zmm7		;; I2+I12
	vsubpd	zmm3, zmm3, zmm7		;; I2-I12
	zstore	ZMM_TMPS[2*32], zmm0		;; Save R2+R12
	zstore	ZMM_TMPS[3*32], zmm2		;; Save R2-R12
	zstore	ZMM_TMPS[4*32], zmm1		;; Save I2+I12
	zstore	ZMM_TMPS[5*32], zmm3		;; Save I2-I12

	vmovapd	zmm0, [screg+64+32]		;; cosine/sine for R3/I3
	vmovapd	zmm1, [srcreg+d2]		;; R3
	vmulpd	zmm2, zmm1, zmm0		;; A3 = R3 * cosine/sine
	vmovapd	zmm4, [screg+9*64+32]		;; cosine/sine for R11/I11
	vmovapd	zmm5, [srcreg+5*d2]		;; R11
	vmulpd	zmm6, zmm5, zmm4		;; A11 = R11 * cosine/sine
	vmovapd	zmm3, [srcreg+d2+32]		;; I3
	vaddpd	zmm2, zmm2, zmm3		;; A3 = A3 + I3
	vmulpd	zmm3, zmm3, zmm0		;; B3 = I3 * cosine/sine
	vmovapd	zmm7, [srcreg+5*d2+32]		;; I11
	vaddpd	zmm6, zmm6, zmm7		;; A11 = A11 + I11
	vmulpd	zmm7, zmm7, zmm4		;; B11 = I11 * cosine/sine
	vsubpd	zmm3, zmm3, zmm1		;; B3 = B3 - R3
	vmovapd	zmm0, [screg+64]		;; sine for R3/I3
	vmulpd	zmm2, zmm2, zmm0		;; R3 = A3 * sine
	vsubpd	zmm7, zmm7, zmm5		;; B11 = B11 - R11
	vmovapd	zmm4, [screg+9*64]		;; sine for R11/I11
	vmulpd	zmm6, zmm6, zmm4		;; R11 = A11 * sine
	vmulpd	zmm3, zmm3, zmm0		;; I3 = B3 * sine
	vmulpd	zmm7, zmm7, zmm4		;; I11 = B11 * sine
	vaddpd	zmm0, zmm2, zmm6		;; R3+R11
	vsubpd	zmm2, zmm2, zmm6		;; R3-R11
	L1prefetchw srcreg+d1+L1pd, L1pt
	vaddpd	zmm1, zmm3, zmm7		;; I3+I11
	vsubpd	zmm3, zmm3, zmm7		;; I3-I11
	zstore	ZMM_TMPS[6*32], zmm0		;; Save R3+R11
	zstore	ZMM_TMPS[7*32], zmm2		;; Save R3-R11
	zstore	ZMM_TMPS[8*32], zmm1		;; Save I3+I11
	zstore	ZMM_TMPS[9*32], zmm3		;; Save I3-I11

	vmovapd	zmm0, [screg+2*64+32]		;; cosine/sine for R4/I4
	vmovapd	zmm1, [srcreg+d2+d1]		;; R4
	vmulpd	zmm2, zmm1, zmm0		;; A4 = R4 * cosine/sine
	vmovapd	zmm4, [screg+8*64+32]		;; cosine/sine for R10/I10
	vmovapd	zmm5, [srcreg+4*d2+d1]		;; R10
	vmulpd	zmm6, zmm5, zmm4		;; A10 = R10 * cosine/sine
	vmovapd	zmm3, [srcreg+d2+d1+32]		;; I4
	vaddpd	zmm2, zmm2, zmm3		;; A4 = A4 + I4
	vmulpd	zmm3, zmm3, zmm0		;; B4 = I4 * cosine/sine
	vmovapd	zmm7, [srcreg+4*d2+d1+32]	;; I10
	vaddpd	zmm6, zmm6, zmm7		;; A10 = A10 + I10
	vmulpd	zmm7, zmm7, zmm4		;; B10 = I10 * cosine/sine
	vsubpd	zmm3, zmm3, zmm1		;; B4 = B4 - R4
	vmovapd	zmm0, [screg+2*64]		;; sine for R4/I4
	vmulpd	zmm2, zmm2, zmm0		;; R4 = A4 * sine
	vsubpd	zmm7, zmm7, zmm5		;; B10 = B10 - R10
	vmovapd	zmm4, [screg+8*64]		;; sine for R10/I10
	vmulpd	zmm6, zmm6, zmm4		;; R10 = A10 * sine
	vmulpd	zmm3, zmm3, zmm0		;; I4 = B4 * sine
	vmulpd	zmm7, zmm7, zmm4		;; I10 = B10 * sine
	vaddpd	zmm0, zmm2, zmm6		;; R4+R10
	vsubpd	zmm2, zmm2, zmm6		;; R4-R10
	L1prefetchw srcreg+d2+L1pd, L1pt
	vaddpd	zmm1, zmm3, zmm7		;; I4+I10
	vsubpd	zmm3, zmm3, zmm7		;; I4-I10
	zstore	ZMM_TMPS[10*32], zmm0		;; Save R4+R10
	zstore	ZMM_TMPS[11*32], zmm2		;; Save R4-R10
	zstore	ZMM_TMPS[12*32], zmm1		;; Save I4+I10
	zstore	ZMM_TMPS[13*32], zmm3		;; Save I4-I10

	vmovapd	zmm0, [screg+3*64+32]		;; cosine/sine for R5/I5
	vmovapd	zmm1, [srcreg+2*d2]		;; R5
	vmulpd	zmm2, zmm1, zmm0		;; A5 = R5 * cosine/sine
	vmovapd	zmm4, [screg+7*64+32]		;; cosine/sine for R9/I9
	vmovapd	zmm5, [srcreg+4*d2]		;; R9
	vmulpd	zmm6, zmm5, zmm4		;; A9 = R9 * cosine/sine
	vmovapd	zmm3, [srcreg+2*d2+32]		;; I5
	vaddpd	zmm2, zmm2, zmm3		;; A5 = A5 + I5
	vmulpd	zmm3, zmm3, zmm0		;; B5 = I5 * cosine/sine
	vmovapd	zmm7, [srcreg+4*d2+32]		;; I9
	vaddpd	zmm6, zmm6, zmm7		;; A9 = A9 + I9
	vmulpd	zmm7, zmm7, zmm4		;; B9 = I9 * cosine/sine
	vsubpd	zmm3, zmm3, zmm1		;; B5 = B5 - R5
	vmovapd	zmm0, [screg+3*64]		;; sine for R5/I5
	vmulpd	zmm2, zmm2, zmm0		;; R5 = A5 * sine
	vsubpd	zmm7, zmm7, zmm5		;; B9 = B9 - R9
	vmovapd	zmm4, [screg+7*64]		;; sine for R9/I9
	vmulpd	zmm6, zmm6, zmm4		;; R9 = A9 * sine
	vmulpd	zmm3, zmm3, zmm0		;; I5 = B5 * sine
	vmulpd	zmm7, zmm7, zmm4		;; I9 = B9 * sine
	vaddpd	zmm0, zmm2, zmm6		;; R5+R9
	vsubpd	zmm2, zmm2, zmm6		;; R5-R9
	vaddpd	zmm1, zmm3, zmm7		;; I5+I9
	vsubpd	zmm3, zmm3, zmm7		;; I5-I9
	zstore	ZMM_TMPS[14*32], zmm0		;; Save R5+R9
	zstore	ZMM_TMPS[15*32], zmm2		;; Save R5-R9
	zstore	ZMM_TMPS[16*32], zmm1		;; Save I5+I9
	zstore	ZMM_TMPS[17*32], zmm3		;; Save I5-I9

	vmovapd	zmm0, [screg+4*64+32]		;; cosine/sine for R6/I6
	vmovapd	zmm1, [srcreg+2*d2+d1]		;; R6
	vmulpd	zmm2, zmm1, zmm0		;; A6 = R6 * cosine/sine
	vmovapd	zmm4, [screg+6*64+32]		;; cosine/sine for R8/I8
	vmovapd	zmm5, [srcreg+3*d2+d1]		;; R8
	vmulpd	zmm6, zmm5, zmm4		;; A8 = R8 * cosine/sine
	vmovapd	zmm3, [srcreg+2*d2+d1+32]	;; I6
	vaddpd	zmm2, zmm2, zmm3		;; A6 = A6 + I6
	vmulpd	zmm3, zmm3, zmm0		;; B6 = I6 * cosine/sine
	vmovapd	zmm7, [srcreg+3*d2+d1+32]	;; I8
	vaddpd	zmm6, zmm6, zmm7		;; A8 = A8 + I8
	vmulpd	zmm7, zmm7, zmm4		;; B8 = I8 * cosine/sine
	vsubpd	zmm3, zmm3, zmm1		;; B6 = B6 - R6
	vmovapd	zmm0, [screg+4*64]		;; sine for R6/I6
	vmulpd	zmm2, zmm2, zmm0		;; R6 = A6 * sine
	vsubpd	zmm7, zmm7, zmm5		;; B8 = B8 - R8
	vmovapd	zmm4, [screg+6*64]		;; sine for R8/I8
	vmulpd	zmm6, zmm6, zmm4		;; R8 = A8 * sine
	vmulpd	zmm3, zmm3, zmm0		;; I6 = B6 * sine
	vmulpd	zmm7, zmm7, zmm4		;; I8 = B8 * sine
	vaddpd	zmm0, zmm2, zmm6		;; R6+R8
	vsubpd	zmm2, zmm2, zmm6		;; R6-R8
	L1prefetchw srcreg+d2+d1+L1pd, L1pt
	vaddpd	zmm1, zmm3, zmm7		;; I6+I8
	vsubpd	zmm3, zmm3, zmm7		;; I6-I8
	zstore	ZMM_TMPS[18*32], zmm0		;; Save R6+R8
;;	zstore	ZMM_TMPS[?*32], zmm2		;; Save R6-R8
	zstore	ZMM_TMPS[19*32], zmm1		;; Save I6+I8
	zstore	ZMM_TMPS[20*32], zmm3		;; Save I6-I8

	;; Do the 24 reals inverse FFT

	;; Calculate even rows derived from real inputs

	vmovapd	zmm0, ZMM_TMPS[11*32]		;; r4-r10
	vmovapd	zmm1, ZMM_TMPS[3*32]		;; r2-r12
;;	vmovapd	zmm2, ZMM_TMPS[?*32]		;; r6-r8

	vmulpd	zmm3, zmm0, ZMM_SQRTHALF	;; .707(r4-r10)						; 1-5
	vsubpd	zmm4, zmm1, zmm0		;; (r2-r12) - (r4-r10)					; 1-3

	vmulpd	zmm0, zmm1, ZMM_P966		;; .966(r2-r12)						; 2-6

	vmulpd	zmm1, zmm1, ZMM_P259		;; .259(r2-r12)						; 3-7

	vmulpd	zmm5, zmm2, ZMM_P259		;; .259(r6-r8)						; 4-8
	vsubpd	zmm4, zmm4, zmm2		;; (r2-r12) - (r4-r10) - (r6-r8) (r4e)			; 4-6

	vmulpd	zmm2, zmm2, ZMM_P966		;; .966(r6-r8)						; 5-9

	vaddpd	zmm0, zmm0, zmm3		;; .966(r2-r12) + .707(r4-r10)				; 7-9

	vsubpd	zmm1, zmm1, zmm3		;; .259(r2-r12) - .707(r4-r10)				; 8-10

	vmovapd	zmm7, ZMM_TMPS[15*32]		;; r5-r9
	vmulpd	zmm3, zmm7, ZMM_HALF		;; .5(r5-r9)						; 6-10
	vmovapd	zmm6, [srcreg+32]		;; r1-r13
	vsubpd	zmm7, zmm6, zmm7		;; r1-r13 - (r5-r9)  (r4o)				; 6-8

	vaddpd	zmm0, zmm0, zmm5		;; .966(r2-r12) + .707(r4-r10) + .259(r6-r8) (r2e)	; 10-12

	vmovapd	zmm5, ZMM_TMPS[7*32]		;; r3-r11
	vmulpd	zmm5, zmm5, ZMM_P866		;; .866(r3-r11)  (r26oB)				; 7-11

	vmulpd	zmm4, zmm4, ZMM_SQRTHALF	;; .707*r4e						; 9-13

	vaddpd	zmm1, zmm1, zmm2		;; .259(r2-r12) - .707(r4-r10) + .966(r6-r8) (r6e)	; 11-13
	L1prefetchw srcreg+2*d2+L1pd, L1pt

	vaddpd	zmm6, zmm6, zmm3		;; r1-r13 + .5(r5-r9)  (r26oA)				; 12-14

	vaddpd	zmm2, zmm7, zmm4		;; real-cols row #4 (ro4 + re4)				; 14-16
	L1prefetchw srcreg+2*d2+d1+L1pd, L1pt

	vaddpd	zmm3, zmm6, zmm5		;; r26oA + r26oB (r2o)					; 15-17

	vsubpd	zmm6, zmm6, zmm5		;; r26oA - r26oB (r6o)					; 16-18
	vmovapd	zmm5, ZMM_TMPS[2*32]		;; r2+r12

	;; Combine even and odd columns (even rows)

	vsubpd	zmm7, zmm7, zmm4		;; real-cols row #10 (ro4 - re4)			; 17-19
	vmovapd	zmm4, ZMM_TMPS[18*32]		;; r6+r8
	zstore	ZMM_TMPS[2*32], zmm2		;; Save real-cols row #4				; 17

	vaddpd	zmm2, zmm3, zmm0		;; real-cols row #2 (ro2 + re2)				; 18-20
	L1prefetchw srcreg+3*d2+L1pd, L1pt

	vsubpd	zmm3, zmm3, zmm0		;; real-cols row #12 (ro2 - re2)			; 19-21

	vaddpd	zmm0, zmm6, zmm1		;; real-cols row #6 (ro6 + re6)				; 20-22
	zstore	ZMM_TMPS[3*32], zmm7		;; Save real-cols row #10				; 20
	vmovapd	zmm7, ZMM_TMPS[6*32]		;; r3+r11

	vsubpd	zmm6, zmm6, zmm1		;; real-cols row #8 (ro6 - re6)				; 21-23
	zstore	ZMM_TMPS[6*32], zmm2		;; Save real-cols row #2				; 21

	;; Calculate odd rows derived from real inputs

	vaddpd	zmm1, zmm5, zmm4		;; (r2+r12)+(r6+r8)					; 22-24
	vmulpd	zmm2, zmm7, ZMM_HALF		;; .5(r3+r11)						; 22-26
	zstore	ZMM_TMPS[7*32], zmm3		;; Save real-cols row #12				; 22

	vsubpd	zmm5, zmm5, zmm4		;; (r2+r12)-(r6+r8) (r3e)				; 23-25
	vmovapd	zmm4, ZMM_TMPS[14*32]		;; r5+r9
	vmulpd	zmm3, zmm4, ZMM_HALF		;; .5(r5+r9)						; 23-27
	zstore	ZMM_TMPS[11*32], zmm0		;; Save real-cols row #6				; 23

	vmovapd	zmm0, ZMM_TMPS[0*32]		;; r7
	vaddpd	zmm7, zmm7, zmm0		;; (r3+r11)+r7 (r17oB)					; 24-26
	zstore	ZMM_TMPS[0*32], zmm6		;; Save real-cols row #8				; 24

	vaddpd	zmm6, zmm1, ZMM_TMPS[10*32]	;; (r2+r12)+(r6+r8)+(r4+r10) (r1e)			; 25-27
	vmulpd	zmm1, zmm1, ZMM_HALF		;; .5((r2+r12)+(r6+r8))					; 25-29

	vaddpd	zmm4, zmm4, [srcreg]		;; (r1+r13)+(r5+r9) (r17oA)				; 26-28
	vmulpd	zmm5, zmm5, ZMM_P866		;; .866*r3e						; 26-30

	vsubpd	zmm2, zmm2, zmm0		;; .5(r3+r11)-r7 (r35oB)				; 27-29

	vmovapd	zmm0, [srcreg]			;; r1+r13
	vsubpd	zmm0, zmm0, zmm3		;; (r1+r13)-.5(r5+r9) (r35oA)				; 28-30

	vaddpd	zmm3, zmm4, zmm7		;; r17oA + r17oB (r1o)					; 29-31

	vsubpd	zmm1, zmm1, ZMM_TMPS[10*32]	;; .5((r2+r12)+(r6+r8))-(r4+r10) (r5e)			; 30-32

	vsubpd	zmm4, zmm4, zmm7		;; r17oA - r17oB (r7)					; 31-33
	L1prefetchw srcreg+3*d2+d1+L1pd, L1pt

	vaddpd	zmm7, zmm0, zmm2		;; r35oA + r35oB (r3o)					; 32-34

	vsubpd	zmm0, zmm0, zmm2		;; r35oA - r35oB (r5o)					; 33-35
	L1prefetchw srcreg+4*d2+L1pd, L1pt

	;; Combine even and odd columns (odd rows)

	vaddpd	zmm2, zmm3, zmm6		;; r1o + r1e (final R1)					; 34-36
	zstore	ZMM_TMPS[10*32], zmm4		;; Save real-cols row #7				; 34
	vmovapd	zmm4, ZMM_TMPS[4*32]		;; i2+i12

	vsubpd	zmm3, zmm3, zmm6		;; r1o - r1e (final R13)				; 35-37
	vmulpd	zmm6, zmm4, ZMM_P259		;; .259(i2+i12)						; 35-39

	zstore	[srcreg], zmm2			;; Save final R1					; 37
	vaddpd	zmm2, zmm7, zmm5		;; r3o + r3e (real-cols row #3)				; 36-38
	zstore	[srcreg+32], zmm3		;; Save final R13					; 38
	vmovapd	zmm3, ZMM_TMPS[12*32]		;; i4+i10
	zstore	ZMM_TMPS[4*32], zmm2		;; Save real-cols row #3				; 39
	vmulpd	zmm2, zmm3, ZMM_SQRTHALF	;; .707(i4+i10)						; 36-40

	vsubpd	zmm7, zmm7, zmm5		;; r3o - r3e (real-cols row #11)			; 37-39
	vmulpd	zmm5, zmm4, ZMM_P966		;; .966(i2+i12)						; 37-41

	zstore	ZMM_TMPS[12*32], zmm7		;; Save real-cols row #11				; 40
	vaddpd	zmm7, zmm0, zmm1		;; r5o + r5e (real-cols row #5)				; 38-40

	vsubpd	zmm0, zmm0, zmm1 		;; r5o - r5e (real-cols row #9)				; 39-41
	vmovapd	zmm1, ZMM_TMPS[19*32]		;; i6+i8
	zstore	ZMM_TMPS[14*32], zmm7		;; Save real-cols row #5				; 41
	vmulpd	zmm7, zmm1, ZMM_P966		;; .966(i6+i8)						; 39-43

	;; Calculate even rows derived from imaginary inputs

	vaddpd	zmm4, zmm4, zmm3		;; (i2+i12)+(i4+i10)					; 40-42
	vmulpd	zmm3, zmm1, ZMM_P259		;; .259(i6+i8)						; 40-44

	vaddpd	zmm6, zmm6, zmm2		;; .259(i2+i12)+.707(i4+i10)				; 41-43

	vsubpd	zmm5, zmm5, zmm2		;; .966(i2+i12)-.707(i4+i10)				; 42-44
	vmovapd	zmm2, ZMM_TMPS[8*32]		;; i3+i11
	zstore	ZMM_TMPS[8*32], zmm0		;; Save real-cols row #9				; 42
	vmulpd	zmm0, zmm2, ZMM_HALF		;; .500(i3+i11)						; 41-45

	vsubpd	zmm4, zmm4, zmm1		;; (i2+i12)+(i4+i10)-(i6+i8) (i4e)			; 43-45
	vmovapd	zmm1, ZMM_TMPS[16*32]		;; i5+i9

	vaddpd	zmm6, zmm6, zmm7		;; .259(i2+i12)+.707(i4+i10)+.966(i6+i8) (i2e)		; 44-46
	vmulpd	zmm1, zmm1, ZMM_P866		;; .866(i5+i9)						; 44-48

	vaddpd	zmm5, zmm5, zmm3		;; .966(i2+i12)-.707(i4+i10)+.259(i6+i8) (i6e)		; 45-47

	vmovapd	zmm3, ZMM_TMPS[1*32]		;; i7
	vaddpd	zmm0, zmm0, zmm3		;; .500(i3+i11)+i7					; 46-48
	vmulpd	zmm4, zmm4, ZMM_SQRTHALF	;; .707*i4e						; 46-50

	vsubpd	zmm2, zmm2, zmm3		;; (i3+i11)-i7 (i4o)					; 47-49
	L1prefetchw srcreg+4*d2+d1+L1pd, L1pt

	vaddpd	zmm3, zmm0, zmm1		;; .500(i3+i11)+i7+.866(i5+i9) (i2o)			; 49-51

	vsubpd	zmm0, zmm0, zmm1		;; .500(i3+i11)+i7-.866(i5+i9) (i6o)			; 50-52

	vsubpd	zmm1, zmm4, zmm2		;; imag-cols row #10 (i4e - i4o)			; 51-53
	L1prefetchw srcreg+5*d2+L1pd, L1pt

	vaddpd	zmm4, zmm4, zmm2		;; imag-cols row #4 (i4e + i4o)				; 52-54

	vsubpd	zmm2, zmm6, zmm3		;; imag-cols row #12 (i2e - i2o)			; 53-55

	vaddpd	zmm6, zmm6, zmm3		;; imag-cols row #2 (i2e + i2o)				; 54-56
	L1prefetchw srcreg+5*d2+d1+L1pd, L1pt

	vsubpd	zmm3, zmm5, zmm0		;; imag-cols row #8 (i6e - i6o)				; 55-57

	vaddpd	zmm5, zmm5, zmm0		;; imag-cols row #6 (i6e + i6o)				; 56-58

	vmovapd	zmm0, ZMM_TMPS[3*32]		;; Load real-cols row #10
	vsubpd	zmm7, zmm0, zmm1		;; (real#10 - imag#10) final R16
	vaddpd	zmm0, zmm0, zmm1		;; (real#10 + imag#10) final R10
	zstore	[srcreg+d2+d1+32], zmm7		;; Save R16
	zstore	[srcreg+4*d2+d1], zmm0		;; Save R10

	vmovapd	zmm1, ZMM_TMPS[2*32]		;; Load real-cols row #4
	vsubpd	zmm7, zmm1, zmm4		;; (real#4 - imag#4) final R22
	vaddpd	zmm1, zmm1, zmm4		;; (real#4 + imag#4) final R4
	zstore	[srcreg+4*d2+d1+32], zmm7	;; Save R22
	zstore	[srcreg+d2+d1], zmm1		;; Save R4

	vmovapd	zmm4, ZMM_TMPS[7*32]		;; Load real-cols row #12
	vsubpd	zmm7, zmm4, zmm2		;; (real#12 - imag#12) final R14
	vaddpd	zmm4, zmm4, zmm2		;; (real#12 + imag#12) final R12
	zstore	[srcreg+d1+32], zmm7		;; Save R14
	zstore	[srcreg+5*d2+d1], zmm4		;; Save R12

	vmovapd	zmm2, ZMM_TMPS[6*32]		;; Load real-cols row #2
	vsubpd	zmm7, zmm2, zmm6		;; (real#2 - imag#2) final R24
	vaddpd	zmm2, zmm2, zmm6		;; (real#2 + imag#2) final R2
	zstore	[srcreg+5*d2+d1+32], zmm7	;; Save R24
	zstore	[srcreg+d1], zmm2		;; Save R2

	vmovapd	zmm6, ZMM_TMPS[0*32]		;; Load real-cols row #8
	vsubpd	zmm7, zmm6, zmm3		;; (real#8 - imag#8) final R18
	vaddpd	zmm6, zmm6, zmm3		;; (real#8 + imag#8) final R8
	zstore	[srcreg+2*d2+d1+32], zmm7	;; Save R18
	zstore	[srcreg+3*d2+d1], zmm6		;; Save R8

	;; Calculate odd rows derived from imaginary inputs

	vmovapd	zmm0, ZMM_TMPS[5*32]		;; i2-i12
	vmovapd	zmm1, ZMM_TMPS[20*32]		;; i6-i8
	vaddpd	zmm4, zmm0, zmm1		;; (i2-i12)+(i6-i8)				; 67-69

	vsubpd	zmm0, zmm0, zmm1		;; (i2-i12)-(i6-i8) (i5e)			; 68-70

	vmovapd	zmm1, ZMM_TMPS[9*32]		;; i3-i11
	vmovapd	zmm2, ZMM_TMPS[17*32]		;; i5-i9
	vaddpd	zmm3, zmm1, zmm2		;; (i3-i11)+(i5-i9) (i3o)			; 69-71

	vsubpd	zmm1, zmm1, zmm2		;; (i3-i11)-(i5-i9) (i5o)			; 70-72
	vmulpd	zmm6, zmm4, ZMM_HALF		;; .5((i2-i12)+(i6-i8))				; 70-74

	vmovapd	zmm2, ZMM_TMPS[11*32]		;; Load real-cols row #6
	vsubpd	zmm7, zmm2, zmm5		;; (real#6 - imag#6) final R20			; 71-73
	vaddpd	zmm2, zmm2, zmm5		;; (real#6 + imag#6) final R6			; 72-74
	vmovapd zmm5, ZMM_P866
	vmulpd	zmm3, zmm5, zmm3		;; .866*i3o					; 72-76

	zstore	[srcreg+3*d2+d1+32], zmm7	;; Save R20					; 74
	vaddpd	zmm7, zmm0, zmm1		;; i5e + i5o (i5)				; 73-75

	vsubpd	zmm0, zmm0, zmm1		;; i5e - i5o (i9)				; 74-76

	vmovapd	zmm1, ZMM_TMPS[13*32]		;; i4-i10
	vaddpd	zmm6, zmm6, zmm1		;; .5((i2-i12)+(i6-i8))+(i4-i10) (i3e)		; 75-77
	zstore	[srcreg+2*d2+d1], zmm2		;; Save R6					; 75

	vsubpd	zmm4, zmm4, zmm1		;; (i2-i12)+(i6-i8)-(i4-i10) (i7)		; 76-78
	vmulpd	zmm7, zmm5, zmm7		;; .866*i5					; 76-80

	vmulpd	zmm0, zmm5, zmm0		;; .866*i9					; 77-81

	vaddpd	zmm1, zmm6, zmm3		;; i3e + i3o (i3)				; 78-80

	vsubpd	zmm6, zmm6, zmm3		;; i3e - i3o (i11)				; 79-81

	;; Combine even and odd columns, then real and imag data (odd rows)

	vmovapd	zmm3, ZMM_TMPS[10*32]		;; Load real-cols row #7
	vsubpd	zmm5, zmm3, zmm4		;; (r7 - i7) final R19
	vaddpd	zmm3, zmm3, zmm4		;; (r7 + i7) final R7
	zstore	[srcreg+3*d2+32], zmm5		;; Save R19
	zstore	[srcreg+3*d2], zmm3		;; Save R7

	vmovapd	zmm3, ZMM_TMPS[14*32]		;; Load real-cols row #5
	vsubpd	zmm5, zmm3, zmm7		;; (r5 - i5) final R21
	vaddpd	zmm3, zmm3, zmm7		;; (r5 + i5) final R5
	zstore	[srcreg+4*d2+32], zmm5		;; Save R21
	zstore	[srcreg+2*d2], zmm3		;; Save R5

	vmovapd	zmm3, ZMM_TMPS[4*32]		;; Load real-cols row #3
	vsubpd	zmm5, zmm3, zmm1		;; (r3 - i3) final R23
	vaddpd	zmm3, zmm3, zmm1		;; (r3 + i3) final R3
	zstore	[srcreg+5*d2+32], zmm5		;; Save R23
	zstore	[srcreg+d2], zmm3		;; Save R3

	vmovapd	zmm3, ZMM_TMPS[8*32]		;; Load real-cols row #9
	vsubpd	zmm5, zmm3, zmm0		;; (r9 - i9) final R17
	vaddpd	zmm3, zmm3, zmm0		;; (r9 + i9) final R9
	zstore	[srcreg+2*d2+32], zmm5		;; Save R17
	zstore	[srcreg+4*d2], zmm3		;; Save R9

	vmovapd	zmm3, ZMM_TMPS[12*32]		;; Load real-cols row #11
	vsubpd	zmm5, zmm3, zmm6		;; (r11 - i11) final R15
	vaddpd	zmm3, zmm3, zmm6		;; (r11 + i11) final R11
	zstore	[srcreg+d2+32], zmm5		;; Save R15
	zstore	[srcreg+5*d2], zmm3		;; Save R11

	bump	srcreg, srcinc
	bump	screg, scinc
	ENDM

;; 64-bit version

IFDEF X86_64

yr6_12cl_24_reals_unfft_preload MACRO
	ENDM

yr6_12cl_24_reals_unfft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd

;; Apply the 11 twiddle factors.  Also do the first +/- on the reals so that
;; we have a perfect balance of adds and multiplies.

	vmovapd	zmm0, [screg+32]		;; cosine/sine for R2/I2
	vmovapd	zmm1, [srcreg+d1]		;; R2
	vmulpd	zmm2, zmm1, zmm0		;; A2 = R2 * cosine/sine				;	1-5

	vmovapd	zmm3, [srcreg+d1+32]		;; I2
	vmulpd	zmm0, zmm3, zmm0		;; B2 = I2 * cosine/sine				;	2-6

	vmovapd	zmm4, [screg+10*64+32]		;; cosine/sine for R12/I12
	vmovapd	zmm5, [srcreg+5*d2+d1]		;; R12
	vmulpd	zmm6, zmm5, zmm4		;; A12 = R12 * cosine/sine				;	3-7

	vmovapd	zmm7, [srcreg+5*d2+d1+32]	;; I12
	vmulpd	zmm4, zmm7, zmm4		;; B12 = I12 * cosine/sine				;	4-8

	vmovapd	zmm8, [screg+4*64+32]		;; cosine/sine for R6/I6
	vmovapd	zmm9, [srcreg+2*d2+d1]		;; R6
	vmulpd	zmm10, zmm9, zmm8		;; A6 = R6 * cosine/sine				;	5-9

	vmovapd	zmm11, [srcreg+2*d2+d1+32]	;; I6
	vaddpd	zmm2, zmm2, zmm3		;; A2 = A2 + I2						; 6-8
	vmulpd	zmm8, zmm11, zmm8		;; B6 = I6 * cosine/sine				;	6-10

	vmovapd	zmm12, [screg+6*64+32]		;; cosine/sine for R8/I8
	vmovapd	zmm13, [srcreg+3*d2+d1]		;; R8
	vsubpd	zmm0, zmm0, zmm1		;; B2 = B2 - R2						; 7-9
	vmulpd	zmm14, zmm13, zmm12		;; A8 = R8 * cosine/sine				;	7-11

	vaddpd	zmm6, zmm6, zmm7		;; A12 = A12 + I12					; 8-10
	vmovapd	zmm15, [srcreg+3*d2+d1+32]	;; I8
	vmulpd	zmm12, zmm15, zmm12		;; B8 = I8 * cosine/sine				;	8-12

	vmovapd	zmm3, [screg]			;; sine for R2/I2
	vsubpd	zmm4, zmm4, zmm5		;; B12 = B12 - R12					; 9-11
	vmulpd	zmm2, zmm2, zmm3		;; R2 = A2 * sine					;	9-13

	vaddpd	zmm10, zmm10, zmm11		;; A6 = A6 + I6						; 10-12
	vmulpd	zmm0, zmm0, zmm3		;; I2 = B2 * sine					;	10-14

	vmovapd	zmm1, [screg+10*64]		;; sine for R12/I12
	vsubpd	zmm8, zmm8, zmm9		;; B6 = B6 - R6						; 11-13
	vmulpd	zmm6, zmm6, zmm1		;; R12 = A12 * sine					;	11-15

	vaddpd	zmm14, zmm14, zmm15		;; A8 = A8 + I8						; 12-14
	vmulpd	zmm4, zmm4, zmm1		;; I12 = B12 * sine					;	12-16

	vmovapd	zmm7, [screg+4*64]		;; sine for R6/I6
	vsubpd	zmm12, zmm12, zmm13		;; B8 = B8 - R8						; 13-15
	vmulpd	zmm10, zmm10, zmm7		;; R6 = A6 * sine					;	13-17

	vmovapd	zmm5, [screg+6*64]		;; sine for R8/I8
	vmulpd	zmm8, zmm8, zmm7		;; I6 = B6 * sine					;	14-18

	vmovapd	zmm11, [screg+64+32]		;; cosine/sine for R3/I3
	vmulpd	zmm14, zmm14, zmm5		;; R8 = A8 * sine					;	15-19

	vmovapd	zmm3, [srcreg+d2]		;; R3
	vaddpd	zmm7, zmm2, zmm6		;; R2+R12						; 16-18
	vmulpd	zmm12, zmm12, zmm5		;; I8 = B8 * sine					;	16-20

	vmovapd	zmm9, [srcreg+d2+32]		;; I3
	vsubpd	zmm2, zmm2, zmm6		;; R2-R12						; 17-19
	vmulpd	zmm6, zmm3, zmm11		;; A3 = R3 * cosine/sine				;	17-21

	vmovapd	zmm15, [screg+9*64+32]		;; cosine/sine for R11/I11
	vaddpd	zmm5, zmm0, zmm4		;; I2+I12						; 18-20
	vmulpd	zmm11, zmm9, zmm11		;; B3 = I3 * cosine/sine				;	18-22

	vmovapd	zmm1, [srcreg+5*d2]		;; R11
	vsubpd	zmm0, zmm0, zmm4		;; I2-I12						; 19-21
	vmulpd	zmm4, zmm1, zmm15		;; A11 = R11 * cosine/sine				;	19-23
	zstore	ZMM_TMPS[0*32], zmm7		;; Save R2+R12						; 19

	vmovapd	zmm13, [srcreg+5*d2+32]		;; I11
	vaddpd	zmm7, zmm10, zmm14		;; R6+R8						; 20-22
	vmulpd	zmm15, zmm13, zmm15		;; B11 = I11 * cosine/sine				;	20-24
	zstore	ZMM_TMPS[1*32], zmm2		;; Save R2-R12						; 20

	vmovapd	zmm2, [screg+3*64+32]		;; cosine/sine for R5/I5
	vsubpd	zmm10, zmm10, zmm14		;; R6-R8						; 21-23
	vmovapd	zmm14, [srcreg+2*d2+32]		;; I5
	zstore	ZMM_TMPS[2*32], zmm5		;; Save I2+I12						; 21
	vmulpd	zmm5, zmm14, zmm2		;; B5 = I5 * cosine/sine				;	21-25

	vaddpd	zmm6, zmm6, zmm9		;; A3 = A3 + I3						; 22-24
	vmovapd	zmm9, [srcreg+2*d2]		;; R5
	vmulpd	zmm2, zmm9, zmm2		;; A5 = R5 * cosine/sine				;	22-26
	zstore	ZMM_TMPS[3*32], zmm0		;; Save I2-I12						; 22

	vmovapd	zmm0, [screg+7*64+32]		;; cosine/sine for R9/I9
	vsubpd	zmm11, zmm11, zmm3		;; B3 = B3 - R3						; 23-25
	vmovapd	zmm3, [srcreg+4*d2+32]		;; I9
	zstore	ZMM_TMPS[4*32], zmm7		;; Save R6+R8						; 23
	vmulpd	zmm7, zmm3, zmm0		;; B9 = I9 * cosine/sine				;	23-27

	vaddpd	zmm4, zmm4, zmm13		;; A11 = A11 + I11					; 24-26
	vmovapd	zmm13, [srcreg+4*d2]		;; R9
	vmulpd	zmm0, zmm13, zmm0		;; A9 = R9 * cosine/sine				;	24-28
	zstore	ZMM_TMPS[5*32], zmm10		;; Save R6-R8						; 24

	vmovapd	zmm10, [screg+64]		;; sine for R3/I3
	vsubpd	zmm15, zmm15, zmm1		;; B11 = B11 - R11					; 25-27
	vmulpd	zmm6, zmm6, zmm10		;; R3 = A3 * sine					;	25-29

	vmovapd	zmm1, [screg+9*64]		;; sine for R11/I11
	vsubpd	zmm5, zmm5, zmm9		;; B5 = B5 - R5						; 26-28
	vmulpd	zmm11, zmm11, zmm10		;; I3 = B3 * sine					;	26-30

	vmovapd	zmm9, [screg+3*64]		;; sine for R5/I5
	vaddpd	zmm2, zmm2, zmm14		;; A5 = A5 + I5						; 27-29
	vmulpd	zmm4, zmm4, zmm1		;; R11 = A11 * sine					;	27-31

	vmovapd	zmm10, [screg+7*64]		;; sine for R9/I9
	vsubpd	zmm7, zmm7, zmm13		;; B9 = B9 - R9						; 28-30
	vmulpd	zmm15, zmm15, zmm1		;; I11 = B11 * sine					;	28-32

	vmovapd	zmm14, [screg+2*64+32]		;; cosine/sine for R4/I4
	vaddpd	zmm0, zmm0, zmm3		;; A9 = A9 + I9						; 29-31
	vmulpd	zmm5, zmm5, zmm9		;; I5 = B5 * sine					;	29-33

	vmovapd	zmm13, [srcreg+d2+d1+32]	;; I4
	vaddpd	zmm1, zmm8, zmm12		;; I6+I8						; 30-32
	vmulpd	zmm2, zmm2, zmm9		;; R5 = A5 * sine					;	30-34

	vmovapd	zmm3, [srcreg+d2+d1]		;; R4
	vsubpd	zmm8, zmm8, zmm12		;; I6-I8						; 31-33
	vmulpd	zmm7, zmm7, zmm10		;; I9 = B9 * sine					;	31-35

	vmovapd	zmm9, [screg+8*64+32]		;; cosine/sine for R10/I10
	vaddpd	zmm12, zmm6, zmm4		;; R3+R11						; 32-34
	vmulpd	zmm0, zmm0, zmm10		;; R9 = A9 * sine					;	32-36

	vmovapd	zmm10, [srcreg+4*d2+d1+32]	;; I10
	vsubpd	zmm6, zmm6, zmm4		;; R3-R11						; 33-35
	vmulpd	zmm4, zmm13, zmm14		;; B4 = I4 * cosine/sine				;	33-37
	zstore	ZMM_TMPS[6*32], zmm1		;; Save I6+I8						; 33

	vaddpd	zmm1, zmm11, zmm15		;; I3+I11						; 34-36
	vmulpd	zmm14, zmm3, zmm14		;; A4 = R4 * cosine/sine				;	34-38
	L1prefetchw srcreg+L1pd, L1pt

	vsubpd	zmm11, zmm11, zmm15		;; I3-I11						; 35-37
	vmulpd	zmm15, zmm10, zmm9		;; B10 = I10 * cosine/sine				;	35-39
	zstore	ZMM_TMPS[7*32], zmm12		;; Save R3+R11						; 35

	vsubpd	zmm12, zmm5, zmm7		;; I5-I9						; 36-38
	zstore	ZMM_TMPS[8*32], zmm6		;; Save R3-R11						; 36
	vmovapd	zmm6, [srcreg+4*d2+d1]		;; R10
	vmulpd	zmm9, zmm6, zmm9		;; A10 = R10 * cosine/sine				;	36-40

	vaddpd	zmm5, zmm5, zmm7		;; I5+I9						; 37-39
	vmovapd	zmm7, [screg+5*64+32]		;; cosine/sine for R7/I7
	zstore	ZMM_TMPS[9*32], zmm1		;; Save I3+I11						; 37
	vmovapd	zmm1, [srcreg+3*d2+32]		;; I7

	vsubpd	zmm4, zmm4, zmm3		;; B4 = B4 - R4						; 38-40
	vmulpd	zmm3, zmm1, zmm7		;; B7 = I7 * cosine/sine				;	37-41

	vaddpd	zmm14, zmm14, zmm13		;; A4 = A4 + I4						; 39-41
	vmovapd	zmm13, [srcreg+3*d2]		;; R7
	vmulpd	zmm7, zmm13, zmm7		;; A7 = R7 * cosine/sine				;	38-42

	vsubpd	zmm15, zmm15, zmm6		;; B10 = B10 - R10					; 40-42
	zstore	ZMM_TMPS[10*32], zmm5		;; Save I5+I9						; 40

	vmovapd	zmm6, [screg+2*64]		;; sine for R4/I4
	vaddpd	zmm9, zmm9, zmm10		;; A10 = A10 + I10					; 41-43
	vmulpd	zmm4, zmm4, zmm6		;; I4 = B4 * sine					;	41-45

	vmovapd	zmm10, [screg+8*64]		;; sine for R10/I10
	vsubpd	zmm3, zmm3, zmm13		;; B7 = B7 - R7						; 42-44
	vmulpd	zmm14, zmm14, zmm6		;; R4 = A4 * sine					;	42-46

	vmovapd	zmm13, [screg+5*64]		;; sine for R7/I7
	vaddpd	zmm7, zmm7, zmm1		;; A7 = A7 + I7						; 43-45
	vmulpd	zmm15, zmm15, zmm10		;; I10 = B10 * sine					;	43-47

	vmovapd	zmm5, ZMM_TMPS[3*32]		;; i2-i12
	vaddpd	zmm1, zmm2, zmm0		;; R5+R9						; 44-46
	vmulpd	zmm9, zmm9, zmm10		;; R10 = A10 * sine					;	44-48

	vmovapd	zmm6, ZMM_HALF
	vsubpd	zmm2, zmm2, zmm0		;; R5-R9						; 45-47
	vmulpd	zmm3, zmm3, zmm13		;; I7 = B7 * sine					;	45-49

	;; Do the 24 reals inverse FFT

	;; Calculate odd rows

	vmovapd zmm10, ZMM_P866
	vaddpd	zmm0, zmm5, zmm8		;; (i2-i12)+(i6-i8)					; 46-48
	vmulpd	zmm7, zmm7, zmm13		;; R7 = A7 * sine					;	46-50

	vmovapd	zmm13, ZMM_TMPS[7*32]		;; r3+r11
	vsubpd	zmm5, zmm5, zmm8		;; (i2-i12)-(i6-i8) (i5e)				; 47-49

	vaddpd	zmm8, zmm11, zmm12		;; (i3-i11)+(i5-i9) (i3o)				; 48-50
	zstore	ZMM_TMPS[3*32], zmm2		;; Save R5-R9						; 48

	vsubpd	zmm11, zmm11, zmm12		;; (i3-i11)-(i5-i9) (i5o)				; 49-51
	vmulpd	zmm12, zmm6, zmm0		;; .5((i2-i12)+(i6-i8))					;	49-53
	L1prefetchw srcreg+d1+L1pd, L1pt

	vsubpd	zmm2, zmm4, zmm15		;; I4-I10						; 50-52
	vmulpd	zmm5, zmm10, zmm5		;; .866*i5e						;	50-54
	zstore	ZMM_TMPS[7*32], zmm3		;; Save I7						; 50
	vmovapd	zmm3, ZMM_TMPS[0*32]		;; r2+r12

	vaddpd	zmm4, zmm4, zmm15		;; I4+I10						; 51-53
	vmulpd	zmm8, zmm10, zmm8		;; .866*i3o						;	51-55
	L1prefetchw srcreg+d2+L1pd, L1pt

	vaddpd	zmm15, zmm14, zmm9		;; R4+R10						; 52-54
	vmulpd	zmm11, zmm10, zmm11		;; .866*i5o						;	52-56

	vsubpd	zmm14, zmm14, zmm9		;; R4-R10						; 53-55
	vmulpd	zmm9, zmm6, zmm13		;; .5(r3+r11)						;	53-57

	zstore	ZMM_TMPS[0*32], zmm4		;; Save I4+I10						; 54
	vmovapd	zmm4, ZMM_TMPS[4*32]		;; r6+r8
	zstore	ZMM_TMPS[4*32], zmm14		;; Save R4-R10						; 56
	vaddpd	zmm14, zmm3, zmm4		;; (r2+r12)+(r6+r8)					; 54-56

	vsubpd	zmm3, zmm3, zmm4		;; (r2+r12)-(r6+r8) (r3e)				; 55-57
	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	vaddpd	zmm12, zmm12, zmm2		;; .5((i2-i12)+(i6-i8))+(i4-i10) (i3e)			; 56-58

	vsubpd	zmm0, zmm0, zmm2		;; (i2-i12)+(i6-i8)-(i4-i10) (i7)			; 57-59
	vmulpd	zmm2, zmm6, zmm14		;; .5((r2+r12)+(r6+r8))					;	57-61
	L1prefetchw srcreg+2*d2+L1pd, L1pt

	vaddpd	zmm13, zmm13, zmm7		;; (r3+r11)+r7 (r17oB)					; 58-60
	vmulpd	zmm3, zmm10, zmm3		;; .866*r3e						;	58-62

	vsubpd	zmm9, zmm9, zmm7		;; .5(r3+r11)-r7 (r35oB)				; 59-61
	L1prefetchw srcreg+2*d2+d1+L1pd, L1pt

	vaddpd	zmm7, zmm5, zmm11		;; i5e + i5o (i5)					; 60-62
	vmulpd	zmm4, zmm6, zmm1		;; .5(r5+r9)						;	60-64

	vsubpd	zmm5, zmm5, zmm11		;; i5e - i5o (i9)					; 61-63

	vmovapd	zmm11, [srcreg]			;; r1+r13
	vaddpd	zmm14, zmm14, zmm15		;; (r2+r12)+(r6+r8)+(r4+r10) (r1e)			; 62-64

	vsubpd	zmm2, zmm2, zmm15		;; .5((r2+r12)+(r6+r8))-(r4+r10) (r5e)			; 63-65
	L1prefetchw srcreg+3*d2+L1pd, L1pt

	vaddpd	zmm1, zmm11, zmm1		;; (r1+r13)+(r5+r9) (r17oA)				; 64-66

	vsubpd	zmm11, zmm11, zmm4		;; (r1+r13)-.5(r5+r9) (r35oA)				; 65-67
	L1prefetchw srcreg+3*d2+d1+L1pd, L1pt

	vaddpd	zmm4, zmm12, zmm8		;; i3e + i3o (i3)					; 66-68
	vsubpd	zmm12, zmm12, zmm8		;; i3e - i3o (i11)					; 67-69

	vaddpd	zmm8, zmm1, zmm13		;; r17oA + r17oB (r1o)					; 68-70
	vsubpd	zmm1, zmm1, zmm13		;; r17oA - r17oB (r7)					; 69-71

	vaddpd	zmm13, zmm11, zmm9		;; r35oA + r35oB (r3o)					; 70-72
	vsubpd	zmm11, zmm11, zmm9		;; r35oA - r35oB (r5o)					; 71-73

	vaddpd	zmm9, zmm8, zmm14		;; r1o + r1e (final R1)					; 72-74
	L1prefetchw srcreg+4*d2+L1pd, L1pt

	vsubpd	zmm8, zmm8, zmm14		;; r1o - r1e (final R13)				; 73-75

	vaddpd	zmm14, zmm13, zmm3		;; r3o + r3e (real-cols row #3)				; 74-76
	L1prefetchw srcreg+4*d2+d1+L1pd, L1pt

	vsubpd	zmm13, zmm13, zmm3		;; r3o - r3e (real-cols row #11)			; 75-77
	zstore	[srcreg], zmm9			;; Save final R1					; 75

	vaddpd	zmm3, zmm11, zmm2		;; r5o + r5e (real-cols row #5)				; 76-78
	vmovapd	zmm15, [srcreg+32]		;; r1-r13
	zstore	[srcreg+32], zmm8		;; Save final R13					; 76

	vsubpd	zmm11, zmm11, zmm2 		;; r5o - r5e (real-cols row #9)				; 77-79
	vmovapd	zmm9, ZMM_TMPS[4*32]		;; r4-r10

	vsubpd	zmm2, zmm1, zmm0		;; (r7 - i7) final R19					; 78-80
	vmovapd	zmm8, ZMM_SQRTHALF

	vaddpd	zmm1, zmm1, zmm0		;; (r7 + i7) final R7					; 79-81

	vsubpd	zmm0, zmm14, zmm4		;; (r3 - i3) final R23					; 80-82

	vaddpd	zmm14, zmm14, zmm4		;; (r3 + i3) final R3					; 81-83
	zstore	[srcreg+3*d2+32], zmm2		;; Save R19						; 81
	vmovapd	zmm2, ZMM_TMPS[1*32]		;; r2-r12

	vsubpd	zmm4, zmm13, zmm12		;; (r11 - i11) final R15				; 82-84
	zstore	[srcreg+3*d2], zmm1		;; Save R7						; 82
	vmovapd	zmm1, ZMM_P966

	vaddpd	zmm13, zmm13, zmm12		;; (r11 + i11) final R11				; 83-85
	zstore	[srcreg+5*d2+32], zmm0		;; Save R23						; 83

	vsubpd	zmm12, zmm3, zmm7		;; (r5 - i5) final R21					; 84-86
	vmulpd	zmm0, zmm8, zmm9		;; .707(r4-r10)						;	84-88
	zstore	[srcreg+d2], zmm14		;; Save R3						; 84
	vmovapd	zmm14, ZMM_P259

	vaddpd	zmm3, zmm3, zmm7		;; (r5 + i5) final R5					; 85-87
	vmulpd	zmm7, zmm1, zmm2		;; .966(r2-r12)						;	85-89
	zstore	[srcreg+d2+32], zmm4		;; Save R15						; 85
	vmovapd	zmm4, ZMM_TMPS[3*32]		;; r5-r9

	;; Calculate even rows

	vsubpd	zmm9, zmm2, zmm9		;; (r2-r12) - (r4-r10)					; 86-88
	vmulpd	zmm2, zmm14, zmm2		;; .259(r2-r12)						;	86-90
	zstore	[srcreg+5*d2], zmm13		;; Save R11						; 86

	vsubpd	zmm13, zmm11, zmm5		;; (r9 - i9) final R17					; 87-89
	zstore	[srcreg+4*d2+32], zmm12		;; Save R21						; 87

	vaddpd	zmm11, zmm11, zmm5		;; (r9 + i9) final R9					; 88-90
	vmulpd	zmm12, zmm6, zmm4		;; .5(r5-r9)						;	88-92
	vmovapd	zmm5, ZMM_TMPS[5*32]		;; r6-r8
	zstore	[srcreg+2*d2], zmm3		;; Save R5						; 88

	vsubpd	zmm9, zmm9, zmm5		;; (r2-r12) - (r4-r10) - (r6-r8) (r4e)			; 89-91
	vmulpd	zmm3, zmm14, zmm5		;; .259(r6-r8)						;	89-93

	vaddpd	zmm7, zmm7, zmm0		;; .966(r2-r12) + .707(r4-r10)				; 90-92
	vmulpd	zmm5, zmm1, zmm5		;; .966(r6-r8)						;	90-94
	zstore	[srcreg+2*d2+32], zmm13		;; Save R17						; 90
	vmovapd	zmm13, ZMM_TMPS[8*32]		;; r3-r11

	vsubpd	zmm2, zmm2, zmm0		;; .259(r2-r12) - .707(r4-r10)				; 91-93
	vmulpd	zmm13, zmm10, zmm13		;; .866(r3-r11)  (r26oB)				;	91-95
	zstore	[srcreg+4*d2], zmm11		;; Save R9						; 91

	vsubpd	zmm4, zmm15, zmm4		;; r1-r13 - (r5-r9)  (r4o)				; 92-94
	vmulpd	zmm9, zmm8, zmm9		;; .707*r4e						;	92-96
	vmovapd	zmm0, ZMM_TMPS[0*32]		;; i4+i10

	vaddpd	zmm15, zmm15, zmm12		;; r1-r13 + .5(r5-r9)  (r26oA)				; 93-95
	vmovapd	zmm11, ZMM_TMPS[2*32]		;; i2+i12

	vaddpd	zmm7, zmm7, zmm3		;; .966(r2-r12) + .707(r4-r10) + .259(r6-r8) (r2e)	; 94-96
	L1prefetchw srcreg+5*d2+L1pd, L1pt

	vaddpd	zmm2, zmm2, zmm5		;; .259(r2-r12) - .707(r4-r10) + .966(r6-r8) (r6e)	; 95-97

	vaddpd	zmm5, zmm15, zmm13		;; r26oA + r26oB (r2o)					; 96-98

	vsubpd	zmm15, zmm15, zmm13		;; r26oA - r26oB (r6o)					; 97-99
	vmulpd	zmm13, zmm8, zmm0		;; .707(i4+i10)						;	97-101

	vaddpd	zmm3, zmm4, zmm9		;; real-cols row #4 (ro4 + re4)				; 98-100
	vmulpd	zmm12, zmm14, zmm11		;; .259(i2+i12)						;	98-102

	vsubpd	zmm4, zmm4, zmm9		;; real-cols row #10 (ro4 - re4)			; 99-101
	vmulpd	zmm9, zmm1, zmm11		;; .966(i2+i12)						;	99-103

	vaddpd	zmm11, zmm11, zmm0		;; (i2+i12)+(i4+i10)					; 100-102
	L1prefetchw srcreg+5*d2+d1+L1pd, L1pt

	vaddpd	zmm0, zmm5, zmm7		;; real-cols row #2 (ro2 + re2)				; 101-103

	vsubpd	zmm5, zmm5, zmm7		;; real-cols row #12 (ro2 - re2)			; 102-104

	vmovapd	zmm7, ZMM_TMPS[9*32]		;; i3+i11
	vaddpd	zmm12, zmm12, zmm13		;; .259(i2+i12)+.707(i4+i10)				; 103-105
	vmulpd	zmm6, zmm6, zmm7		;; .500(i3+i11)						;	103-107

	vsubpd	zmm9, zmm9, zmm13		;; .966(i2+i12)-.707(i4+i10)				; 104-106
	vmovapd	zmm13, ZMM_TMPS[6*32]		;; i6+i8
	vmulpd	zmm1, zmm1, zmm13		;; .966(i6+i8)						;	104-108

	vsubpd	zmm11, zmm11, zmm13		;; (i2+i12)+(i4+i10)-(i6+i8) (i4e)			; 105-107
	vmulpd	zmm13, zmm14, zmm13		;; .259(i6+i8)						;	105-109

	vaddpd	zmm14, zmm15, zmm2		;; real-cols row #6 (ro6 + re6)				; 106-108

	vsubpd	zmm15, zmm15, zmm2		;; real-cols row #8 (ro6 - re6)				; 107-109
	vmovapd	zmm2, ZMM_TMPS[10*32]		;; i5+i9
	vmulpd	zmm2, zmm10, zmm2		;; .866(i5+i9)						;	107-111

	vmovapd	zmm10, ZMM_TMPS[7*32]		;; i7
	vaddpd	zmm6, zmm6, zmm10		;; .500(i3+i11)+i7					; 108-110

	vsubpd	zmm7, zmm7, zmm10		;; (i3+i11)-i7 (i4o)					; 109-111
	vmulpd	zmm11, zmm8, zmm11		;; .707*i4e						;	109-113

	vaddpd	zmm12, zmm12, zmm1		;; .259(i2+i12)+.707(i4+i10)+.966(i6+i8) (i2e)		; 110-112

	vaddpd	zmm9, zmm9, zmm13		;; .966(i2+i12)-.707(i4+i10)+.259(i6+i8) (i6e)		; 111-113

	vaddpd	zmm13, zmm6, zmm2		;; .500(i3+i11)+i7+.866(i5+i9) (i2o)			; 112-114

	vsubpd	zmm6, zmm6, zmm2		;; .500(i3+i11)+i7-.866(i5+i9) (i6o)			; 113-115

	vsubpd	zmm2, zmm11, zmm7		;; imag-cols row #10 (i4e - i4o)			; 114-116

	vaddpd	zmm11, zmm11, zmm7		;; imag-cols row #4 (i4e + i4o)				; 115-117

	vsubpd	zmm7, zmm12, zmm13		;; imag-cols row #12 (i2e - i2o)			; 116-118

	vaddpd	zmm12, zmm12, zmm13		;; imag-cols row #2 (i2e + i2o)				; 117-119

	vsubpd	zmm13, zmm9, zmm6		;; imag-cols row #8 (i6e - i6o)				; 118-120

	vaddpd	zmm9, zmm9, zmm6		;; imag-cols row #6 (i6e + i6o)				; 119-121

	vsubpd	zmm6, zmm4, zmm2		;; (real#10 - imag#10) final R16			; 120-122

	vaddpd	zmm4, zmm4, zmm2		;; (real#10 + imag#10) final R10			; 121-123

	vsubpd	zmm2, zmm3, zmm11		;; (real#4 - imag#4) final R22				; 122-124

	vaddpd	zmm3, zmm3, zmm11		;; (real#4 + imag#4) final R4				; 123-125
	zstore	[srcreg+d2+d1+32], zmm6		;; Save R16						; 123

	vsubpd	zmm11, zmm5, zmm7		;; (real#12 - imag#12) final R14			; 124-126
	zstore	[srcreg+4*d2+d1], zmm4		;; Save R10						; 124

	vaddpd	zmm5, zmm5, zmm7		;; (real#12 + imag#12) final R12			; 125-127
	zstore	[srcreg+4*d2+d1+32], zmm2	;; Save R22						; 125

	vsubpd	zmm7, zmm0, zmm12		;; (real#2 - imag#2) final R24				; 126-128
	zstore	[srcreg+d2+d1], zmm3		;; Save R4						; 126

	vaddpd	zmm0, zmm0, zmm12		;; (real#2 + imag#2) final R2				; 127-129
	zstore	[srcreg+d1+32], zmm11		;; Save R14						; 127

	vsubpd	zmm12, zmm15, zmm13		;; (real#8 - imag#8) final R18				; 128-130
	zstore	[srcreg+5*d2+d1], zmm5		;; Save R12						; 128

	vaddpd	zmm15, zmm15, zmm13		;; (real#8 + imag#8) final R8				; 129-131
	zstore	[srcreg+5*d2+d1+32], zmm7	;; Save R24						; 129

	vsubpd	zmm13, zmm14, zmm9		;; (real#6 - imag#6) final R20				; 130-132
	zstore	[srcreg+d1], zmm0		;; Save R2						; 130

	vaddpd	zmm14, zmm14, zmm9		;; (real#6 + imag#6) final R6				; 131-133
	zstore	[srcreg+2*d2+d1+32], zmm12	;; Save R18						; 131

	zstore	[srcreg+3*d2+d1], zmm15		;; Save R8						; 132
	zstore	[srcreg+3*d2+d1+32], zmm13	;; Save R20						; 133
	zstore	[srcreg+2*d2+d1], zmm14		;; Save R6						; 134

	bump	srcreg, srcinc
	bump	screg, scinc
	ENDM

;; Haswell FMA3 version
;; This is the original Bulldozer version timed at 80.2 clocks.  Converting adds and subs to FMA3 did not improve the timings.

IF (@INSTR(,%zarch,<FMA3>) NE 0)

yr6_12cl_24_reals_unfft_preload MACRO
	ENDM

yr6_12cl_24_reals_unfft MACRO srcreg,srcinc,d1,d2,screg,scinc,maxrpt,L1pt,L1pd

;; Apply the 11 twiddle factors.  Also do the first +/- on the reals so that
;; we have a perfect balance of adds and multiplies.

	vmovapd	zmm3, [screg+64+32]		;; cosine/sine for R3/I3
	vmovapd	zmm2, [srcreg+d2]		;; R3
	vmovapd	zmm1, [srcreg+d2+32]		;; I3
	zfmaddpd zmm0, zmm2, zmm3, zmm1		;; A3 = R3 * cosine/sine + I3				; 1-5		n 6
	zfmsubpd zmm1, zmm1, zmm3, zmm2		;; B3 = I3 * cosine/sine - R3				; 1-5		n 6

	vmovapd	zmm5, [screg+3*64+32]		;; cosine/sine for R5/I5
	vmovapd	zmm4, [srcreg+2*d2]		;; R5
	vmovapd	zmm3, [srcreg+2*d2+32]		;; I5
	zfmaddpd zmm2, zmm4, zmm5, zmm3		;; A5 = R5 * cosine/sine + I5				; 2-6		n 7
	zfmsubpd zmm3, zmm3, zmm5, zmm4		;; B5 = I5 * cosine/sine - R5				; 2-6		n 7

	vmovapd	zmm7, [screg+5*64+32]		;; cosine/sine for R7/I7
	vmovapd	zmm6, [srcreg+3*d2]		;; R7
	vmovapd	zmm5, [srcreg+3*d2+32]		;; I7
	zfmaddpd zmm4, zmm6, zmm7, zmm5		;; A7 = R7 * cosine/sine + I7				; 3-7		n 8
	zfmsubpd zmm5, zmm5, zmm7, zmm6		;; B7 = I7 * cosine/sine - R7				; 3-7		n 8

	vmovapd	zmm9, [screg+9*64+32]		;; cosine/sine for R11/I11
	vmovapd	zmm8, [srcreg+5*d2]		;; R11
	vmovapd	zmm7, [srcreg+5*d2+32]		;; I11
	zfmaddpd zmm6, zmm8, zmm9, zmm7		;; A11 = R11 * cosine/sine + I11			; 4-8		n 11
	zfmsubpd zmm7, zmm7, zmm9, zmm8		;; B11 = I11 * cosine/sine - R11			; 4-8		n 12

	vmovapd	zmm11, [screg+7*64+32]		;; cosine/sine for R9/I9
	vmovapd	zmm10, [srcreg+4*d2]		;; R9
	vmovapd	zmm9, [srcreg+4*d2+32]		;; I9
	zfmaddpd zmm8, zmm10, zmm11, zmm9	;; A9 = R9 * cosine/sine + I9				; 5-9		n 13
	zfmsubpd zmm9, zmm9, zmm11, zmm10	;; B9 = I9 * cosine/sine - R9				; 5-9		n 14

	vmovapd	zmm10, [screg+64]		;; sine for R3/I3
	vmulpd	zmm0, zmm0, zmm10		;; R3 = A3 * sine					; 6-10		n 11
	vmulpd	zmm1, zmm1, zmm10		;; I3 = B3 * sine					; 6-10		n 12

	vmovapd	zmm10, [screg+3*64]		;; sine for R5/I5
	vmulpd	zmm2, zmm2, zmm10		;; R5 = A5 * sine					; 7-11		n 13
	vmulpd	zmm3, zmm3, zmm10		;; I5 = B5 * sine					; 7-11		n 14

	vmovapd	zmm10, [screg+5*64]		;; sine for R7/I7
	vmulpd	zmm4, zmm4, zmm10		;; R7 = A7 * sine					; 8-12		n 28
	vmulpd	zmm5, zmm5, zmm10		;; I7 = B7 * sine					; 8-12

	vmovapd	zmm13, [screg+32]		;; cosine/sine for R2/I2
	vmovapd	zmm12, [srcreg+d1]		;; R2
	vmovapd	zmm11, [srcreg+d1+32]		;; I2
	zfmaddpd zmm10, zmm12, zmm13, zmm11	;; A2 = R2 * cosine/sine + I2				; 9-13		n 17
	zfmsubpd zmm11, zmm11, zmm13, zmm12	;; B2 = I2 * cosine/sine - R2				; 9-13		n 17

	vmovapd	zmm15, [screg+4*64+32]		;; cosine/sine for R6/I6
	vmovapd	zmm14, [srcreg+2*d2+d1]		;; R6
	vmovapd	zmm13, [srcreg+2*d2+d1+32]	;; I6
	zfmaddpd zmm12, zmm14, zmm15, zmm13	;; A6 = R6 * cosine/sine + I6				; 10-14		n 19
	zfmsubpd zmm13, zmm13, zmm15, zmm14	;; B6 = I6 * cosine/sine - R6				; 10-14		n 19

	vmovapd	zmm15, [screg+9*64]		;; sine for R11/I11
	zfmaddpd zmm14, zmm6, zmm15, zmm0	;; R3+(R11 = A11 * sine)				; 11-15		n 28
	zfnmaddpd zmm6, zmm6, zmm15, zmm0	;; R3-(R11 = A11 * sine)				; 11-15

	zfmaddpd zmm0, zmm7, zmm15, zmm1	;; I3+(I11 = B11 * sine)				; 12-16
	zfnmaddpd zmm7, zmm7, zmm15, zmm1	;; I3-(I11 = B11 * sine)				; 12-16		n 32

	vmovapd	zmm15, [screg+7*64]		;; sine for R9/I9
	zfmaddpd zmm1, zmm8, zmm15, zmm2	;; R5+(R9 = A9 * sine)					; 13-17
	zfnmaddpd zmm8, zmm8, zmm15, zmm2	;; R5-(R9 = A9 * sine)					; 13-17
	zstore	ZMM_TMPS[0*32], zmm4		;; Save R7						; 13
	vmovapd	zmm4, [screg+2*64+32]		;; cosine/sine for R4/I4

	zfmaddpd zmm2, zmm9, zmm15, zmm3	;; I5+(I9 = B9 * sine)					; 14-18
	zfnmaddpd zmm9, zmm9, zmm15, zmm3	;; I5-(I9 = B9 * sine)					; 14-18
	vmovapd	zmm15, [srcreg+d2+d1]		;; R4
	vmovapd	zmm3, [srcreg+d2+d1+32]		;; I4
	zstore	ZMM_TMPS[1*32], zmm5		;; Save I7						; 13+1

	zfmaddpd zmm5, zmm15, zmm4, zmm3	;; A4 = R4 * cosine/sine + I4				; 15-19		n 21
	zfmsubpd zmm3, zmm3, zmm4, zmm15	;; B4 = I4 * cosine/sine - R4				; 15-19		n 21

	vmovapd	zmm4, [screg+10*64+32]		;; cosine/sine for R12/I12
	vmovapd	zmm15, [srcreg+5*d2+d1]		;; R12
	zstore	ZMM_TMPS[2*32], zmm6		;; Save R3-R11						; 16
	vmovapd	zmm6, [srcreg+5*d2+d1+32]	;; I12
	zstore	ZMM_TMPS[3*32], zmm0		;; Save I3+I11						; 17
	zfmaddpd zmm0, zmm15, zmm4, zmm6	;; A12 = R12 * cosine/sine + I12			; 16-20		n 22
	zfmsubpd zmm6, zmm6, zmm4, zmm15	;; B12 = I12 * cosine/sine - R12			; 16-20		n 23

	vmovapd	zmm4, [screg]			;; sine for R2/I2
	vmulpd	zmm10, zmm10, zmm4		;; R2 = A2 * sine					; 17-21		n 22
	vmulpd	zmm11, zmm11, zmm4		;; I2 = B2 * sine					; 17-21		n 23

	vmovapd	zmm4, [screg+6*64+32]		;; cosine/sine for R8/I8
	vmovapd	zmm15, [srcreg+3*d2+d1]		;; R8
	zstore	ZMM_TMPS[4*32], zmm7		;; Save I3-I11						; 17+1
	vmovapd	zmm7, [srcreg+3*d2+d1+32]	;; I8
	zstore	ZMM_TMPS[5*32], zmm8		;; Save R5-R9						; 18+1
	zfmaddpd zmm8, zmm15, zmm4, zmm7	;; A8 = R8 * cosine/sine + I8				; 18-22		n 24
	zfmsubpd zmm7, zmm7, zmm4, zmm15	;; B8 = I8 * cosine/sine - R8				; 18-22		n 25

	vmovapd	zmm4, [screg+4*64]		;; sine for R6/I6
	vmulpd	zmm12, zmm12, zmm4		;; R6 = A6 * sine					; 19-23		n 24
	vmulpd	zmm13, zmm13, zmm4		;; I6 = B6 * sine					; 19-23		n 25

	vmovapd	zmm4, [screg+8*64+32]		;; cosine/sine for R10/I10
	vmovapd	zmm15, [srcreg+4*d2+d1]		;; R10
	zstore	ZMM_TMPS[6*32], zmm2		;; Save I5+I9						; 19+1
	vmovapd	zmm2, [srcreg+4*d2+d1+32]	;; I10
	zstore	ZMM_TMPS[7*32], zmm9		;; Save I5-I9						; 19+2
	zfmaddpd zmm9, zmm15, zmm4, zmm2	;; A10 = R10 * cosine/sine + I10			; 20-24		n 26
	zfmsubpd zmm2, zmm2, zmm4, zmm15	;; B10 = I10 * cosine/sine - R10			; 20-24		n 27

	vmovapd	zmm4, [screg+2*64]		;; sine for R4/I4
	vmulpd	zmm5, zmm5, zmm4		;; R4 = A4 * sine					; 21-25		n 26
	vmulpd	zmm3, zmm3, zmm4		;; I4 = B4 * sine					; 21-25		n 27

	vmovapd	zmm4, [screg+10*64]		;; sine for R12/I12
	zfmaddpd zmm15, zmm0, zmm4, zmm10	;; R2+(R12 = A12 * sine)				; 22-26		n 29
	zfnmaddpd zmm0, zmm0, zmm4, zmm10	;; R2-(R12 = A12 * sine)				; 22-26

	zfnmaddpd zmm10, zmm6, zmm4, zmm11	;; I2-(I12 = B12 * sine)				; 23-27		n 31
	zfmaddpd zmm6, zmm6, zmm4, zmm11	;; I2+(I12 = B12 * sine)				; 23-27

	vmovapd	zmm4, [screg+6*64]		;; sine for R8/I8
	zfmaddpd zmm11, zmm8, zmm4, zmm12	;; R6+(R8 = A8 * sine)					; 24-28		n 29
	zfnmaddpd zmm8, zmm8, zmm4, zmm12	;; R6-(R8 = A8 * sine)					; 24-28

	zfnmaddpd zmm12, zmm7, zmm4, zmm13	;; I6-(I8 = B8 * sine)					; 25-29		n 31
	zfmaddpd zmm7, zmm7, zmm4, zmm13	;; I6+(I8 = B8 * sine)					; 25-29

	vmovapd	zmm4, [screg+8*64]		;; sine for R10/I10
	zfmaddpd zmm13, zmm9, zmm4, zmm5	;; R4+(R10 = A10 * sine)				; 26-30		n 34
	zfnmaddpd zmm9, zmm9, zmm4, zmm5	;; R4-(R10 = A10 * sine)				; 26-30
	vmovapd	zmm5, ZMM_HALF

	zstore	ZMM_TMPS[8*32], zmm0		;; Save R2-R12						; 27
	zfnmaddpd zmm0, zmm2, zmm4, zmm3	;; I4-(I10 = B10 * sine)				; 27-31		n 36
	zfmaddpd zmm2, zmm2, zmm4, zmm3		;; I4+(I10 = B10 * sine)				; 27-31

	;; Do the 24 reals inverse FFT

	vmovapd	zmm4, ZMM_TMPS[0*32]		;; r7
	vmovapd	zmm3, [srcreg]			;; r1+r13
	zstore	ZMM_TMPS[0*32], zmm6		;; Save I2+I12						; 28
	zfmsubpd zmm6, zmm5, zmm14, zmm4	;; .5(r3+r11)-r7 (r35oB)				; 28-32		n 33
	zstore	ZMM_TMPS[9*32], zmm8		;; Save R6-R8						; 29
	zfnmaddpd zmm8, zmm5, zmm1, zmm3	;; (r1+r13)-.5(r5+r9) (r35oA)				; 28-32		n 33

	zstore	ZMM_TMPS[10*32], zmm7		;; Save I6+I8						; 30
	vmovapd	zmm7, ZMM_ONE
	zstore	ZMM_TMPS[11*32], zmm2		;; Save I4+I10						; 32
	vaddpd	zmm2, zmm15, zmm11		;; (r2+r12)+(r6+r8)					; 29-33		n 34
	zfmsubpd zmm15, zmm15, zmm7, zmm11	;; (r2+r12)-(r6+r8) (r3e/.866)				; 29-33		n 38

	vaddpd	zmm14, zmm14, zmm4		;; (r3+r11)+r7 (r17oB)					; 30-34		n 35
	zfmaddpd zmm3, zmm3, zmm7, zmm1		;; (r1+r13)+(r5+r9) (r17oA)				; 30-34		n 35
	vmovapd	zmm11, ZMM_TMPS[4*32]		;; i3-i11

	vaddpd	zmm1, zmm10, zmm12		;; (i2-i12)+(i6-i8)					; 31-35		n 36
	zfmsubpd zmm10, zmm10, zmm7, zmm12	;; (i2-i12)-(i6-i8) (i5e/.866)				; 31-35		n 37

	vmovapd	zmm4, ZMM_TMPS[7*32]		;; i5-i9
	vsubpd	zmm12, zmm11, zmm4		;; (i3-i11)-(i5-i9) (i5o/.866)				; 32-36		n 37
	zfmaddpd zmm11, zmm11, zmm7, zmm4	;; (i3-i11)+(i5-i9) (i3o/.866)				; 32-36		n 41

	vaddpd	zmm4, zmm8, zmm6		;; r35oA + r35oB (r3o)					; 33-37		n 38
	zfmsubpd zmm8, zmm8, zmm7, zmm6		;; r35oA - r35oB (r5o)					; 33-37		n 39
	L1prefetchw srcreg+L1pd, L1pt

	zfmsubpd zmm6, zmm5, zmm2, zmm13	;; .5((r2+r12)+(r6+r8))-(r4+r10) (r5e)			; 34-38		n 39
	vaddpd	zmm2, zmm2, zmm13		;; (r2+r12)+(r6+r8)+(r4+r10) (r1e)			; 34-38		n 40
	L1prefetchw srcreg+d1+L1pd, L1pt

	vaddpd	zmm13, zmm3, zmm14		;; r17oA + r17oB (r1o)					; 35-39		n 40
	zfmsubpd zmm3, zmm3, zmm7, zmm14	;; r17oA - r17oB (r7)					; 35-39		n 42
	vmovapd zmm14, ZMM_P866

	zfmaddpd zmm5, zmm5, zmm1, zmm0		;; .5((i2-i12)+(i6-i8))+(i4-i10) (i3e)			; 36-40		n 41
	vsubpd	zmm1, zmm1, zmm0		;; (i2-i12)+(i6-i8)-(i4-i10) (i7)			; 36-40		n 42
	L1prefetchw srcreg+d2+L1pd, L1pt

	vaddpd	zmm0, zmm10, zmm12		;; i5e + i5o (i5/.866)					; 37-41		n 44
	zfmsubpd zmm10, zmm10, zmm7, zmm12	;; i5e - i5o (i9/.866)					; 37-41		n 45

	zfmaddpd zmm12, zmm14, zmm15, zmm4	;; r3o + .866*r3e (real-cols row #3)			; 38-42		n 46
	zfnmaddpd zmm15, zmm14, zmm15, zmm4	;; r3o - .866*r3e (real-cols row #11)			; 38-42		n 56
	L1prefetchw srcreg+d2+d1+L1pd, L1pt

	vaddpd	zmm4, zmm8, zmm6		;; r5o + r5e (real-cols row #5)				; 39-43		n 44
	zfmsubpd zmm8, zmm8, zmm7, zmm6		;; r5o - r5e (real-cols row #9)				; 39-43		n 45

	vaddpd	zmm6, zmm13, zmm2		;; r1o + r1e (final R1)					; 40-44
	zfmsubpd zmm13, zmm13, zmm7, zmm2	;; r1o - r1e (final R13)				; 40-44
	L1prefetchw srcreg+2*d2+L1pd, L1pt

	zfmaddpd zmm2, zmm14, zmm11, zmm5	;; i3e + .866*i3o (i3)					; 41-45		n 46
	zfnmaddpd zmm11, zmm14, zmm11, zmm5	;; i3e - .866*i3o (i11)					; 41-45		n 56
	L1prefetchw srcreg+2*d2+d1+L1pd, L1pt

	vsubpd	zmm5, zmm3, zmm1		;; (r7 - i7) final R19					; 42-46
	zfmaddpd zmm3, zmm3, zmm7, zmm1		;; (r7 + i7) final R7					; 42-46

	vmovapd	zmm1, ZMM_TMPS[8*32]		;; r2-r12
	vmovapd	zmm7, ZMM_P966_P707
	zstore	[srcreg], zmm6			;; Save final R1					; 45
	vsubpd	zmm6, zmm1, zmm9		;; (r2-r12) - (r4-r10)					; 43-47		n 51
	zstore	[srcreg+3*d2+32], zmm5		;; Save R19						; 47
	zfmaddpd zmm5, zmm7, zmm1, zmm9		;; .966/.707(r2-r12) + (r4-r10)				; 43-47		n 51

	zstore	[srcreg+3*d2], zmm3		;; Save R7						; 47+1
	zfnmaddpd zmm3, zmm0, zmm14, zmm4	;; (r5 - i5*.866) final R21				; 44-48
	zfmaddpd zmm0, zmm0, zmm14, zmm4	;; (r5 + i5*.866) final R5				; 44-48

	zfnmaddpd zmm4, zmm10, zmm14, zmm8	;; (r9 - i9*.866) final R17				; 45-49
	zfmaddpd zmm10, zmm10, zmm14, zmm8	;; (r9 + i9*.866) final R9				; 45-49

	vsubpd	zmm14, zmm12, zmm2		;; (r3 - i3) final R23					; 46-50
	zfmaddpd zmm12, zmm12, ZMM_ONE, zmm2	;; (r3 + i3) final R3					; 46-50

	vmovapd	zmm8, ZMM_P259_P707
	zfmsubpd zmm1, zmm8, zmm1, zmm9		;; .259/.707(r2-r12) - (r4-r10)				; 47-51		n 52
	vmovapd	zmm2, ZMM_TMPS[0*32]		;; i2+i12
	vmovapd	zmm9, ZMM_TMPS[11*32]		;; i4+i10
	zstore	[srcreg+4*d2+32], zmm3		;; Save R21						; 49
	vaddpd	zmm3, zmm2, zmm9		;; (i2+i12)+(i4+i10)					; 47-51		n 52

	zstore	[srcreg+2*d2], zmm0		;; Save R5						; 49+1
	zfmaddpd zmm0, zmm8, zmm2, zmm9		;; .259/.707(i2+i12)+(i4+i10)				; 48-52		n 53
	zfmsubpd zmm2, zmm7, zmm2, zmm9		;; .966/.707(i2+i12)-(i4+i10)				; 48-52		n 53

	vmovapd	zmm9, [srcreg+32]		;; r1-r13
	zstore	[srcreg+32], zmm13		;; Save final R13					; 45+1
	vmovapd	zmm13, ZMM_TMPS[5*32]		;; r5-r9
	zstore	[srcreg+2*d2+32], zmm4		;; Save R17						; 50+1
	vmovapd	zmm4, ZMM_HALF
	zstore	[srcreg+4*d2], zmm10		;; Save R9						; 50+2
	zfmaddpd zmm10, zmm4, zmm13, zmm9	;; r1-r13 + .5(r5-r9)  (r26oA)				; 49-53		n 54
	vsubpd	zmm9, zmm9, zmm13		;; r1-r13 - (r5-r9)  (r4o)				; 49-53		n 57

	vmovapd	zmm13, ZMM_TMPS[3*32]		;; i3+i11
	zstore	[srcreg+5*d2+32], zmm14		;; Save R23						; 51+2
	vmovapd	zmm14, ZMM_TMPS[1*32]		;; i7
	zfmaddpd zmm4, zmm4, zmm13, zmm14	;; .5(i3+i11)+i7					; 50-54		n 55
	vsubpd	zmm13, zmm13, zmm14		;; (i3+i11)-i7 (i4o)					; 50-54		n 58

	vmovapd	zmm14, ZMM_TMPS[9*32]		;; r6-r8
	vsubpd	zmm6, zmm6, zmm14		;; (r2-r12)-(r4-r10) - (r6-r8) (r4e/.707)		; 51-55		n 57
	zfmaddpd zmm5, zmm8, zmm14, zmm5	;; .966/.707(r2-r12)+(r4-r10)+.259/.707(r6-r8) (r2e/.707) ; 51-55	n 59

	zfmaddpd zmm1, zmm7, zmm14, zmm1	;; .259/.707(r2-r12)-(r4-r10)+.966/.707(r6-r8) (r6e/.707) ; 52-56	n 61
	vmovapd	zmm14, ZMM_TMPS[10*32]		;; i6+i8
	vsubpd	zmm3, zmm3, zmm14		;; (i2+i12)+(i4+i10)-(i6+i8) (i4e/.707)			; 52-56		n 58

	zfmaddpd zmm0, zmm7, zmm14, zmm0	;; .259/.707(i2+i12)+(i4+i10)+.966/.707(i6+i8) (i2e/.707) ; 53-57	n 60
	zfmaddpd zmm2, zmm8, zmm14, zmm2	;; .966/.707(i2+i12)-(i4+i10)+.259/.707(i6+i8) (i6e/.707) ; 53-57	n 62

	vmovapd	zmm7, ZMM_TMPS[2*32]		;; r3-r11
	vmovapd zmm8, ZMM_P866
	zfmaddpd zmm14, zmm8, zmm7, zmm10	;; r26oA + .866(r3-r11) (r2o)				; 54-58		n 59
	zfnmaddpd zmm7, zmm8, zmm7, zmm10	;; r26oA - .866(r3-r11) (r6o)				; 54-58		n 61

	vmovapd	zmm10, ZMM_TMPS[6*32]		;; i5+i9
	zstore	[srcreg+d2], zmm12		;; Save R3						; 51+3
	zfmaddpd zmm12, zmm8, zmm10, zmm4	;; .5(i3+i11)+i7+.866(i5+i9) (i2o)			; 55-59		n 60
	zfnmaddpd zmm8, zmm8, zmm10, zmm4	;; .5(i3+i11)+i7-.866(i5+i9) (i6o)			; 55-59		n 62

	vmovapd	zmm10, ZMM_ONE
	vsubpd	zmm4, zmm15, zmm11		;; (r11 - i11) final R15				; 56-60
	zfmaddpd zmm15, zmm15, zmm10, zmm11	;; (r11 + i11) final R11				; 56-60

	vmovapd	zmm11, ZMM_SQRTHALF
	zstore	[srcreg+d2+32], zmm4		;; Save R15						; 61
	zfnmaddpd zmm4, zmm11, zmm6, zmm9	;; real-cols row #10 (ro4 - .707*re4)			; 57-61		n 63
	zfmaddpd zmm6, zmm11, zmm6, zmm9	;; real-cols row #4 (ro4 + .707*re4)			; 57-61		n 64

	zfmsubpd zmm9, zmm3, zmm11, zmm13	;; imag-cols row #10 (i4e*.707 - i4o)			; 58-62		n 63
	zfmaddpd zmm3, zmm3, zmm11, zmm13	;; imag-cols row #4 (i4e*.707 + i4o)			; 58-62		n 64
	L1prefetchw srcreg+3*d2+L1pd, L1pt

	zfnmaddpd zmm13, zmm11, zmm5, zmm14	;; real-cols row #12 (ro2 - .707*re2)			; 59-63		n 65
	zfmaddpd zmm5, zmm11, zmm5, zmm14	;; real-cols row #2 (ro2 + .707*re2)			; 59-63		n 66
	L1prefetchw srcreg+3*d2+d1+L1pd, L1pt

	zfmsubpd zmm14, zmm0, zmm11, zmm12	;; imag-cols row #12 (i2e*.707 - i2o)			; 60-64		n 65
	zfmaddpd zmm0, zmm0, zmm11, zmm12	;; imag-cols row #2 (i2e*.707 + i2o)			; 60-64		n 66

	zfnmaddpd zmm12, zmm11, zmm1, zmm7	;; real-cols row #8 (ro6 - .707*re6)			; 61-65		n 67
	zfmaddpd zmm1, zmm11, zmm1, zmm7	;; real-cols row #6 (ro6 + .707*re6)			; 61-65		n 68
	L1prefetchw srcreg+4*d2+L1pd, L1pt

	zfmsubpd zmm7, zmm2, zmm11, zmm8	;; imag-cols row #8 (i6e*.707 - i6o)			; 62-66		n 67
	zfmaddpd zmm2, zmm2, zmm11, zmm8	;; imag-cols row #6 (i6e*.707 + i6o)			; 62-66		n 68
	zstore	[srcreg+5*d2], zmm15		;; Save R11						; 61+1

	vsubpd	zmm8, zmm4, zmm9		;; (real#10 - imag#10) final R16			; 63-67
	zfmaddpd zmm4, zmm4, zmm10, zmm9	;; (real#10 + imag#10) final R10			; 63-67
	L1prefetchw srcreg+4*d2+d1+L1pd, L1pt

	vsubpd	zmm9, zmm6, zmm3		;; (real#4 - imag#4) final R22				; 64-68
	zfmaddpd zmm6, zmm6, zmm10, zmm3	;; (real#4 + imag#4) final R4				; 64-68
	L1prefetchw srcreg+5*d2+L1pd, L1pt

	vsubpd	zmm3, zmm13, zmm14		;; (real#12 - imag#12) final R14			; 65-69
	zfmaddpd zmm13, zmm13, zmm10, zmm14	;; (real#12 + imag#12) final R12			; 65-69

	vsubpd	zmm14, zmm5, zmm0		;; (real#2 - imag#2) final R24				; 66-70
	zfmaddpd zmm5, zmm5, zmm10, zmm0	;; (real#2 + imag#2) final R2				; 66-70
	L1prefetchw srcreg+5*d2+d1+L1pd, L1pt

	vsubpd	zmm0, zmm12, zmm7		;; (real#8 - imag#8) final R18				; 67-71
	zfmaddpd zmm12, zmm12, zmm10, zmm7	;; (real#8 + imag#8) final R8				; 67-71

	vsubpd	zmm7, zmm1, zmm2		;; (real#6 - imag#6) final R20				; 68-72
	zfmaddpd zmm1, zmm1, zmm10, zmm2	;; (real#6 + imag#6) final R6				; 68-72

	zstore	[srcreg+d2+d1+32], zmm8		;; Save R16						; 68
	zstore	[srcreg+4*d2+d1], zmm4		;; Save R10						; 68+1
	zstore	[srcreg+4*d2+d1+32], zmm9	;; Save R22						; 69+1
	zstore	[srcreg+d2+d1], zmm6		;; Save R4						; 69+2
	zstore	[srcreg+d1+32], zmm3		;; Save R14						; 70+2
	zstore	[srcreg+5*d2+d1], zmm13		;; Save R12						; 70+3
	zstore	[srcreg+5*d2+d1+32], zmm14	;; Save R24						; 71+3
	zstore	[srcreg+d1], zmm5		;; Save R2						; 71+4
	zstore	[srcreg+2*d2+d1+32], zmm0	;; Save R18						; 72+4
	zstore	[srcreg+3*d2+d1], zmm12		;; Save R8						; 72+5
	zstore	[srcreg+3*d2+d1+32], zmm7	;; Save R20						; 73+5
	zstore	[srcreg+2*d2+d1], zmm1		;; Save R6						; 73+6

	bump	srcreg, srcinc
	bump	screg, scinc
	ENDM

ENDIF

ENDIF
